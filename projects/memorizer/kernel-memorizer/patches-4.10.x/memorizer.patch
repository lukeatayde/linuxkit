diff --git a/Documentation/memorizer.txt b/Documentation/memorizer.txt
new file mode 100644
index 000000000000..197b9cc5fea4
--- /dev/null
+++ b/Documentation/memorizer.txt
@@ -0,0 +1,111 @@
+             
+             +--------------------------------------------------+
+             | Memorizer: Kernel Memory Access Patterns (KMAPs) |
+             +--------------------------------------------------+
+
+Introduction
+============
+
+Memorizer is a tool to record information about access to kernel objects:
+specifically, it counts memory accesses from distinct IP addresses in the
+kernel source and also the PID that accessed, thereby providing spatial and
+temporal dimensions.
+
+Interface via debugfs
+=====================
+
+The tool has a very simple interface at the moment. It can:
+
+- Print out some statistics about memory allocations and memory accesses
+- Control enable/disable of memory object allocation tracking and memory access
+  tracing
+- Print the KMAP using the debugfs file system
+
+Enable object allocation tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/memorizer_enabled
+```
+
+Enable object access tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/memorizer_log_access
+```
+
+Show allocation statistics:
+```bash
+cat /sys/kernel/debug/memorizer/show_stats
+```
+
+Clear free'd objects:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/clear_object_list
+```
+Enable function call tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/cfg_log_on
+```
+
+Enable function call and stack frame tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/stack_trace_on
+```
+
+Clear function call and stack frame tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/cfgmap
+```
+
+Using Memorizer to Collect KMAPs
+================================
+
+Memorizer lacks push style logging and clearing of the object lists, therefore
+it has the propensity of overflowing memory. The only way to manage the log and
+current set of objects is to manually clear and print the KMAPs.
+
+Therefore, a typical run using memorizer to create KMAPs includes:
+
+```bash
+# mount the debugfs filesystem if it isn't already
+mount -t debugfs nodev /sys/kernel/debug
+# clear free objects: the current system traces from boot with a lot of
+# uninteresting data
+echo 1 > /sys/kernel/debug/clear_object_list
+# enable memorizer object access tracking, which by default is off
+echo 1 > /sys/kernel/debug/memorizer_log_access
+# Now run whatever test
+tar zcf something.tar.gz /somedir &
+ssh u@h:/somefile 
+...
+# Disable access logging
+echo 0 > /sys/kernel/debug/memorizer/memorizer_log_access
+# Disable memorizer object tracking: isn't necessary but will reduce noise
+echo 0 > /sys/kernel/debug/memorizer/memorizer_enabled
+# Cat the results: make sure to pipe to something
+cat /sys/kernel/debug/memorizer/kmap > test.kmap
+```
+
+Output Format
+=============
+
+Memorizer outputs data as text, which may change if space is a problem. The
+format of the kmap file is as follows:
+
+alloc_ip,pid,obj_va_ptr,size,alloc_jiffies,free_jiffies,free_ip,executable
+  access_ip,access_pid,write_count,read_count
+  access_ip,access_pid,write_count,read_count
+  access_ip,access_pid,write_count,read_count
+    ...
+    ...
+
+There are a few special error codes: 
+
+    - Not all free_ip's could be obtained correctly and therefore some of these
+      will be 0.
+    - There is a bug where we insert into the live object map over another
+      allocation, this implies that we are missing a free. So for now we mark
+      the free_ip as 0xDEADBEEF.
+
+Note
+====
+cfg_log_on and stack_trace_on shares the same <caller, callee> mapping structure. Please
+choose either one to turn on and clean the cfgmap after finished. 
\ No newline at end of file
diff --git a/Makefile b/Makefile
index f1e6a02a0c19..c06545767f68 100644
--- a/Makefile
+++ b/Makefile
@@ -742,6 +742,11 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_MEMORIZER
+KBUILD_CFLAGS += -finstrument-functions \
+		   -finstrument-functions-exclude-file-list=mm/memorizer/,kernel/locking/,mm/kasan/
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifndef CC_FLAGS_FTRACE
 CC_FLAGS_FTRACE := -pg
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 44163e8c3868..3e8b82fa4d45 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -39,7 +39,9 @@ KBUILD_AFLAGS  := $(KBUILD_CFLAGS) -D__ASSEMBLY__
 GCOV_PROFILE := n
 UBSAN_SANITIZE :=n
 
-LDFLAGS := -m elf_$(UTS_MACHINE)
+# Memorizer: added allowing multiple definitions so that inlining libs works with
+# the compressed vmlinux too. There's probably a better way...
+LDFLAGS := -m elf_$(UTS_MACHINE) --allow-multiple-definition
 # Compressed kernel should be built as PIE since it may be loaded at any
 # address by the bootloader.
 ifeq ($(CONFIG_X86_32),y)
diff --git a/arch/x86/boot/string.h b/arch/x86/boot/string.h
index 113588ddb43f..af7dfa691676 100644
--- a/arch/x86/boot/string.h
+++ b/arch/x86/boot/string.h
@@ -2,7 +2,9 @@
 #define BOOT_STRING_H
 
 /* Undef any of these macros coming from string_32.h. */
+#ifdef CONFIG_INLINE_LIBS
 #undef memcpy
+#endif
 #undef memset
 #undef memcmp
 
diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S
index 57f7ec35216e..74d8b489464a 100644
--- a/arch/x86/entry/entry_32.S
+++ b/arch/x86/entry/entry_32.S
@@ -1012,9 +1012,7 @@ ftrace_stub:
 	movl	0xc(%esp), %eax
 	movl	0x4(%ebp), %edx
 	subl	$MCOUNT_INSN_SIZE, %eax
-
 	call	*ftrace_trace_function
-
 	popl	%edx
 	popl	%ecx
 	popl	%eax
diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h
index ea148313570f..6ae617726fda 100644
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -677,8 +677,18 @@ extern struct movsl_mask {
 
 unsigned long __must_check _copy_from_user(void *to, const void __user *from,
 					   unsigned n);
-unsigned long __must_check _copy_to_user(void __user *to, const void *from,
-					 unsigned n);
+
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) static unsigned long __must_check _copy_to_user(void __user *to, const void *from, unsigned n);
+inline static unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
+{
+  if (access_ok(VERIFY_WRITE, to, n))
+    n = __copy_to_user(to, from, n);
+  return n;
+}
+#else
+unsigned long __must_check _copy_to_user(void __user *to, const void *from, unsigned n);
+#endif
 
 extern void __compiletime_error("usercopy buffer size is too small")
 __bad_copy_user(void);
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index 4cfba947d774..411e7ab9febe 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -69,6 +69,7 @@
 #include <linux/crash_dump.h>
 #include <linux/tboot.h>
 #include <linux/jiffies.h>
+#include <linux/memorizer.h>
 
 #include <video/edid.h>
 
@@ -1205,6 +1206,7 @@ void __init setup_arch(char **cmdline_p)
 #ifdef CONFIG_KVM_GUEST
 	kvmclock_init();
 #endif
+	memorizer_init();
 
 	x86_init.paging.pagetable_init();
 
diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index c074799bddae..ad11ff11b750 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -49,6 +49,7 @@ EXPORT_SYMBOL_GPL(copy_from_user_nmi);
  * Returns number of bytes that could not be copied.
  * On success, this will be zero.
  */
+#ifndef CONFIG_INLINE_LIBS
 unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
 {
 	if (access_ok(VERIFY_WRITE, to, n))
@@ -56,7 +57,7 @@ unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
 	return n;
 }
 EXPORT_SYMBOL(_copy_to_user);
-
+#endif
 /**
  * copy_from_user: - Copy a block of data from user space.
  * @to:   Destination address, in kernel space.
diff --git a/include/asm-generic/uaccess.h b/include/asm-generic/uaccess.h
index cc6bb319e464..3ad2f5e7463a 100644
--- a/include/asm-generic/uaccess.h
+++ b/include/asm-generic/uaccess.h
@@ -103,6 +103,35 @@ static inline __must_check long __copy_from_user(void *to,
 #endif
 
 #ifndef __copy_to_user
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) static inline __must_check long __copy_to_user(void __user *to,
+		const void *from, unsigned long n)
+{
+	if (__builtin_constant_p(n)) {
+		switch(n) {
+		case 1:
+			*(u8 __force *)to = *(u8 *)from;
+			return 0;
+		case 2:
+			*(u16 __force *)to = *(u16 *)from;
+			return 0;
+		case 4:
+			*(u32 __force *)to = *(u32 *)from;
+			return 0;
+#ifdef CONFIG_64BIT
+		case 8:
+			*(u64 __force *)to = *(u64 *)from;
+			return 0;
+#endif
+		default:
+			break;
+		}
+	}
+
+	memcpy((void __force *)to, from, n);
+	return 0;
+}
+#else
 static inline __must_check long __copy_to_user(void __user *to,
 		const void *from, unsigned long n)
 {
@@ -131,6 +160,7 @@ static inline __must_check long __copy_to_user(void __user *to,
 	return 0;
 }
 #endif
+#endif
 
 /*
  * These are the main single-value transfer routines.  They automatically
@@ -267,6 +297,17 @@ static inline long copy_from_user(void *to,
 	return res;
 }
 
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) static inline long copy_to_user(void __user *to,
+		const void *from, unsigned long n)
+{
+	might_fault();
+	if (access_ok(VERIFY_WRITE, to, n))
+		return __copy_to_user(to, from, n);
+	else
+		return n;
+}
+#else
 static inline long copy_to_user(void __user *to,
 		const void *from, unsigned long n)
 {
@@ -276,6 +317,7 @@ static inline long copy_to_user(void __user *to,
 	else
 		return n;
 }
+#endif
 
 /*
  * Copy a null terminated string from userspace.
diff --git a/include/linux/kasan-checks.h b/include/linux/kasan-checks.h
index b7f8aced7870..6a8b8f47de96 100644
--- a/include/linux/kasan-checks.h
+++ b/include/linux/kasan-checks.h
@@ -1,10 +1,13 @@
 #ifndef _LINUX_KASAN_CHECKS_H
 #define _LINUX_KASAN_CHECKS_H
+#include <linux/types.h>
 
 #ifdef CONFIG_KASAN
+enum AllocType kasan_obj_type(const void *p, unsigned int size);
 void kasan_check_read(const void *p, unsigned int size);
 void kasan_check_write(const void *p, unsigned int size);
 #else
+static inline void kasan_obj_type(const void *p, unsigned int size) { }
 static inline void kasan_check_read(const void *p, unsigned int size) { }
 static inline void kasan_check_write(const void *p, unsigned int size) { }
 #endif
diff --git a/include/linux/log2.h b/include/linux/log2.h
index ef3d4f67118c..c373295f359f 100644
--- a/include/linux/log2.h
+++ b/include/linux/log2.h
@@ -15,12 +15,6 @@
 #include <linux/types.h>
 #include <linux/bitops.h>
 
-/*
- * deal with unrepresentable constant logarithms
- */
-extern __attribute__((const, noreturn))
-int ____ilog2_NaN(void);
-
 /*
  * non-constant log of base 2 calculators
  * - the arch may override these in asm/bitops.h if they can be implemented
@@ -85,7 +79,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
 #define ilog2(n)				\
 (						\
 	__builtin_constant_p(n) ? (		\
-		(n) < 1 ? ____ilog2_NaN() :	\
+		(n) < 2 ? 0 :			\
 		(n) & (1ULL << 63) ? 63 :	\
 		(n) & (1ULL << 62) ? 62 :	\
 		(n) & (1ULL << 61) ? 61 :	\
@@ -148,10 +142,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
 		(n) & (1ULL <<  4) ?  4 :	\
 		(n) & (1ULL <<  3) ?  3 :	\
 		(n) & (1ULL <<  2) ?  2 :	\
-		(n) & (1ULL <<  1) ?  1 :	\
-		(n) & (1ULL <<  0) ?  0 :	\
-		____ilog2_NaN()			\
-				   ) :		\
+		1 ) :				\
 	(sizeof(n) <= 4) ?			\
 	__ilog2_u32(n) :			\
 	__ilog2_u64(n)				\
diff --git a/include/linux/memorizer.h b/include/linux/memorizer.h
new file mode 100644
index 000000000000..6733879b2025
--- /dev/null
+++ b/include/linux/memorizer.h
@@ -0,0 +1,195 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2015, The Board of Trustees of the University of Illinois.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
+ * THE SOFTWARE.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.h
+ *
+ *    Description:  Memorizer records data for kernel object lifetime analysis.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _LINUX_MEMORIZER_H
+#define _LINUX_MEMORIZER_H
+
+#include <linux/types.h>
+
+#if 1
+#define FILTER_KASAN 1
+#endif
+
+/**
+ * struct memorizer_kobj - metadata for kernel objects
+ */
+enum AllocType {
+    MEM_STACK=0,
+    MEM_STACK_FRAME,
+    MEM_STACK_ARGS,
+    MEM_STACK_PAGE,
+    MEM_HEAP,
+    MEM_UFO_HEAP,
+    MEM_GLOBAL,
+    MEM_KMALLOC,
+    MEM_KMALLOC_ND,
+    MEM_KMEM_CACHE,
+    MEM_KMEM_CACHE_ND,
+    MEM_VMALLOC,
+    MEM_ALLOC_PAGES,
+    MEM_INDUCED,
+    MEM_BOOTMEM,
+    MEM_MEMBLOCK,
+    MEM_UFO_MEMBLOCK,
+    MEM_MEMORIZER,
+    MEM_USER,
+    MEM_BUG,
+    MEM_UFO_GLOBAL,
+    MEM_UFO_NONE,
+    /* TODO: Legacy type, fix in tracking code to not use */
+    MEM_NONE,
+    NumAllocTypes
+};
+
+/* Storage for global metadata table. Used for offline processing of globals */
+extern char * global_table_text;
+extern char * global_table_ptr;
+
+/* Special value to indicate the alloc_ip of preallocated objects */
+#define MEMORIZER_PREALLOCED 0xfeedbeef
+
+#ifdef CONFIG_MEMORIZER /*----------- !CONFIG_MEMORIZER -------------------- */
+
+/* Special codes */
+enum MEMORIZER_CODES {
+    /* Assume this is the compiler but don't know */
+    MEM_KASAN_N = 0x5, /* for KASAN with no ret ip */
+};
+
+/* Init and Misc */
+void __init memorizer_init(void);
+int memorizer_init_from_driver(void);
+void memorizer_alloc_init(void);
+
+/* Memorize access */
+void memorizer_mem_access(uintptr_t addr, size_t size, bool write, uintptr_t ip);
+
+/* Allocation memorization */
+void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t
+		      bytes_req, size_t bytes_alloc, gfp_t gfp_flags);
+void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t
+			   bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int
+			   node);
+void memorizer_kfree(unsigned long call_site, const void *ptr);
+void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned
+        int order, gfp_t gfp_flags);
+void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned int size, gfp_t gfp_flags);
+void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+        int order, gfp_t gfp_flags);
+
+void memorizer_start_getfreepages(void);
+
+void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned
+			  int order);
+
+void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags); 
+void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags, int node); 
+bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void *ptr);
+
+void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr);
+void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr, unsigned long size, gfp_t gfp_flags);
+void memorizer_vmalloc_free(unsigned long call_site, const void *ptr);
+void memorizer_register_global(const void *ptr, size_t size);
+void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t
+        size);
+void memorizer_alloc(unsigned long call_site, const void *ptr, size_t size,
+		     enum AllocType AT);
+void memorizer_fork(struct task_struct *p, long nr);
+void switchBuffer(void);
+void memorizer_print_stats(void);
+void memorizer_stack_page_alloc(struct task_struct * task);
+void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size);
+void memorizer_memblock_alloc(phys_addr_t base, phys_addr_t size);
+
+/* Temporary Debug and test code */
+int __memorizer_get_opsx(void);
+int __memorizer_get_allocs(void);
+void __memorizer_print_events(unsigned int num_events);
+
+#else /*----------- !CONFIG_MEMORIZER ------------------------- */
+
+static inline void __init memorizer_init(void) {}
+static inline void memorizer_init_from_driver(void) {}
+static inline void memorizer_alloc_init(void) {}
+static inline void memorizer_mem_access(uintptr_t addr, size_t size, bool write, uintptr_t ip) {}
+static inline void __memorizer_get_opsx(void) {}
+static inline void __memorizer_print_events(unsigned int num_events) {}
+static inline void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t bytes_req, size_t bytes_alloc, gfp_t gfp_flags) {}
+static inline void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int node) {}
+static inline void memorizer_kfree(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned int order, gfp_t gfp_flags) {}
+static inline void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned int size, gfp_t gfp_flags){}
+static inline void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned int order) {}
+static inline void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr, size_t bytes_alloc, gfp_t gfp_flags) {}
+static inline bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void *ptr){return true;}
+static inline void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr, struct kmem_cache *s, gfp_t gfp_flags, int node) {}
+static inline void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr, unsigned long size, gfp_t gfp_flags) {}
+static inline void memorizer_vmalloc_free(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_register_global(const void *ptr, size_t size) {}
+static inline void memorizer_alloc(unsigned long call_site, const void *ptr,
+				   size_t size, enum AllocType AT){}
+static inline void memorizer_fork(struct task_struct *p, long nr) {}
+static inline void switchBuffer(void) {}
+static inline void memorizer_print_stats(void) {}
+static inline void memorizer_stack_page_alloc(struct task_struct * task){}
+static inline void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t size){}
+static inline void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size){}
+static inline void memorizer_memblock_alloc(unsigned long base, unsigned long size){}
+static inline void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+						int order, gfp_t gfp_flags){}
+
+static inline void memorizer_start_getfreepages(void){}
+
+#endif /* CONFIG_MEMORIZER */
+
+#endif /* __MEMORIZER_H_ */
+
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ad3ec9ec61f7..125a9d56dca9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1938,6 +1938,9 @@ struct task_struct {
 #ifdef CONFIG_KASAN
 	unsigned int kasan_depth;
 #endif
+#ifdef CONFIG_MEMORIZER
+	unsigned long memorizer_recursion;
+#endif
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored address in ret_stack */
 	int curr_ret_stack;
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 4c5363566815..6ee207d3c27f 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -116,6 +116,7 @@
 
 #include <linux/kmemleak.h>
 #include <linux/kasan.h>
+#include <linux/memorizer.h>
 
 struct mem_cgroup;
 /*
diff --git a/include/linux/string.h b/include/linux/string.h
index 26b6f6a66f83..5ac79e4878ee 100644
--- a/include/linux/string.h
+++ b/include/linux/string.h
@@ -20,12 +20,52 @@ extern void *memdup_user_nul(const void __user *, size_t);
 #ifndef __HAVE_ARCH_STRCPY
 extern char * strcpy(char *,const char *);
 #endif
+
+// strncpy()
 #ifndef __HAVE_ARCH_STRNCPY
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strncpy(char *,const char *, __kernel_size_t);
+extern inline char *strncpy(char *dest, const char *src, size_t count)
+{
+	char *tmp = dest;
+
+	while (count) {
+		if ((*tmp = *src) != 0)
+			src++;
+		tmp++;
+		count--;
+	}
+	return dest;
+}
+#else
 extern char * strncpy(char *,const char *, __kernel_size_t);
 #endif
+#endif
+
+// strlcpy
 #ifndef __HAVE_ARCH_STRLCPY
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) size_t strlcpy(char *, const char *, size_t);
+extern inline size_t strlcpy(char *dest, const char *src, size_t size)
+{
+	size_t ret = strlen(src);
+
+	if (size) {
+		size_t len = (ret >= size) ? size - 1 : ret;
+		//memcpy(dest, src, len);
+		int i;
+		for (i = 0; i < len; i++){
+		  dest[i] = src[i];
+		}
+		dest[len] = '\0';
+	}
+	return ret;
+}
+#else
 size_t strlcpy(char *, const char *, size_t);
 #endif
+#endif
+
 #ifndef __HAVE_ARCH_STRSCPY
 ssize_t __must_check strscpy(char *, const char *, size_t);
 #endif
@@ -38,9 +78,31 @@ extern char * strncat(char *, const char *, __kernel_size_t);
 #ifndef __HAVE_ARCH_STRLCAT
 extern size_t strlcat(char *, const char *, __kernel_size_t);
 #endif
+
+// strcmp()
 #ifndef __HAVE_ARCH_STRCMP
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) int strcmp(const char *,const char *);
+extern inline int strcmp(const char *cs, const char *ct)
+{
+	unsigned char c1, c2;
+
+	while (1) {
+		c1 = *cs++;
+		c2 = *ct++;
+		if (c1 != c2)
+			return c1 < c2 ? -1 : 1;
+		if (!c1)
+			break;
+	}
+	return 0;
+}
+#else
 extern int strcmp(const char *,const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNCMP
 extern int strncmp(const char *,const char *,__kernel_size_t);
 #endif
@@ -50,9 +112,24 @@ extern int strcasecmp(const char *s1, const char *s2);
 #ifndef __HAVE_ARCH_STRNCASECMP
 extern int strncasecmp(const char *s1, const char *s2, size_t n);
 #endif
+
+// strchr()
 #ifndef __HAVE_ARCH_STRCHR
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strchr(const char *,int);
+extern inline char *strchr(const char *s, int c)
+{
+  for (; *s != (char)c; ++s)
+    if (*s == '\0')
+      return NULL;
+  return (char *)s;
+}
+#else
 extern char * strchr(const char *,int);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRCHRNUL
 extern char * strchrnul(const char *,int);
 #endif
@@ -71,15 +148,53 @@ static inline __must_check char *strstrip(char *str)
 	return strim(str);
 }
 
+
+// strstr()
 #ifndef __HAVE_ARCH_STRSTR
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strstr(const char *, const char *);
+extern inline char *strstr(const char *s1, const char *s2)
+{
+	size_t l1, l2;
+
+	l2 = strlen(s2);
+	if (!l2)
+		return (char *)s1;
+	l1 = strlen(s1);
+	while (l1 >= l2) {
+		l1--;
+		if (!memcmp(s1, s2, l2))
+			return (char *)s1;
+		s1++;
+	}
+	return NULL;
+}
+#else
 extern char * strstr(const char *, const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNSTR
 extern char * strnstr(const char *, const char *, size_t);
 #endif
+
+// strlen()
 #ifndef __HAVE_ARCH_STRLEN
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) __kernel_size_t strlen(const char *);
+extern inline size_t strlen(const char *s)
+{
+  const char *sc;
+  for (sc = s; *sc != '\0'; ++sc){}
+  return sc - s;
+}
+#else
 extern __kernel_size_t strlen(const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNLEN
 extern __kernel_size_t strnlen(const char *,__kernel_size_t);
 #endif
@@ -108,9 +223,26 @@ extern void * memmove(void *,const void *,__kernel_size_t);
 #ifndef __HAVE_ARCH_MEMSCAN
 extern void * memscan(void *,int,__kernel_size_t);
 #endif
+
+// memcmp()
 #ifndef __HAVE_ARCH_MEMCMP
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) int memcmp(const void *,const void *,__kernel_size_t);
+extern inline int memcmp(const void *cs, const void *ct, size_t count)
+{
+	const unsigned char *su1, *su2;
+	int res = 0;
+
+	for (su1 = cs, su2 = ct; 0 < count; ++su1, ++su2, count--)
+		if ((res = *su1 - *su2) != 0)
+			break;
+	return res;
+}
+#else
 extern int memcmp(const void *,const void *,__kernel_size_t);
 #endif
+#endif
+
 #ifndef __HAVE_ARCH_MEMCHR
 extern void * memchr(const void *,int,__kernel_size_t);
 #endif
diff --git a/init/main.c b/init/main.c
index b0c9d6facef9..4915a089f25c 100644
--- a/init/main.c
+++ b/init/main.c
@@ -56,6 +56,7 @@
 #include <linux/debugobjects.h>
 #include <linux/lockdep.h>
 #include <linux/kmemleak.h>
+#include <linux/memorizer.h>
 #include <linux/pid_namespace.h>
 #include <linux/device.h>
 #include <linux/kthread.h>
diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8ab827c..08484cc82b39 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -76,6 +76,7 @@
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
 #include <linux/kcov.h>
+#include <linux/memorizer.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -507,6 +508,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (err)
 		goto free_stack;
 
+    memorizer_stack_page_alloc(tsk);
+
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we're under
@@ -1979,6 +1982,9 @@ long _do_fork(unsigned long clone_flags,
 	} else {
 		nr = PTR_ERR(p);
 	}
+
+	//memorizer_fork(p,nr);
+
 	return nr;
 }
 
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index e57980845549..90615773c329 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -1,4 +1,6 @@
 
+KASAN_SANITIZE := n
+
 # Do not instrument the tracer itself:
 
 ifdef CONFIG_FUNCTION_TRACER
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index d7449783987a..a38baf5fc7b3 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -42,6 +42,7 @@
 #include <linux/fs.h>
 #include <linux/trace.h>
 #include <linux/sched/rt.h>
+#include <linux/memorizer.h>
 
 #include "trace.h"
 #include "trace_output.h"
@@ -2359,7 +2360,7 @@ trace_function(struct trace_array *tr,
 	struct ring_buffer_event *event;
 	struct ftrace_entry *entry;
 
-	event = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),
+    event = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),
 					    flags, pc);
 	if (!event)
 		return;
diff --git a/lib/Kconfig b/lib/Kconfig
index 260a80e313b9..d2084c7a3044 100644
--- a/lib/Kconfig
+++ b/lib/Kconfig
@@ -550,4 +550,11 @@ config STACKDEPOT
 config SBITMAP
 	bool
 
+config INLINE_LIBS
+	bool "Force inlining of library functions (strlen, memcmp, etc)"
+	default y
+	help
+	  Forces gcc to inline calls to some library functions. This was
+	  added to Memorizer Linux so that these stateless lib functions do
+	  not need to exclusively live in one compartment.
 endmenu
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index eb9e9a7870fa..c8913615976b 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -716,6 +716,24 @@ source "lib/Kconfig.kmemcheck"
 
 source "lib/Kconfig.kasan"
 
+config MEMORIZER
+	bool "Memorizer: kernel object lifetime access tracing"
+	depends on KASAN 
+        help 
+          Enables memory allocation and tracing tool that combines compiler
+          instrumentation on all loads/stores with embedded hooks for
+          allocations to track lifetime access patterns for kernel memory
+          objects. 
+
+config MEMORIZER_STATS
+	bool "Memorizer stats: Enable the statistic summary for Memorizer"
+	depends on MEMORIZER
+		help
+		  Enables the statistic summary including the number of accesses and
+		  number of shadow object allocated for Memorizer which will slow down the
+		  performance.
+
+
 endmenu # "Memory Debugging"
 
 config ARCH_HAS_KCOV
diff --git a/lib/test_kasan.c b/lib/test_kasan.c
index fbdf87920093..8047d9216941 100644
--- a/lib/test_kasan.c
+++ b/lib/test_kasan.c
@@ -19,6 +19,8 @@
 #include <linux/string.h>
 #include <linux/uaccess.h>
 #include <linux/module.h>
+#include <linux/kasan.h>
+#include <linux/memorizer.h>
 
 /*
  * Note: test functions are marked noinline so that their names appear in
@@ -441,6 +443,7 @@ static noinline void __init use_after_scope_test(void)
 
 static int __init kmalloc_tests_init(void)
 {
+#if 0
 	kmalloc_oob_right();
 	kmalloc_oob_left();
 	kmalloc_node_oob_right();
@@ -465,6 +468,10 @@ static int __init kmalloc_tests_init(void)
 	ksize_unpoisons_memory();
 	copy_user_test();
 	use_after_scope_test();
+#endif
+	memorizer_init_from_driver();
+	__memorizer_print_events(10);
+	/* error statement will unload the module for fast extra checking */
 	return -EAGAIN;
 }
 
diff --git a/mm/Makefile b/mm/Makefile
index 295bd7a9f76b..3e645e7caa3d 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -69,6 +69,7 @@ obj-$(CONFIG_SLAB) += slab.o
 obj-$(CONFIG_SLUB) += slub.o
 obj-$(CONFIG_KMEMCHECK) += kmemcheck.o
 obj-$(CONFIG_KASAN)	+= kasan/
+obj-$(CONFIG_MEMORIZER)	+= memorizer/
 obj-$(CONFIG_FAILSLAB) += failslab.o
 obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
 obj-$(CONFIG_MEMTEST)		+= memtest.o
diff --git a/mm/bootmem.c b/mm/bootmem.c
index e8a55a3c9feb..8214e0dcd5a2 100644
--- a/mm/bootmem.c
+++ b/mm/bootmem.c
@@ -17,6 +17,7 @@
 #include <linux/bug.h>
 #include <linux/io.h>
 #include <linux/bootmem.h>
+#include <linux/memorizer.h>
 
 #include "internal.h"
 
@@ -589,6 +590,7 @@ static void * __init alloc_bootmem_bdata(struct bootmem_data *bdata,
 		 * are never reported as leaks.
 		 */
 		kmemleak_alloc(region, size, 0, 0);
+		memorizer_alloc_bootmem(_RET_IP_, region, size);
 		return region;
 	}
 
diff --git a/mm/filemap.c b/mm/filemap.c
index 3f9afded581b..6776302d72bf 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1322,6 +1322,7 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 			gfp_mask &= ~__GFP_FS;
 
 		page = __page_cache_alloc(gfp_mask);
+		
 		if (!page)
 			return NULL;
 
@@ -1341,6 +1342,10 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 				goto repeat;
 		}
 	}
+	
+	// This function allocates a single page. We can reuse the below
+	// Memorizer hook with order 0 to track 1 page.
+	memorizer_alloc_pages(_RET_IP_, page, 0, gfp_mask);
 
 	return page;
 }
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index b2a0cff2bb35..f0e568e8a949 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -35,6 +35,7 @@
 #include <linux/types.h>
 #include <linux/vmalloc.h>
 #include <linux/bug.h>
+#include <linux/memorizer.h>
 
 #include "kasan.h"
 #include "../slab.h"
@@ -302,6 +303,8 @@ static __always_inline void check_memory_region_inline(unsigned long addr,
 {
 	if (unlikely(size == 0))
 		return;
+	
+	memorizer_mem_access(addr, size, write, ret_ip);
 
 	if (unlikely((void *)addr <
 		kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
@@ -322,6 +325,140 @@ static void check_memory_region(unsigned long addr,
 	check_memory_region_inline(addr, size, write, ret_ip);
 }
 
+bool kasan_obj_alive(const void *p, unsigned int size)
+{
+	if (unlikely((void *)p <
+		kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
+		return false;
+    }
+	if (likely(!memory_is_poisoned(p, size)))
+		return true;
+    return false;
+}
+
+/* Memorizer-introduced function to classify an access based on the
+   metadata in shadow space made available by KASAN. It returns the
+   shadow value type that it finds. See kasan.h for the possible
+   values. With the current design, it will return 0x00 if the obj
+   is larger than a page. This might make it unsuitable for heap
+   objects, but for stacks and globals it should be very accurate.
+   Now deprecated, see new implementation below.*/
+u8 detect_access_kind(void * p){
+
+    /* get shadow info for access address */
+    u8 shadow_val = *(u8 *)kasan_mem_to_shadow(p);
+    const void *first_poisoned_addr = p;
+
+    /* We now search for a shadow value. We search both forwards and
+       backwards without leaving the current page so we don't trigger
+       any invalid accesses. This may fail if there really is an obj
+       larger than a page, but for now we will accept these as losses.
+       That should be very rare for stacks/globals. A possible
+       extension is searching beyond 1 page, but first checking to see
+       if that will be valid.  */
+    
+    // Calculate the page-aligned address we are on
+    void * p_aligned = (long) p & (~((1 << PAGE_SHIFT) - 1));
+    
+    // Calculate the max forwards search distance
+    long search_size = (long) (p_aligned + PAGE_SIZE - p);
+    
+    // Search forwards
+    while (shadow_val < KASAN_SHADOW_SCALE_SIZE && first_poisoned_addr < p + search_size) {
+        first_poisoned_addr += KASAN_SHADOW_SCALE_SIZE;
+        shadow_val = *(u8 *)kasan_mem_to_shadow(first_poisoned_addr);
+    }
+
+    // If no hit, search backwards too. Stay higher than p_aligned
+    first_poisoned_addr = p;
+    while (shadow_val < KASAN_SHADOW_SCALE_SIZE && first_poisoned_addr > (p_aligned + KASAN_SHADOW_SCALE_SIZE)) {
+        first_poisoned_addr -= KASAN_SHADOW_SCALE_SIZE;
+        shadow_val = *(u8 *)kasan_mem_to_shadow(first_poisoned_addr);
+    }
+
+    return shadow_val;
+}
+
+// Another variant of this logic. Still debugging.
+u8 detect_access_kind_alt(void * p){
+
+  // Calculate page-aligned address
+  void * p_aligned = (unsigned long) p & (~((1 << PAGE_SHIFT) - 1));
+
+  // Initialize shadow pointer and current shadow value
+  u8* shadow_ptr = kasan_mem_to_shadow(p_aligned);
+  u8 shadow_val = *shadow_ptr;
+
+  // Set maximum search distance
+  u8* search_max = kasan_mem_to_shadow(p_aligned + 1*PAGE_SIZE);
+  
+  /* Search until we (1) find a valid shadow type identifier, (2)
+     exceed the max search distance, or (3) would go beyond end of
+     shadow space.
+     Note that shadow values that are nonzero but less than
+     KASAN_SHADOW_SCALE encode a partial red zone, and you need
+     to look at the next byte to get the kind. */
+  
+  while (shadow_val < KASAN_SHADOW_SCALE_SIZE &&
+	 shadow_ptr < search_max &&
+	 shadow_ptr < (void *)KASAN_SHADOW_END){
+    shadow_ptr++;
+    shadow_val = *shadow_ptr;
+  }
+  
+  return shadow_val;
+}
+
+enum AllocType kasan_obj_type(const void *p, unsigned int size)
+{
+    /* If we are below the Kernel address space */
+	if (p < kasan_shadow_to_mem((void *)KASAN_SHADOW_START)) {
+        /* our pointer is to page 0... null ptr */
+		if ((unsigned long)p < PAGE_SIZE)
+            return MEM_BUG;
+        /* our pointer is in 0 to User space end addr range  */
+		else if ((unsigned long)p < TASK_SIZE)
+            return MEM_USER;
+        /* crazy other stuff */
+		else
+            return MEM_BUG;
+    } else {
+        /* get shadow info for access address */
+        u8 shadow_val = detect_access_kind(p);
+        switch(shadow_val)
+        {
+            case KASAN_PAGE_REDZONE:
+                return MEM_ALLOC_PAGES;
+            case KASAN_KMALLOC_REDZONE:
+                return MEM_HEAP;
+            case KASAN_GLOBAL_REDZONE:
+                return MEM_GLOBAL;
+            case KASAN_STACK_LEFT:
+            case KASAN_STACK_MID:
+            case KASAN_STACK_RIGHT:
+            case KASAN_STACK_PARTIAL:
+                return MEM_STACK_PAGE;
+            default:
+	      
+	      /* There are some global objects that are not registered by KASAN.
+		 We can use the section that the address is in to classify it
+		 as an unknown global. We'll count anything in rodata, data or bss.
+		 Very strangely, only a few of the section starts and ends are defined
+		 constants. I wish they were all defined...
+		 For now, taking the beginning of rodata to the end of bss as unknown
+		 global. There are some other sections in there, but we shouldn't
+		 be getting data accesses to them. In the future we could split these
+		 down more finely if we want to. 
+	      */
+	      if (p >= __start_rodata && p <= __bss_start + 0x01fea000){
+		return MEM_GLOBAL;
+	      }
+	      
+	      return MEM_NONE;
+        }
+    }
+}
+
 void kasan_check_read(const void *p, unsigned int size)
 {
 	check_memory_region((unsigned long)p, size, false, _RET_IP_);
@@ -364,6 +501,7 @@ void kasan_alloc_pages(struct page *page, unsigned int order)
 {
 	if (likely(!PageHighMem(page)))
 		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+	//memorizer_alloc_pages(page,order);
 }
 
 void kasan_free_pages(struct page *page, unsigned int order)
@@ -372,6 +510,7 @@ void kasan_free_pages(struct page *page, unsigned int order)
 		kasan_poison_shadow(page_address(page),
 				PAGE_SIZE << order,
 				KASAN_FREE_PAGE);
+	//memorizer_free_pages(page,order);
 }
 
 /*
@@ -554,6 +693,7 @@ static void kasan_poison_slab_free(struct kmem_cache *cache, void *object)
 		return;
 
 	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
+	//memorizer_kfree(object, cache->object_size);
 }
 
 bool kasan_slab_free(struct kmem_cache *cache, void *object)
@@ -575,6 +715,9 @@ bool kasan_slab_free(struct kmem_cache *cache, void *object)
 	if (unlikely(!(cache->flags & SLAB_KASAN)))
 		return false;
 
+#ifdef FILTER_KASAN
+    return false;
+#endif
 	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
 	quarantine_put(get_free_info(cache, object), cache);
 	return true;
@@ -601,6 +744,9 @@ void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
+#ifdef FILTER_KASAN
+    return;
+#endif
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 }
@@ -626,6 +772,7 @@ void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 	kasan_unpoison_shadow(ptr, size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_PAGE_REDZONE);
+	//memorizer_alloc(ptr, size);
 }
 
 void kasan_krealloc(const void *object, size_t size, gfp_t flags)
@@ -662,6 +809,7 @@ void kasan_kfree_large(const void *ptr)
 
 	kasan_poison_shadow(ptr, PAGE_SIZE << compound_order(page),
 			KASAN_FREE_PAGE);
+	//memorizer_kfree(ptr);
 }
 
 int kasan_module_alloc(void *addr, size_t size)
@@ -707,6 +855,11 @@ static void register_global(struct kasan_global *global)
 	kasan_poison_shadow(global->beg + aligned_size,
 		global->size_with_redzone - aligned_size,
 		KASAN_GLOBAL_REDZONE);
+
+	memorizer_register_global(global->beg, global->size);
+	int written = sprintf(global_table_ptr, "%p %d %s %s\n", global -> beg,
+			      global -> size, global -> name, global -> module_name);
+	global_table_ptr += written;
 }
 
 void __asan_register_globals(struct kasan_global *globals, size_t size)
@@ -715,6 +868,7 @@ void __asan_register_globals(struct kasan_global *globals, size_t size)
 
 	for (i = 0; i < size; i++)
 		register_global(&globals[i]);
+
 }
 EXPORT_SYMBOL(__asan_register_globals);
 
@@ -780,6 +934,7 @@ void __asan_poison_stack_memory(const void *addr, size_t size)
 	 */
 	kasan_poison_shadow(addr, round_up(size, KASAN_SHADOW_SCALE_SIZE),
 			    KASAN_USE_AFTER_SCOPE);
+	memorizer_kfree(_RET_IP_, addr);
 }
 EXPORT_SYMBOL(__asan_poison_stack_memory);
 
@@ -787,6 +942,7 @@ EXPORT_SYMBOL(__asan_poison_stack_memory);
 void __asan_unpoison_stack_memory(const void *addr, size_t size)
 {
 	kasan_unpoison_shadow(addr, size);
+	memorizer_stack_alloc(_RET_IP_, addr, size);
 }
 EXPORT_SYMBOL(__asan_unpoison_stack_memory);
 
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 1c260e6b3b3c..93a90e7de7f9 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -13,6 +13,10 @@
 #define KASAN_KMALLOC_FREE      0xFB  /* object was freed (kmem_cache_free/kfree) */
 #define KASAN_GLOBAL_REDZONE    0xFA  /* redzone for global variable */
 
+// Memorizer-added exports from KASAN
+bool in_kernel_space(void * p);
+u8 detect_access_kind(void * p);
+
 /*
  * Stack redzone shadow values
  * (Those are compiler's ABI, don't change them)
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index f479365530b6..eb53f4a3d4e0 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -310,6 +310,8 @@ void kasan_report(unsigned long addr, size_t size,
 
 	kasan_report_error(&info);
 }
+// Nick temporarily exporting the report function
+// EXPORT_SYMBOL(kasan_report);
 
 
 #define DEFINE_ASAN_REPORT_LOAD(size)                     \
diff --git a/mm/memblock.c b/mm/memblock.c
index 7608bc305936..c191ec30fad4 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -499,6 +499,7 @@ static void __init_memblock memblock_insert_region(struct memblock_type *type,
 	memblock_set_region_node(rgn, nid);
 	type->cnt++;
 	type->total_size += size;
+	memorizer_memblock_alloc(base,size);
 }
 
 /**
diff --git a/mm/memorizer/FunctionHashTable.c b/mm/memorizer/FunctionHashTable.c
new file mode 100644
index 000000000000..4284fe566c53
--- /dev/null
+++ b/mm/memorizer/FunctionHashTable.c
@@ -0,0 +1,256 @@
+#ifndef FUNCTIONHASHTABLE_C
+#define FUNCTIONHASHTABLE_C
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include "FunctionHashTable.h"
+#include "memalloc.h"
+#include "kobj_metadata.h"
+#include "memorizer.h"
+
+
+#define NUMBUCKS 1000000
+
+DEFINE_RWLOCK(fht_rwlock);
+
+/* object cache for Edge Buckets */
+static struct kmem_cache *eb_cache;
+
+/* Initialize the FHT global data */
+void func_hash_tbl_init(void)
+{
+	//eb_cache = KMEM_CACHE(EdgeBucket, SLAB_PANIC);
+}
+
+struct FunctionHashTable * create_function_hashtable() {
+
+	struct FunctionHashTable * h = memalloc(sizeof(struct FunctionHashTable));
+	h -> buckets = zmemalloc(NUM_BUCKETS * sizeof(struct EdgeBucket *));
+	h -> number_buckets = NUM_BUCKETS;
+
+	return h;
+}
+
+/* Check whether arguments have been pushed to the stack */
+bool push_args_to_stack(struct pt_regs *pt_regs, struct memorizer_kobj*
+last_edge_frame_kobj) 
+{
+	/**
+	 * The caller_bp might be 0, for example, entry_SYSCALL_64_fastpath ->
+	 * sys_dup2 where entry_SYSCALL_64_fastpath the first entry function in 
+	 * the kernel, then its bp could be 0.
+	 */
+	if (last_edge_frame_kobj != NULL) {
+		/*
+		 * Before stack_trace is turned on, the caller's alloc type would be 
+		 * MEM_STACK_PAGE. In this case, we don't know the caller's stack 
+		 * frame shadow kobject information.
+		 */
+		if (last_edge_frame_kobj->alloc_type == MEM_STACK_FRAME) {
+			/**
+			 * There are cases that caller rbp and callee rbp's difference is 
+			 * larger than THREAD_SIZE, for exmaple, ret_from_intr -> do_IRQ, 
+			 * we need to understand how interrupt handle their rbp.
+			 */
+			if(abs(last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10) < THREAD_SIZE) {
+				/**
+				 * If the caller's sp is not equal to callee's bp, we should 
+				 * allocate an argument kobj.
+				 */ 
+				if(last_edge_frame_kobj->va_ptr != pt_regs->bp + 0x10) {
+					return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+/* Update shadow stack frame and argument kobj's meta data and the lookup table. */
+void update_stack_kobj(struct EdgeBucket *new_bucket, struct pt_regs *pt_regs) 
+{
+	/** Interrupt confuses the kobj call stack, so we stop tracing interrupt for now */
+#if defined(__x86_64__)
+	/**
+	 * If we find a <caller, callee> pair exists, then update the function's kobj
+	 * and argument kobj metadata type and lookup table. 
+	 */
+	lt_insert_kobj(new_bucket->kobj);
+
+	/**
+	 * Update the function's argument kobj metadata and lookup table in case of 
+	 * variable length arguments 
+	 */
+	uintptr_t caller_bp = *(uintptr_t *)pt_regs->bp;
+	struct memorizer_kobj *last_edge_frame_kobj = lt_get_kobj(caller_bp);
+
+	if (push_args_to_stack(pt_regs, last_edge_frame_kobj)) {
+		new_bucket->kobj->args_kobj->size = last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10;
+		lt_insert_kobj(new_bucket->kobj->args_kobj); 
+	}
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+}
+
+/* Create shadow stack frame and argument kobj's meta data and update the lookup table. */
+void create_stack_kobj(uintptr_t to, struct EdgeBucket *new_bucket, struct pt_regs *pt_regs) 
+{
+#if defined(__x86_64__)
+	/* Allocate callee's stack frame */
+	new_bucket->kobj = create_kobj(to, pt_regs->sp, 
+			0x10 + pt_regs->bp - pt_regs->sp, MEM_STACK_FRAME);
+
+	/* Allocate arg_kobj and its size is the difference of the caller's sp and callee's bp*/
+	uintptr_t caller_bp = *(uintptr_t *)pt_regs->bp;
+	struct memorizer_kobj *last_edge_frame_kobj = lt_get_kobj(caller_bp);
+	if (push_args_to_stack(pt_regs, last_edge_frame_kobj)) {
+		new_bucket->kobj->args_kobj = create_kobj(to, pt_regs->bp + 0x10, 
+				last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10, MEM_STACK_ARGS);
+	} else {
+		/* If no arguments are pushed to the stack, create an argument kobj with size 0 */
+		new_bucket->kobj->args_kobj = create_kobj(to, pt_regs->bp + 0x10, 
+				0, MEM_STACK_ARGS);
+	}
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+}
+
+/**
+ * This function puts the <from, to, stack frame kobj, function argument kobj>
+ * tuple into the hash table. When stack_trace_on is disabled, stack frame kobj
+ * points to NULL value. If a stack frame kobj already exists, we use allocation
+ * promotion to overide the existing one.
+ * @ht: hash table pointer
+ * @from: caller's virtual address
+ * @to: callee's virtual address
+ * @pt_regs: a structure for base pointer and stack pointer, calculated at
+ * cyg_profile_function_enter.
+ * @stack_trace_on: if turned on, allocate the stack frame kobj and argument
+ * kobj.
+ */
+void cfg_update_counts(struct FunctionHashTable * ht, uintptr_t from, uintptr_t to, 
+		struct pt_regs *pt_regs, bool stack_trace_on)
+{
+	// pr_crit("Entering: %p -> %p", from,to);
+	// Compute index by xoring the from and to fields then masking away high bits
+	int index = (from ^ to) & (ht -> number_buckets - 1);
+
+	// Search for edge. If found, increment count and return
+	struct EdgeBucket * search = ht -> buckets[index];
+	struct EdgeBucket * prev = NULL;
+	while (search != NULL) {
+		if (search -> from == from && search -> to == to) {
+			atomic_long_inc(&search -> count);
+			/**
+			 * Need to check if search-kobj is null or not. If a bucket is
+			 * created before we enable stack trace, then we will get a null for
+			 * kobj.
+			 */
+			if (stack_trace_on && search->kobj != NULL) {
+				update_stack_kobj(search, pt_regs);
+			} else if (stack_trace_on && search->kobj == NULL) {
+				/**
+				 * If a (caller, callee) pair exists before we enabled the stack trace,
+				 * then we need to create a new stack kobj for this frame.
+				 */
+				create_stack_kobj(to, search, pt_regs);
+			}
+			return;
+		} else {
+			// Collision, loop through the linked list
+			prev = search;
+			search = search -> next;
+		}
+	}
+
+
+	/**
+	 * If we can't find the match, there are two scenarios:
+	 * 1. The hash bucket does not have an entry yet.
+	 * 2. The hash bucket already have an entry (which is a colloision)
+	 * and prev points to that location. The new entry will be appended to 
+	 * the end of the linked list.
+	 */
+	write_lock(&fht_rwlock);
+	struct EdgeBucket *new_bucket = NULL;
+	if (ht -> buckets[index] == NULL) {
+		// 1) Create new bucket if empty root
+		ht -> buckets[index] = memalloc(sizeof(struct EdgeBucket));
+		new_bucket = ht -> buckets[index];
+	} else if (prev -> next == NULL) {
+		// 2) Insert item onto end of existing chain
+		prev -> next = memalloc(sizeof(struct EdgeBucket));
+		new_bucket = prev -> next;
+	}
+  
+
+	// Update bucket information
+	//ht -> buckets[index] = kmem_cache_alloc(eb_cache, GFP_ATOMIC);
+	new_bucket -> from = from;
+	new_bucket -> to = to;
+	atomic_long_set(&ht -> buckets[index] -> count, 1);
+	new_bucket -> next = NULL;
+	new_bucket -> kobj = NULL;
+  
+	// Create new stack frame kobj and arguments kobj for callee
+	if (stack_trace_on) {
+		create_stack_kobj(to, new_bucket, pt_regs);
+	}
+	write_unlock(&fht_rwlock);
+
+	return;
+}
+
+// Write hashtable contents (edge hits) to file
+void console_print(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;    
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			pr_crit("%lx %lx %ld\n", b -> from, b -> to, b -> count);
+			b = b -> next;
+		}
+	}  
+}
+
+// Clear the entries
+void cfgmap_clear(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			struct EdgeBucket * prev = b;
+			b = b -> next;
+			memset(prev,NULL,sizeof(struct EdgeBucket));
+			//kmem_cache_free(eb_cache, prev);
+		}
+		ht -> buckets[index] = NULL;
+	}
+}
+
+// Release all allocated memory
+void destroy_function_hashtable(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			struct EdgeBucket * prev = b;
+			b = b -> next;
+			memset(prev,NULL,sizeof(struct EdgeBucket));
+			//kmem_cache_free(eb_cache, prev);
+		}
+	}
+	kfree(ht->buckets);
+	kfree(ht);
+}
+
+#endif
diff --git a/mm/memorizer/FunctionHashTable.h b/mm/memorizer/FunctionHashTable.h
new file mode 100644
index 000000000000..26703851d97b
--- /dev/null
+++ b/mm/memorizer/FunctionHashTable.h
@@ -0,0 +1,43 @@
+// FunctionHashTable is a lightweight hashtable implementation for tracking 
+// call/return edges in uSCOPE.
+
+#ifndef FUNCTIONHASHTABLE_H
+#define FUNCTIONHASHTABLE_H
+
+#include <linux/types.h>
+
+#define NUM_BUCKETS (_AC(1,UL) << 19)
+
+struct EdgeBucket {
+  uintptr_t from, to;
+  atomic_long_t count;
+  struct memorizer_kobj *kobj;
+  struct EdgeBucket * next;
+};
+
+struct FunctionHashTable {
+  struct EdgeBucket ** buckets;
+  int number_buckets;
+  int full_buckets;
+  int stored_items;
+};
+
+// Initialization for the table data structures
+void func_hash_tbl_init(void);
+
+// Create a new FunctionHashTable
+struct FunctionHashTable * create_function_hashtable(void);
+
+// Update the counts for an edge, adding to table if not already there
+void cfg_update_counts(struct FunctionHashTable * ht, uintptr_t from, uintptr_t to, struct pt_regs *pt_regs, bool stack_trace_on);
+
+// Clear entries and reset
+void cfgmap_clear(struct FunctionHashTable * ht);
+
+// Print directly to console TODO: this is just temp hack for check
+void console_print(struct FunctionHashTable * ht);
+
+// Release memory
+void destroy_function_hashtable(struct FunctionHashTable * ht);
+
+#endif
diff --git a/mm/memorizer/Makefile b/mm/memorizer/Makefile
new file mode 100644
index 000000000000..56cade5e1c31
--- /dev/null
+++ b/mm/memorizer/Makefile
@@ -0,0 +1,11 @@
+KASAN_SANITIZE := n
+KCOV_INSTRUMENT := n
+
+CFLAGS_REMOVE_memorizer.o = -pg
+# Function splitter causes unnecessary splits in __asan_load1/__asan_store1
+# see: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63533
+#
+# FIXME NDD: These flags were copied from kasan, I'm not sure if they are needed. 
+CFLAGS_memorizer.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+
+obj-y := memorizer.o kobj_metadata.o FunctionHashTable.o stats.o memalloc.o
diff --git a/mm/memorizer/README.md b/mm/memorizer/README.md
new file mode 100644
index 000000000000..8d8a34c6623e
--- /dev/null
+++ b/mm/memorizer/README.md
@@ -0,0 +1,81 @@
+# Memorizer
+Memorizer is a tool to track all the allocs, accesses and frees for every object inside the kernel and output them as a CAPMAP, to be used for further analysis.
+
+# Registration
+The memorizer, if compiled into the kernel, is initialized by calling memorizer_init() from inside init/main.c 
+The init function sets up the data structures to be used for keeping track of the events and kernel objects 
+
+# Memorizer Hooks
+The memorizer uses hooks to track events within the kernel. 
+Allocs and Frees are hooked by adding function hooks into the individual allocators, present of slub.c (We're only concerned about slub for now since that what most of the current systems use, although extending it to other allocators (SLAB and SLOB) should be trivial.
+Loads and Stores(Accesses) are tracked by using KASAN's instrumentation for Loads and Stores. It instruments __asan_load*(addr), and __asan_load*(addr) at the time of kernel compilation.
+
+The following table gives a summary of all the Hooks in Memorizer(Needs Revising): 
+
+Hook | Type | Location | Recording Function | Description
+--- | --- | --- | --- | ---
+kmem_cache_alloc() | Function Call | slub.c | __memorizer_kmalloc() | Records kmem_cache_alloc()
+kmalloc() | Function Call | slub.c | __memorizer_kmalloc() | Records kmalloc()
+page_alloc() | Funtion Call | page_alloc.c | __memorizer_kmalloc() | Records page_alloc() (NEEDS FIXING)
+globals | Function Call | kasan.c (Check) | __memorizer_kmalloc() | Records globals (NEED to record Alloc Addr)
+loads | KASAN Instrumentation | kasan.c | memorizer_mem_access() | Records loads
+store | KASAN Instrumemtion | kasan.c | memorizer_mem_access() | Records Stores
+kmem_cache_free() | Function Call | slub.c | memorizer_free_kobj() | Records kmem_cache_free()
+kfree() | Function Call | slub.c | memorizer_free_kobj() | Records the kfree()
+
+# CAPMAP 
+The memorizer records event data and outputs it in the form of a CAPMAP. A CAPMAP has two types of entries:
+
+## Alloc/Free Information
+These are denoted by non indented lines. Each line represents a kernel object and the information recorded is as follows:
+lloc IP
+* PID
+* Size
+* Alloc Jiffies
+* Free Jiffies
+* Free IP 
+* Common name for the Process
+    
+## Access Information
+These are denoted by indented lines. Each line represents a memory location that the current memory object has accesses. The information recorded is as follows:
+* Access IP 
+* Access PID
+* Number of Writes
+* Number of Reads
+
+# DebugFS layout
+The memorizer uses the debugfs to communicate between the Kernel and User Space. The DebugFS interface is present in the /sys/kernel/debug/memorizer directory and provides controls for the memorizer. The following section details the use of each file in the DebugFS directory.
+
+## memorizer_enabled
+This turns the memorizer On or Off. When the memorizer is disabled, it doesn't track any information. When enabled, it only tracks the allocs and frees. It is enabled by default during bootup.
+
+Enabling the memorizer:
+```
+echo 1 > memorizer_enabled
+```
+Disabling the memorizer:
+```
+echo 0 > memorizer_enabled
+```
+
+## memorizer_log_access
+This enables/disables the tracking for accesses(loads and stores). It is disabled by default during bootup. To ensure complete tracking, both memorizer_enabled and memorizer_log_access should be enabled.  
+
+Enabling access logging:
+```
+echo 1 > memorizer_log_access
+```
+Disabling access logging
+```
+echo 0 > memorizer_log_access
+```
+
+## kmap
+The CAPMAP generated can be printed out by reading the kmap file present in the directory. This is similar in design to the trace file in ftrace. A callback has been implemented in the kernel that prints out the kmap to the stdio. The CAPMAP can be saved to the file as follows:
+```
+cat kmap > <path_to_file>
+```
+
+The rest of the features can be controlled the same way and their names are self explanatory. The commands are therefore omitted for brevity.
+
+
diff --git a/mm/memorizer/event_structs.h b/mm/memorizer/event_structs.h
new file mode 100644
index 000000000000..3a74f2e83fc5
--- /dev/null
+++ b/mm/memorizer/event_structs.h
@@ -0,0 +1,33 @@
+/* This file describes the structs to be used to describe the events happening inside the kernel:
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * 4. FORKS
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+//enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Read = 0xcc, Memorizer_Mem_Write = 0xdd, Memorizer_Fork = 0xee};
+enum AccessType {
+    Memorizer_READ=0,
+    Memorizer_WRITE,
+    Memorizer_Mem_Alloc,
+    Memorizer_Mem_Free,
+    Memorizer_Fork,
+    Memorizer_NULL
+};
+
+struct memorizer_kernel_event {
+	enum AccessType event_type;
+	pid_t		pid;
+    union EvntData {
+        struct nonfork {
+            uintptr_t	src_va_ptr;
+            uintptr_t	va_ptr;
+            uint64_t	event_size;
+        }et;
+        char comm[TASK_COMM_LEN];
+    }data;
+};
diff --git a/mm/memorizer/kobj_metadata.c b/mm/memorizer/kobj_metadata.c
new file mode 100644
index 000000000000..cf257513a581
--- /dev/null
+++ b/mm/memorizer/kobj_metadata.c
@@ -0,0 +1,478 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2016, The Board of Trustees of the University of Illinois.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2016, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  kobj_metadata.c
+ *
+ *    Description:  Metadata tracking for all kobject allocations. Includes
+ *		    types for metadata as well as data structure
+ *		    implementations.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/gfp.h>
+#include <linux/atomic.h>
+#include <linux/slab.h>
+#include <linux/jiffies.h>
+#include <linux/seq_file.h>
+#include <linux/memorizer.h>
+
+#include "kobj_metadata.h"
+#include "memorizer.h"
+#include "stats.h"
+#include "memalloc.h"
+
+#define ALLOC_CODE_SHIFT    59
+#define ALLOC_INDUCED_CODE	(_AC(MEM_INDUCED,UL) << ALLOC_CODE_SHIFT)
+
+/* atomic object counter */
+static atomic_long_t global_kobj_id = ATOMIC_INIT(0);
+
+/* RW Spinlock for access to table */
+DEFINE_RWLOCK(lookup_tbl_rw_lock);
+
+static struct lt_l3_tbl kobj_l3_tbl;
+static struct lt_pid_tbl pid_tbl;
+
+/* Emergency Pools for l1 + l2 pages */
+#define NUM_EMERGENCY_PAGES 200
+struct pages_pool {
+    uintptr_t base;  /* pointer to array of l1/l2 pages */
+    size_t next;        /* index of next available */
+    size_t entries;     /* number of entries to last page */
+    size_t pg_size;     /* size of object for indexing */
+};
+
+//int test_and_set_bit(unsigned long nr, volatile unsigned long *addr);
+volatile unsigned long inlt;
+
+/**
+ * __lt_enter() - increment recursion counter for entry into memorizer
+ *
+ * The primary goal of this is to stop recursive handling of events. Memorizer
+ * by design tracks two types of events: allocations and accesses. Effectively,
+ * while tracking either type we do not want to re-enter and track memorizer
+ * events that are sources from within memorizer. Yes this means we may not
+ * track legitimate access of some types, but these are caused by memorizer and
+ * we want to ignore them.
+ */
+static inline int __lt_enter(void)
+{
+    return test_and_set_bit_lock(0,&inlt);
+}
+
+static __always_inline void __lt_exit(void)
+{
+    return clear_bit_unlock (0,&inlt);
+}
+
+/**
+ * get_pg_from_pool() --- get the next page from the pool
+ *
+ * @pool: the pool to get the next value
+ *
+ * desc: this should not care about the type, so the type info is put into the
+ * pages_pool struct so that we can do pointer arithmetic to find the next
+ * available entry. The pointer is going to be the next index * the size of the
+ * object, which is set on initializing the pool.
+ *
+ */
+uintptr_t get_pg_from_pool(struct pages_pool *pool)
+{
+    pr_info("Getting page from pool (%p). i=%d e=%d\n",
+            pool->base, pool->next, pool->entries);
+    if (pool->entries == pool->next)
+        return 0;
+    /* next * pg_size is the offset in bytes from the base of the pool */
+    return (uintptr_t) (pool->base + (pool->next++ * pool->pg_size));
+}
+
+struct lt_l1_tbl l1_tbl_pool[NUM_EMERGENCY_PAGES];
+struct pages_pool l1_tbl_reserve =
+{
+    .base = (uintptr_t) &l1_tbl_pool,
+    .next = 0,
+    .entries = NUM_EMERGENCY_PAGES,
+    .pg_size = sizeof(struct lt_l1_tbl)
+};
+
+struct lt_l2_tbl l2_tbl_pool[NUM_EMERGENCY_PAGES];
+struct pages_pool l2_tbl_reserve =
+{
+    .base = (uintptr_t) &l2_tbl_pool,
+    .next = 0,
+    .entries = NUM_EMERGENCY_PAGES,
+    .pg_size = sizeof(struct lt_l2_tbl)
+};
+
+/**
+ * tbl_get_l1_entry() --- get the l1 entry
+ * @addr:	The address to lookup
+ *
+ * Typical table walk starting from top to bottom.
+ *
+ * Return: the return value is a pointer to the entry in the table, which means
+ * it is a double pointer to the object pointed to by the region. To simplify
+ * lookup and setting this returns a double pointer so access to both the entry
+ * and the object in the entry can easily be obtained.
+ */
+static struct memorizer_kobj **tbl_get_l1_entry(uint64_t addr)
+{
+	struct memorizer_kobj **l1e;
+	struct lt_l1_tbl **l2e;
+	struct lt_l2_tbl **l3e;
+
+	/* Do the lookup starting from the top */
+	l3e = lt_l3_entry(&kobj_l3_tbl, addr);
+	if (!*l3e)
+		return NULL;
+	l2e = lt_l2_entry(*l3e, addr);
+	if (!*l2e)
+		return NULL;
+	l1e = lt_l1_entry(*l2e, addr);
+	if (!*l1e)
+		return NULL;
+	return l1e;
+}
+
+/**
+ * l1_alloc() --- allocate an l1 table
+ */
+static struct lt_l1_tbl * l1_alloc(void)
+{
+    struct lt_l1_tbl *l1_tbl;
+    int i = 0;
+
+    l1_tbl = memalloc(sizeof(struct lt_l1_tbl));
+    if (!l1_tbl) {
+        l1_tbl = (struct lt_l1_tbl *) get_pg_from_pool(&l1_tbl_reserve);
+        if (!l1_tbl) {
+            /* while in dev we want to print error and panic */
+            print_stats(KERN_CRIT);
+            panic("Failed to allocate L1 table for memorizer kobj\n");
+        }
+    }
+
+    /* Zero out the memory */
+    for (i = 0; i < LT_L1_ENTRIES; ++i)
+        l1_tbl->kobj_ptrs[i] = 0;
+
+    /* increment stats counter */
+    track_l1_alloc();
+
+    return l1_tbl;
+}
+
+/**
+ * l2_alloc() - alloc level 2 table
+ */
+static struct lt_l2_tbl * l2_alloc(void)
+{
+    struct lt_l2_tbl *l2_tbl;
+    int i = 0;
+
+    l2_tbl = memalloc(sizeof(struct lt_l2_tbl));
+    if (!l2_tbl) {
+        l2_tbl = (struct lt_l2_tbl *) get_pg_from_pool(&l2_tbl_reserve);
+        if (!l2_tbl) {
+            print_stats(KERN_CRIT);
+            panic("Failed to allocate L2 table for memorizer kobj\n");
+        }
+    }
+
+    /* Zero out the memory */
+    for (i = 0; i < LT_L2_ENTRIES; ++i)
+        l2_tbl->l1_tbls[i] = 0;
+
+    /* increment stats counter */
+    track_l2_alloc();
+
+    return l2_tbl;
+}
+
+/**
+ * l2_entry_may_alloc() - get the l2 entry and alloc if needed
+ * @l2_tbl:	pointer to the l2 table to look into
+ * @addr:		Pointer of the addr to index into the table
+ *
+ * Check if the l1 table exists, if not allocate.
+ */
+static struct lt_l1_tbl **l2_entry_may_alloc(struct lt_l2_tbl *l2_tbl, uintptr_t
+					     addr)
+{
+	unsigned long flags;
+	struct lt_l1_tbl **l2e;
+	l2e = lt_l2_entry(l2_tbl, addr);
+	if (unlikely(!*l2e))
+		*l2e = l1_alloc();
+	return l2e;
+}
+
+/**
+ * l3_entry_may_alloc() - get the l3 entry and alloc if needed
+ * @addr:		Pointer of the addr to index into the table
+ *
+ * Check if the l2 table exists, if not allocate.
+ */
+static struct lt_l2_tbl **l3_entry_may_alloc(uintptr_t addr)
+{
+	unsigned long flags;
+	struct lt_l2_tbl **l3e;
+	l3e = lt_l3_entry(&kobj_l3_tbl, addr);
+	if (unlikely(!*l3e))
+		*l3e = l2_alloc();
+	return l3e;
+}
+
+/**
+ *
+ */
+static bool is_tracked_obj(uintptr_t l1entry)
+{
+	return ((uint64_t) l1entry >> ALLOC_CODE_SHIFT) != (uint64_t)
+		MEM_INDUCED;
+}
+
+/**
+ * is_induced_obj() -
+ *
+ * Args:
+ *   @addr: the virtual address to check
+ *
+ * Description:
+ *	Return the code that is stored in the upper 5 bits of the pointer value.
+ *	This is stored when we detect that we've had an induced allocation. A
+ *	normally tracked allocation will have the value 0 and thus evaluate to
+ *	false.
+ */
+bool is_induced_obj(uintptr_t addr)
+{
+    struct memorizer_kobj **l1e = tbl_get_l1_entry(addr);
+    if (!l1e)
+        return false;
+    return ((uint64_t) *l1e >> ALLOC_CODE_SHIFT) == (uint64_t) MEM_INDUCED;
+}
+
+/**
+ * lt_remove_kobj() --- remove object from the table
+ * @addr: pointer to the beginning of the object
+ *
+ * This code assumes that it will only ever get a remove from the beginning of
+ * the kobj. TODO: check the beginning of the kobj to make sure.
+ *
+ * Return: the kobject at the location that was removed.
+ */
+struct memorizer_kobj * lt_remove_kobj(uintptr_t addr)
+{
+        struct memorizer_kobj **l1e, *kobj;
+        uintptr_t nextobj, l1entry = 0;
+
+    /*
+     * Get the l1 entry for the addr, if there is not entry then we not only
+     * haven't tracked the object, but we also haven't allocated a l1 page
+     * for the particular address
+     */
+    l1e = tbl_get_l1_entry(addr);
+    if (!l1e)
+        return NULL;
+
+    /* Setup the return: if it is an induced object then no kobj exists */
+    /* the code is in the most significant bits so shift and compare */
+    if (is_tracked_obj(*l1e)) {
+            kobj = *l1e;
+    } else {
+            kobj = NULL;
+    }
+
+	if(kobj)
+		nextobj = kobj->va_ptr + kobj->size;
+
+    /* For each byte in the object set the l1 entry to NULL */
+    while(nextobj>*l1e)
+    {
+            /* *free* the byte by setting NULL */
+            *l1e = 0;
+
+            /* move l1e to the next entry */
+            l1e = tbl_get_l1_entry(++addr);
+
+            /*
+             * we might get an object that ends at the end of a table and
+             * therefore the next call will fail to get the l1 table.
+             */
+            if(!l1e)
+                    break;
+    }
+    return kobj;
+}
+
+inline struct memorizer_kobj * lt_get_kobj(uintptr_t addr)
+{
+    struct memorizer_kobj **l1e = tbl_get_l1_entry(addr);
+    if (l1e && is_tracked_obj((uintptr_t)*l1e))
+        return *l1e;
+    return NULL;
+}
+
+/*
+ * handle_overalpping_insert() -- hanlde the overlapping insert case
+ * @addr:		the virtual address that is currently not vacant
+ * @l1e:	the l1 entry pointer for the addr
+ *
+ * There is some missing free's currently, it isn't clear what is causing them;
+ * however, if we assume objects are allocated before use then the most recent
+ * allocation will be viable for any writes to these regions so we remove the
+ * previous entry and set up its free times with a special code denoting it was
+ * evicted from the table in an erroneous fasion.
+ */
+static void handle_overlapping_insert(uintptr_t addr)
+{
+    unsigned long flags;
+    struct memorizer_kobj *obj = lt_get_kobj(addr);
+
+    if (!obj)
+        return;
+
+    /*
+     * Note we don't need to free because the object is in the free list and
+     * will get expunged later.
+     */
+    write_lock_irqsave(&obj->rwlock, flags);
+    obj->free_jiffies = get_ts();
+    obj->free_ip = 0xDEADBEEF;
+    write_unlock_irqrestore(&obj->rwlock, flags);
+}
+
+/**
+ * lt_insert_kobj() - insert kobject into the lookup table
+ * @kobj:	pointer to the kobj to insert
+ *
+ * For each virtual address in the range of the kobj allocation set the l1 table
+ * entry mapping for the virtual address to the kobj pointer. The function
+ * starts by getting the l2 table from the global l3 table. If it doesn't exist
+ * then allocates the table. The same goes for looking up the l1 table for the
+ * given addr. Once the particular l1 table is obtained for the start addr of the
+ * object, iterate through the table setting each entry of the object to the
+ * given kobj pointer.
+ */
+int __lt_insert(uintptr_t ptr, size_t size, uintptr_t metadata)
+{
+	struct lt_l1_tbl **l2e;
+	struct lt_l2_tbl **l3e;
+	uint64_t l1_i = 0;
+	uintptr_t addr = ptr;
+	uintptr_t kobjend = ptr + size;
+
+	while (addr < kobjend) {
+		/* Pointer to the l3 entry for addr and alloc if needed */
+		l3e = l3_entry_may_alloc(addr);
+
+		/* Pointer to the l2 entry for addr and alloc if needed */
+		l2e = l2_entry_may_alloc(*l3e, addr);
+
+		/*
+                 * Get the index for this addr for boundary on this l1 table;
+                 * however, TODO, this might not be needed as our table indices
+                 * are page aligned and it might be unlikely allocations are
+                 * page aligned and will not traverse the boundary of an l1
+                 * table. Note that I have not tested this condition yet.
+		 */
+		l1_i = lt_l1_tbl_index(addr);
+
+		while (l1_i < LT_L1_ENTRIES && addr < kobjend) {
+			/* get the pointer to the l1_entry for this addr byte */
+			struct memorizer_kobj **l1e = lt_l1_entry(*l2e,addr);
+
+			/* If it is not null then we are double allocating */
+			if (*l1e)
+				handle_overlapping_insert(addr);
+
+			/* insert object pointer in the table for byte addr */
+			*l1e = metadata;
+
+            /* Track end of the table and the object tracking */
+            addr += 1;
+			++l1_i;
+		}
+	}
+	return 0;
+}
+
+/**
+ * We create a unique label for each induced allocated object so that we can
+ * easily free. We insert a 5 bit code for the type with the MSB as 0 to make
+ * sure we don't have a false positive with a real address. We then make the 59
+ * least significatn bits a unique identifier for this obj. By inserting this
+ * way the free just finds all matching entries in the table.
+ */
+size_t d = 0;
+int lt_insert_induced(void * ptr, size_t size)
+{
+    uintptr_t label = ((uintptr_t) MEM_INDUCED << ALLOC_CODE_SHIFT) |
+        atomic_long_inc_return(&global_kobj_id);
+    __lt_insert(ptr, size, label);
+    return 1;
+}
+
+int lt_insert_kobj(struct memorizer_kobj *kobj)
+{
+        return __lt_insert(kobj->va_ptr, kobj->size, kobj);
+}
+
+void plt_insert(struct pid_obj pobj)
+{
+	// Insert into the PID Table based on the Key of the Object
+	pid_tbl.pid_obj_list[pobj.key] = pobj;
+}
+
+
+void __init lt_init(void)
+{
+	/* Zero the page dir contents */
+	memset(&kobj_l3_tbl, 0, sizeof(kobj_l3_tbl));
+	// Zero Out the Contents of the PID Table
+	memset(&pid_tbl, 0, sizeof(pid_tbl));
+	/* track that we statically allocated an l3 */
+	track_l3_alloc();
+}
diff --git a/mm/memorizer/kobj_metadata.h b/mm/memorizer/kobj_metadata.h
new file mode 100644
index 000000000000..b5efb282efab
--- /dev/null
+++ b/mm/memorizer/kobj_metadata.h
@@ -0,0 +1,270 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ * 
+ * University of Illinois/NCSA Open Source License 
+ *
+ * Copyright (C) 2016, The Board of Trustees of the University of Illinois.
+ * All rights reserved. 
+ *
+ * Developed by: 
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2016, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions: 
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers. 
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission. 
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
+ * THE SOFTWARE. 
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  kobj_metadata.h
+ *
+ *    Description:  Header file for metadata tracking functionality.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef _KOBJ_METADATA_H_
+#define _KOBJ_METADATA_H_
+
+#include <linux/kallsyms.h>
+#include <linux/rbtree.h>
+#include <linux/rwlock.h>
+#include <linux/sched.h>
+
+static char * alloc_type_str (enum AllocType AT)
+{
+	switch(AT)
+	{
+		case MEM_STACK:
+			return "STACK";
+		case MEM_STACK_FRAME:
+			return "STACK_FRAME";
+		case MEM_STACK_ARGS:
+			return "STACK_ARGS";
+		case MEM_STACK_PAGE:
+			return "STACK_PAGE";
+		case MEM_HEAP:
+			return "GEN_HEAP";
+		case MEM_UFO_HEAP:
+			return "UFO_HEAP";
+		case MEM_GLOBAL:
+			return "GLOBAL";
+		case MEM_KMALLOC:
+			return "KMALLOC";
+		case MEM_KMALLOC_ND:
+			return "KMALLOC_ND";
+		case MEM_KMEM_CACHE:
+			return "KMEM_CACHE";
+		case MEM_KMEM_CACHE_ND:
+			return "KMEM_CACHE_ND";
+		case MEM_ALLOC_PAGES:
+			return "ALLOC_PAGES";
+		case MEM_VMALLOC:
+			return "VMALLOC";
+		case MEM_INDUCED:
+			return "INDUCED_ALLOC";
+		case MEM_BOOTMEM:
+			return "BOOTMEM";
+		case MEM_MEMBLOCK:
+			return "MEMBLOCK";
+		case MEM_UFO_MEMBLOCK:
+			return "UFO_MEMBLOCK";
+		case MEM_MEMORIZER:
+			return "MEMORIZER";
+		case MEM_USER:
+			return "USER";
+		case MEM_BUG:
+			return "BUG";
+		case MEM_UFO_GLOBAL:
+			return "UFO_GLOBAL";
+		case MEM_UFO_NONE:
+			return "UFO_NONE";
+		case MEM_NONE:
+			return "NONE";
+		default:
+			pr_info("Searching for unavailable alloc type");
+			return "ALLOC TYPE NOT FOUND";
+	}
+};
+
+/**
+ * struct memorizer_kobj - metadata for kernel objects
+ * @rb_node:		the red-black tree relations
+ * @alloc_ip:		instruction that allocated the object
+ * @va_ptr:		Virtual address of the beginning of the object
+ * @pa_ptr:		Physical address of the beginning of object
+ * @size:		Size of the object
+ * @jiffies:		Time stamp of creation
+ * @pid:		PID of the current task
+ * @comm:		Executable name
+ * @kobj_list:		List of all objects allocated
+ * @access_counts:	List of memory access count structures
+ * @arg_kobj:   Pointer points to a function argument kobj.
+ *
+ * This data structure captures the details of allocated objects
+ */
+struct memorizer_kobj {
+	struct rb_node	    rb_node;
+	enum AllocType      alloc_type;
+	rwlock_t	    rwlock;
+	long		    obj_id;
+	uintptr_t	    alloc_ip;
+	uintptr_t	    free_ip;
+	uintptr_t	    va_ptr;
+	uintptr_t	    pa_ptr;
+	size_t		    size;
+	unsigned long	    alloc_jiffies;
+	unsigned long	    free_jiffies;
+	pid_t		    pid;
+	char		    comm[TASK_COMM_LEN];
+	char		    funcstr[KSYM_NAME_LEN];
+	bool		    printed;
+	//char		    *modsymb[KSYM_NAME_LEN];
+	char		    *slabname;
+	struct list_head    object_list;
+	struct list_head    access_counts;
+	struct memorizer_kobj *args_kobj;
+};
+
+/**
+ * access_counts - track reads/writes from single source IP
+ */
+struct access_from_counts {
+	struct list_head list;
+	uintptr_t ip;
+	uintptr_t caller;
+	uint64_t pid;
+	uint64_t writes;
+	uint64_t reads;
+};
+
+
+struct pid_obj {
+	uint32_t key;
+	pid_t pid;
+	char comm[TASK_COMM_LEN];
+};
+
+/*
+ * Kernel virtual addresses start at ffff880000000000 - ffffc7ffffffffff (=64
+ * TB) direct mapping of all phys. memory --- see
+ * Documentation/x86/x86_64/mm.txt. This means bit 43 is always set, which means
+ * we can remove all bytes where it is unset: TODO Optimization.
+ *
+ *  63             47 46                   24 23        12 11         0
+ * +-----------------+--*--------------------+------------+------------+
+ * |      ---        |          L3           |     L2     |     L1     |
+ * +-----------------+-----------------------+------------+------------+
+ *
+ * The lookup table maps each byte of allocatable virtual address space to a
+ * pointer to kernel object metadata--> 8 byte pointer.
+ *
+ */
+#define LT_L1_SHIFT		    12 
+#define LT_L1_ENTRIES		(_AC(1,UL) << LT_L1_SHIFT)
+#define LT_L1_ENTRY_SIZE	(sizeof(void *))
+#define LT_L1_SIZE		    (LT_L1_ENTRIES * LT_L1_ENTRY_SIZE)
+
+#define LT_L2_SHIFT		    27
+#define LT_L2_ENTRIES		(_AC(1,UL) << (LT_L2_SHIFT - LT_L1_SHIFT))
+#define LT_L2_ENTRY_SIZE	(sizeof(void *))
+#define LT_L2_SIZE		    (LT_L2_ENTRIES * LT_L2_ENTRY_SIZE)
+
+#define LT_L3_SHIFT		    47
+#define LT_L3_ENTRIES		(_AC(1,UL) << (LT_L3_SHIFT - LT_L2_SHIFT))
+#define LT_L3_ENTRY_SIZE	(sizeof(void *))
+#define LT_L3_SIZE		    (LT_L3_ENTRIES * LT_L3_ENTRY_SIZE)
+
+
+#define PID_ENTRIES		    (_AC(1,UL) << 5) 
+//PLACEHOLDER VALUE
+//==-- Table data structures -----------------------------------------------==//
+
+/*
+ * Each structure contains an array of pointers to the next level of the lookup.
+ * So the lowest level L1 has an array of pointers to the kobjects, L2 has an
+ * array of pointers to structs of type l2_tbl.
+ */
+struct lt_l1_tbl {
+	struct memorizer_kobj *kobj_ptrs[LT_L1_ENTRIES];
+};
+
+struct lt_l2_tbl {
+	struct lt_l1_tbl *l1_tbls[LT_L2_ENTRIES];
+};
+
+struct lt_l3_tbl {
+	struct lt_l2_tbl *l2_tbls[LT_L3_ENTRIES];
+};
+
+struct lt_pid_tbl {
+	struct pid_obj pid_obj_list[PID_ENTRIES];
+};
+
+#define lt_l1_tbl_index(va)	(va & (LT_L1_ENTRIES - 1))
+#define lt_l2_tbl_index(va)	((va >> LT_L1_SHIFT) & (LT_L2_ENTRIES - 1))
+#define lt_l3_tbl_index(va)	((va >> LT_L2_SHIFT) & (LT_L3_ENTRIES - 1))
+
+/*
+ * lt_l*_entry() --- get the table entry associated with the virtual address
+ *
+ * This uses ** because the value returned is a pointer to the table entry, but
+ * also can be dereferenced to point to the next level down.
+ */
+static inline struct memorizer_kobj **lt_l1_entry(struct lt_l1_tbl *l1_tbl,
+		uintptr_t va)
+{
+	return &(l1_tbl->kobj_ptrs[lt_l1_tbl_index(va)]);
+}
+
+static inline struct lt_l1_tbl **lt_l2_entry(struct lt_l2_tbl *l2_tbl, uintptr_t
+		va)
+{
+	return &l2_tbl->l1_tbls[lt_l2_tbl_index(va)];
+}
+
+static inline struct lt_l2_tbl **lt_l3_entry(struct lt_l3_tbl *l3_tbl, uintptr_t
+		va)
+{
+	return &l3_tbl->l2_tbls[lt_l3_tbl_index(va)];
+}
+
+static inline struct pid_obj * lt_pid(struct lt_pid_tbl *pid_tbl,  uint32_t key)
+{
+	return &(pid_tbl->pid_obj_list[key]);
+}
+
+//==-- External Interface -------------------------------------------------==//
+void lt_init(void);
+int lt_insert_kobj(struct memorizer_kobj *kobj);
+struct memorizer_kobj * lt_remove_kobj(uintptr_t va);
+struct memorizer_kobj * lt_get_kobj(uintptr_t va);
+int lt_insert_induced(void * vaddr, size_t size);
+bool is_induced_obj(uintptr_t va);
+
+#endif /* __KOBJ_METADATA_H_ */
+
diff --git a/mm/memorizer/memalloc.c b/mm/memorizer/memalloc.c
new file mode 100644
index 000000000000..ceadf93f31db
--- /dev/null
+++ b/mm/memorizer/memalloc.c
@@ -0,0 +1,115 @@
+/*===-- LICENSE -------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2018, The Board of Trustees of Rice University.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Nathan Dautenhahn in the Department of Computer
+ *    Science at Rice Unversity
+ *    http://nathandautenhahn.com
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memalloc.c
+ *
+ *    Description:
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#include <linux/bootmem.h>
+#include <linux/memorizer.h>
+
+#include "memalloc.h"
+
+uintptr_t pool_base = 0;
+uintptr_t pool_end = 0;
+uintptr_t pool_next_avail_byte = 0;
+unsigned long memalloc_size = MEMORIZER_POOL_SIZE;
+
+DEFINE_RWLOCK(mem_rwlock);
+
+/* function to let the size be specified as a boot parameter */
+static int __init early_memalloc_size(char *arg)
+{
+	unsigned long sizeGB;
+	if (!arg || kstrtoul(arg, 0, &sizeGB))
+		return 0;
+	memalloc_size = sizeGB << 30;
+	return 1;
+}
+early_param("memalloc_size", early_memalloc_size);
+
+void __init memorizer_alloc_init(void)
+{
+	pool_base = alloc_bootmem(memalloc_size);
+	if (!pool_base)
+		panic("No memorizer pool");
+	pool_end = pool_base + memalloc_size;
+	pool_next_avail_byte = pool_base;
+}
+
+void * memalloc(unsigned long size)
+{
+	unsigned long flags;
+	write_lock_irqsave(&mem_rwlock, flags);
+	void * va = pool_next_avail_byte;
+	if (!pool_next_avail_byte)
+		return 0;
+	if (pool_next_avail_byte + size > pool_end)
+		panic("Memorizer ran out of internal heap: add more with kernel boot flag (# is read as GB): memalloc_size=60");
+	pool_next_avail_byte += size;
+	write_unlock_irqrestore(&mem_rwlock, flags);
+	return va;
+}
+
+void * zmemalloc(unsigned long size)
+{
+	unsigned long i = 0;
+	void * va = memalloc(size);
+	char * vatmp = va;
+	for (i = 0; i < size; i++)
+		vatmp[i] = 0;
+	return va;
+}
+
+void print_pool_info(void)
+{
+	pr_info("Mempool begin: 0x%p, end: 0x%p, size:%llu GB\n", pool_base,
+		pool_end, (pool_end-pool_base)>>30);
+}
+
+bool in_pool(unsigned long va)
+{
+	return pool_base < va && va < pool_end;
+}
diff --git a/mm/memorizer/memalloc.h b/mm/memorizer/memalloc.h
new file mode 100644
index 000000000000..cb4bcc208e8c
--- /dev/null
+++ b/mm/memorizer/memalloc.h
@@ -0,0 +1,61 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2018, The Board of Trustees of Rice University.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Nathan Dautenhahn in the Department of Computer
+ *    Science at Rice Unversity
+ *    http://nathandautenhahn.com
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memalloc.h
+ *
+ *    Description:
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _MEMALLOC_H_
+#define _MEMALLOC_H_
+
+/* Start with minimally 3GB region else lookup tables will fail */
+#define MEMORIZER_POOL_SIZE     (_AC(1,UL) << 33)
+void * memalloc(unsigned long size);
+void * zmemalloc(unsigned long size);
+void print_pool_info(void);
+bool in_pool(unsigned long va);
+#endif /* __memalloc.h_H_ */
+
diff --git a/mm/memorizer/memorizer.c b/mm/memorizer/memorizer.c
new file mode 100644
index 000000000000..54f33373d71c
--- /dev/null
+++ b/mm/memorizer/memorizer.c
@@ -0,0 +1,2855 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2015, The Board of Trustees of the University of Illinois.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.c
+ *
+ *    Description:  Memorizer is a memory tracing tool. It hooks into KASAN
+ *		    events to record object allocation/frees and all
+ *		    loads/stores.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ * Locking:
+ *
+ *	Memorizer has global and a percpu data structure:
+ *
+ *		- global rbtree of active kernel objects - queue for holding
+ *		  free'd objects that haven't logged - A percpu event queue to
+ *		  track memory access events (Not used in current version, ignore)
+ *
+ * 		- Global objects: object_list, memorizer_kobj, pool_next_avail_byte,
+ * 		  function hash table, and lookup table.
+ *
+ *     Therefore, we have the following locks:
+ *
+ *		- active_kobj_rbtree_spinlock (ignore):
+ *
+ *			The insert routine is generic to any kobj_rbtree and
+ *			therefore is only provided in an unlocked variant
+ *			currently. The code must take this lock prior to
+ *			inserting into the rbtree.
+ *
+ *		- object_list_spinlock:
+ *
+ *			Lock for the list of all objects. This list is added to
+ *			on each kobj free. On log this queue should collect any
+ *			queued writes in the local PerCPU access queues and then
+ *			remove it from the list.
+ *
+ *		- memorizer_kobj.rwlock:
+ *
+ *			RW spinlock for access to object internals.
+ *      
+ *		- mem_rwlock:
+ * 			
+ * 			Lock for memory's next available byte pointer.
+ * 		
+ * 		- fht_rwlock:
+ * 			
+ * 			Lock for function hash table. This lock is to protect
+ * 			the function list when a new bucket is inserted. Note, 
+ * 			we don't need a read or write lock for updating the function 
+ * 			count because we use an atomic variable for the count.
+ * 
+ * 		- lookup_tbl_rw_lock:
+ * 		
+ * 			TODO: Need investigate whether we need this lock.
+ * 
+ *===-----------------------------------------------------------------------===
+
+ * Per-CPU data:
+ *  	- inmem:
+ * 			
+ * 			inmem makes sure we don't have re-entrance problem. We make this 
+ * 			a per-cpu data so that each core can execute Memorizer in parallel.
+ * 
+ *===-----------------------------------------------------------------------===
+ *
+ * Re-Entrance:
+ *
+ *	This system hooks all memory reads/writes and object allocation,
+ *	therefore any external function called will re-enter via ld/st
+ *	instrumentation as well as from allocations. So to avoid this we must be
+ *	very careful about any external functions called to ensure correct
+ *	behavior. This is particulary critical of the memorize access function.
+ *	The others can call external, but note that the memory ld/st as a
+ *	response to that call will be recorded.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/bug.h>
+#include <linux/gfp.h>
+#include <linux/cpumask.h>
+#include <linux/debugfs.h>
+#include <linux/err.h>
+#include <linux/export.h>
+#include <linux/jiffies.h>
+#include <linux/kallsyms.h>
+#include <linux/kernel.h>
+#include <linux/memorizer.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/preempt.h>
+#include <linux/printk.h>
+#include <asm/page_64.h>
+#include <linux/rbtree.h>
+#include <linux/rwlock.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/smp.h>
+#include <linux/workqueue.h>
+#include <asm/atomic.h>
+#include <asm/bitops.h>
+#include <asm/percpu.h>
+#include <linux/relay.h>
+#include <asm-generic/bug.h>
+#include <linux/cdev.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/bootmem.h>
+#include <linux/kasan-checks.h>
+#include <linux/mempool.h>
+
+#include<asm/fixmap.h>
+
+#include "kobj_metadata.h"
+#include "event_structs.h"
+#include "FunctionHashTable.h"
+#include "memorizer.h"
+#include "stats.h"
+#include "util.h"
+#include "memalloc.h"
+#include "../slab.h"
+#include "../kasan/kasan.h"
+
+
+
+//==-- Debugging and print information ------------------------------------==//
+#define MEMORIZER_DEBUG		1
+#define FIXME			0
+
+#define INLINE_EVENT_PARSE	1
+#define WORKQUEUES		0
+
+#define CALL_SITE_STRING	1
+#define TASK_STRING		1
+
+//==-- Prototype Declarations ---------------------------------------------==//
+static struct memorizer_kobj * unlocked_lookup_kobj_rbtree(uintptr_t kobj_ptr,
+		struct rb_root *kobj_rbtree_root);
+static void inline __memorizer_kmalloc(unsigned long call_site, const void *ptr,
+		uint64_t bytes_req, uint64_t bytes_alloc,
+		gfp_t gfp_flags, enum AllocType AT);
+static inline struct memorizer_kobj * __create_kobj(uintptr_t call_site, uintptr_t
+		ptr, uint64_t size, enum AllocType AT);
+void __always_inline wq_push(uintptr_t addr, size_t size, enum AccessType
+		access_type, uintptr_t ip, char * tsk_name);
+static inline struct memorizer_kernel_event * wq_top(void);
+void __drain_active_work_queue(void);
+void switch_to_next_work_queue(void);
+static struct memorizer_kobj * add_heap_UFO(uintptr_t va);
+//==-- Data types and structs for building maps ---------------------------==//
+
+/* Size of the memory access recording worklist arrays */
+#define MEM_ACC_L_SIZE 1
+
+/* Defining the maximum length for the event lists along with variables for character device driver */
+// NB refers to the number of buffers being vmalloced
+#define ML 500000
+#define NB 16
+
+#define BUFF_MUTEX_LOCK { \
+	while(*buff_mutex)\
+	yield(); \
+	*buff_mutex = *buff_mutex + 1;\
+}
+
+#define BUFF_MUTEX_UNLOCK {*buff_mutex = *buff_mutex - 1;}
+
+
+#define BUFF_FILL_SET {*buff_fill = 1;}
+
+static dev_t *dev[NB];
+static struct cdev *cd[NB];
+static void *pages1;
+static void *pages2;
+static char *buff_end;
+static char *buff_start;
+static char *buff_write_end;
+static char *buff_fill;
+static char *buff_mutex;
+static unsigned int *buff_free_size;
+static bool buff_init = false;
+static unsigned int curBuff = 0;
+static char *buffList[NB];
+
+static dev_t *dev1;
+static dev_t *dev2;
+static struct cdev *cd1;
+static struct cdev *cd2;
+
+#define global_table_text_size 1024 * 1024 * 10
+char * global_table_text;
+char * global_table_ptr;
+
+/**
+ * struct memorizer_mem_access - structure to capture all memory related events
+ * @access_type: type of event
+ * @src_ip:	 virtual address of the invoking instruction
+ * @access_addr: starting address of the operation
+ * @access_size: size of the access: for wr/rd size, allocation length
+ * @jiffies:	 timestamp
+ * @pid:	 PID of invoking task
+ * @comm:	 String of executable
+ */
+struct memorizer_mem_access {
+	enum AccessType access_type;
+	uintptr_t src_ip;
+	uintptr_t access_addr;		/* The location being accessed */
+	uint64_t access_size;		/* events can be allocs or memcpy */
+	unsigned long jiffies;		/* creation timestamp */
+	pid_t pid;			/* pid of the current task */
+	char comm[TASK_COMM_LEN];	/* executable name */
+} cdList[NB];
+
+struct memorizer_cdev {
+	char idx;
+	struct cdev charDev;
+};
+
+/**
+ * mem_access_wlists - This struct contains work queues holding accesses
+ *
+ * Size Calculation for memorizer_mem_access:
+ *	(1+64+64+64+64+32+256)*100000 = 54.5Mb
+ */
+struct mem_access_worklists {
+	struct memorizer_mem_access wls[2][MEM_ACC_L_SIZE];
+	size_t selector;
+	long head;
+	long tail;
+};
+
+/*
+ * switchBuffer - switches the the buffer being written to, when the buffer is full
+ */
+void __always_inline switchBuffer()
+{
+	buff_end = (char *)buffList[curBuff] + ML*4096-1;
+	buff_write_end = (char *)buffList[curBuff];
+	buff_fill = buff_write_end;
+	buff_write_end = buff_write_end + 1;
+	buff_mutex = buff_write_end;
+	buff_write_end = buff_write_end + 1;
+	buff_free_size = (unsigned int *)buff_write_end;
+	buff_write_end = buff_write_end + sizeof(unsigned int);
+	buff_start = buff_write_end;
+}
+
+/**
+ * struct code_region - simple struct to capture begin and end of a code region
+ */
+struct code_region {
+	uintptr_t b;
+	uintptr_t e;
+};
+
+struct code_region audit_code_region = {
+	.b = 0xffffffff81158b30,
+	.e = 0xffffffff8116b550
+};
+
+struct code_region selinux = {
+	.b = 0xffffffff81475450,
+	.e = 0xffffffff814a3000
+};
+
+
+struct code_region crypto_code_region = {
+	.b = 0xffffffff814a3000,
+	.e = 0xffffffff814cee00
+};
+
+//==-- PER CPU data structures and control flags --------------------------==//
+
+/* TODO make this dynamically allocated based upon free memory */
+//DEFINE_PER_CPU(struct mem_access_worklists, mem_access_wls = {.selector = 0, .head = 0, .tail = 0 });
+DEFINE_PER_CPU(struct mem_access_worklists, mem_access_wls);
+
+// memorizer atomic flag: when set it means we are operating in memorizer. The
+// point of the flag is so that if we use code outside of memorizer or an
+// interrupt occurs, it won't reenter and go down an infinite loop of
+// recursion.
+DEFINE_PER_CPU(int, recursive_depth = 0);
+
+/*
+ * Flags to keep track of whether or not to track writes
+ *
+ * Make this and the next open for early boot param manipulation via bootloader
+ * kernel args: root=/hda1 memorizer_enabled=[yes|no]
+ */
+static bool memorizer_enabled = false;
+static bool memorizer_enabled_boot = true;
+static int __init early_memorizer_enabled(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot alloc logging\n");
+		memorizer_enabled_boot = true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disable boot alloc logging\n");
+		memorizer_enabled_boot = false;
+	}
+}
+early_param("memorizer_enabled_boot", early_memorizer_enabled);
+
+/* flag enable/disable memory access logging */
+static bool memorizer_log_access = false;
+static bool mem_log_boot = false;
+static int __init early_mem_log_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		mem_log_boot= true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		mem_log_boot= false;
+	}
+}
+early_param("mem_log_boot", early_mem_log_boot);
+
+/* flag enable/disable memory access logging */
+static bool cfg_log_on = false;
+static bool cfg_log_boot = false;
+static int __init early_cfg_log_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		cfg_log_boot= true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		cfg_log_boot= false;
+	}
+}
+early_param("cfg_log_boot", early_cfg_log_boot);
+
+static bool stack_trace_on = false;
+static bool stack_trace_boot = false;
+static int __init early_stack_trace_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		stack_trace_boot = true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		stack_trace_boot= false;
+	}
+}
+early_param("stack_trace_boot", early_stack_trace_boot);
+
+static bool track_calling_context = false;
+static int __init track_cc(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		track_calling_context = true;
+	}
+}
+early_param("mem_track_cc", track_cc);
+
+/* flag enable/disable printing of live objects */
+static bool print_live_obj = true;
+static bool test_obj = false;
+
+/* Function has table */
+struct FunctionHashTable * cfgtbl;
+
+/* Object Cache for Serialized KObjects to be printed out to the RelayFS */
+//static struct kmem_cache *kobj_serial_cache = kmem_cache_create("Serial", sizeof(struct memorizer_kobj), 0, SLAB_PANIC,  NULL);
+
+/* active kobj metadata rb tree */
+static struct rb_root active_kobj_rbtree_root = RB_ROOT;
+
+/* full list of freed kobjs */
+static LIST_HEAD(object_list);
+
+/* global object id reference counter */
+static atomic_long_t global_kobj_id_count = ATOMIC_INIT(0);
+
+/* General kobj for catchall object references */
+static struct memorizer_kobj * general_kobjs[NumAllocTypes];
+
+//==-- Locks --=//
+/* RW Spinlock for access to rb tree */
+DEFINE_RWLOCK(active_kobj_rbtree_spinlock);
+
+/* RW Spinlock for access to freed kobject list */
+DEFINE_RWLOCK(object_list_spinlock);
+
+/* System wide Spinlock for the aggregating thread so nothing else interrupts */
+DEFINE_RWLOCK(aggregator_spinlock);
+
+//--- MEMBLOCK Allocator Tracking ---//
+/* This is somewhat challenging because these blocks are allocated on physical
+ * addresses. So we need to transition them.
+ */
+typedef struct {
+	uintptr_t loc;
+	uint64_t size;
+} memblock_alloc_t;
+memblock_alloc_t memblock_events[100000];
+size_t memblock_events_top = 0;
+bool in_memblocks(uintptr_t va_ptr)
+{
+	int i;
+	uintptr_t pa = __pa(va_ptr);
+	for (i = 0; i < memblock_events_top; i++) {
+		uintptr_t base = memblock_events[i].loc;
+		uintptr_t end = memblock_events[i].loc + memblock_events[i].loc;
+		
+        // TODO: Not sure if it should be pa >= base or pa > base here.
+        if(pa >= base && pa < end)
+			return true;
+	}
+	return false;
+}
+
+//int test_and_set_bit(unsigned long nr, volatile unsigned long *addr);
+//volatile unsigned long inmem;
+DEFINE_PER_CPU(volatile unsigned long, inmem);
+
+volatile unsigned long in_getfreepages;
+
+uintptr_t cur_caller = 0;
+
+/**
+ * __memorizer_enter() - increment recursion counter for entry into memorizer
+ *
+ * The primary goal of this is to stop recursive handling of events. Memorizer
+ * by design tracks two types of events: allocations and accesses. Effectively,
+ * while tracking either type we do not want to re-enter and track memorizer
+ * events that are sources from within memorizer. Yes this means we may not
+ * track legitimate access of some types, but these are caused by memorizer and
+ * we want to ignore them.
+ */
+static inline int __memorizer_enter(void)
+{
+    // TODO: Unsure which is the proper function to call.
+    return this_cpu_cmpxchg(inmem, 0, 1);
+	// return test_and_set_bit_lock(0,&inmem);
+}
+
+static __always_inline void __memorizer_exit(void)
+{
+    // TODO: selected first as it has no return type.
+    this_cpu_write(inmem, 0);
+	//return clear_bit_unlock (0,&inmem);
+}
+
+/**
+ * in_memorizer() - check if this thread has already entered memorizer
+ */
+static __always_inline bool in_memorizer(void)
+{
+	return test_bit(0,&inmem);
+}
+
+
+/**
+ * __print_memorizer_kobj() - print out the object for debuggin
+ *
+ * Grab reader lock if you want to  make sure things don't get modified while we
+ * are printing
+ */
+void __print_memorizer_kobj(struct memorizer_kobj * kobj, char * title)
+{
+	struct list_head * listptr;
+	struct access_from_counts *entry;
+
+	pr_info("%s: \n", title);
+	pr_info("\tkobj_id:	%ld\n", kobj->obj_id);
+	//pr_info("\talloc_mod:	%s\n", *kobj->modsymb);
+	pr_info("\talloc_func:	%s\n", kobj->funcstr);
+	pr_info("\talloc_ip:	0x%p\n", (void*) kobj->alloc_ip);
+	pr_info("\tfree_ip:	0x%p\n", (void*) kobj->free_ip);
+	pr_info("\tva:		0x%p\n", (void*) kobj->va_ptr);
+	pr_info("\tpa:		0x%p\n", (void*) kobj->pa_ptr);
+	pr_info("\tsize:	%lu\n", kobj->size);
+	pr_info("\talloc jiffies: %lu\n", kobj->alloc_jiffies);
+	pr_info("\tfree jiffies:  %lu\n", kobj->free_jiffies);
+	pr_info("\tpid: %d\n", kobj->pid);
+	pr_info("\texecutable: %s\n", kobj->comm);
+	list_for_each(listptr, &(kobj->access_counts)){
+		entry = list_entry(listptr, struct access_from_counts, list);
+		pr_info("\t  Access IP: %p, PID: %d, Writes: %llu, Reads: %llu\n",
+				//(void *) entry->ip, entry->pid,
+				(void *) entry->ip, 0,
+				(unsigned long long) entry->writes,
+				(unsigned long long) entry->reads);
+	}
+}
+EXPORT_SYMBOL(__print_memorizer_kobj);
+
+/**
+ * read_locking_print_memorizer_kobj() - grap the reader spinlock then print
+ */
+static void read_locking_print_memorizer_kobj(struct memorizer_kobj * kobj, char
+		* title)
+{
+	unsigned long flags;
+	read_lock_irqsave(&kobj->rwlock, flags);
+	__print_memorizer_kobj(kobj, title);
+	read_unlock_irqrestore(&kobj->rwlock, flags);
+}
+
+/**
+ * __print_rb_tree() - print the tree
+ */
+static void __print_active_rb_tree(struct rb_node * rb)
+{
+	struct memorizer_kobj * kobj;
+	if (rb) {
+		kobj = rb_entry(rb, struct memorizer_kobj, rb_node);
+		read_locking_print_memorizer_kobj(kobj,"Kernel Object");
+		if (kobj->rb_node.rb_left)
+			__print_active_rb_tree(kobj->rb_node.rb_left);
+		if (kobj->rb_node.rb_right)
+			__print_active_rb_tree(kobj->rb_node.rb_right);
+	}
+}
+
+/**
+ * access_degree() - for the specified access type count the degree of access
+ */
+void access_degree(struct list_head * acl, unsigned int * write_deg,
+		unsigned int * read_deg)
+{
+	struct list_head * listptr;
+	struct access_from_counts * ac;
+	/* For each ld/st in the access counts entry add 1 */
+	list_for_each(listptr, acl) {
+		ac = list_entry(listptr, struct access_from_counts, list);
+		/* if the ac has at least one write then it counts */
+		if (ac->writes > 0)
+			*write_deg += 1;
+		if (ac->reads > 0)
+			*read_deg += 1;
+	}
+}
+
+/**
+ * __print_rb_tree() - print the tree
+ */
+static void print_rb_tree_access_counts(struct rb_node * rb)
+{
+	struct memorizer_kobj * kobj;
+	if (rb) {
+		kobj = rb_entry(rb, struct memorizer_kobj, rb_node);
+
+		unsigned int write_deg = 0, read_deg = 0;
+
+		access_degree(&kobj->access_counts, &write_deg, &read_deg);
+
+		pr_info("%s %d %s %u %u\n", kobj->funcstr, kobj->pid, kobj->comm,
+				write_deg, read_deg);
+
+		if (kobj->rb_node.rb_left)
+			print_rb_tree_access_counts(kobj->rb_node.rb_left);
+		if (kobj->rb_node.rb_right)
+			print_rb_tree_access_counts(kobj->rb_node.rb_right);
+	}
+}
+
+/**
+ */
+static void print_pdf_table(void)
+{
+	unsigned long flags;
+
+	/* calculate stats and print the free'd objects */
+	struct list_head *p;
+	struct memorizer_kobj *kobj;
+
+	read_lock_irqsave(&object_list_spinlock, flags);
+
+	list_for_each(p, &object_list){
+		unsigned int write_deg = 0, read_deg = 0;
+
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+
+		access_degree(&kobj->access_counts, &write_deg, &read_deg);
+
+		pr_info("%s %d %s %u %u\n", kobj->funcstr, kobj->pid, kobj->comm,
+				write_deg, read_deg);
+
+	}
+	read_unlock_irqrestore(&object_list_spinlock, flags);
+
+	/* same for live objects */
+	print_rb_tree_access_counts(active_kobj_rbtree_root.rb_node);
+}
+
+/**
+ * __memorizer_print_events - print the last num events
+ * @num_events:		The total number of events to print
+ *
+ * Simple print assuming an array log. Only tricky thing is to wrap around the
+ * circular buffer when hitting the end or printing the last set of events if
+ * some of them are at the end of the linear buffer. 
+ */
+/* TODO: LEGACY CODE SHOULD BE REMOVED */
+void __memorizer_print_events(unsigned int num_events)
+{
+	int i, e, log_index;
+	struct mem_access_worklists * ma_wls;
+	struct memorizer_mem_access *mal, *ma; /* mal is the list ma is the
+						  instance */
+	__memorizer_enter();
+
+	print_stats(KERN_INFO);
+
+	/* Get data structure for the worklists and init the iterators */
+	ma_wls = &get_cpu_var(mem_access_wls);
+	mal = (struct memorizer_mem_access*) &(ma_wls->wls[ma_wls->selector]);
+	log_index = ma_wls->head;
+
+	pr_info("WLS State: selector = %lu, head = %ld, tail = %ld",
+			ma_wls->selector, ma_wls->head, ma_wls->tail);
+
+	/* 
+	 * If we are at the front of the list then allow wrap back, note that
+	 * this will print garbage if this function is called without having
+	 * wrapped.
+	 */
+	if ((log_index - num_events) > 0)
+		i = log_index - num_events;
+	else
+		i = MEM_ACC_L_SIZE - 1 - (num_events - log_index + 1);
+
+	for (e = 0; e < num_events; e++) {
+		char *type_str[10];
+		ma = &mal[i];
+		pr_info("access from IP 0x%p at addr 0x%p\n", (void *)
+				ma->src_ip, (void *) ma->access_addr);
+		switch (ma->access_type) {
+			case Memorizer_READ:
+				*type_str = "Read\0";
+				break;
+			case Memorizer_WRITE:
+				*type_str = "Write\0";
+				break;
+			default:
+				pr_info("Unmatched event type\n");
+				*type_str = "Unknown\0";
+		}
+		pr_info("%s of size %lu by task %s/%d\n", *type_str,
+				(unsigned long) ma->access_size, ma->comm, ma->pid);
+		if (++i >= MEM_ACC_L_SIZE)
+			i = 0;
+	}
+	put_cpu_var(mem_access_wls);
+
+	__memorizer_exit();
+}
+EXPORT_SYMBOL(__memorizer_print_events);
+
+
+void memorizer_print_stats(void)
+{
+	print_stats(KERN_CRIT);
+}
+EXPORT_SYMBOL(memorizer_print_stats);
+
+
+/**
+ * dump_object_list() - print out the list of free'd objects
+ */
+static void dump_object_list(void)
+{
+	unsigned long flags;
+	struct list_head *p;
+	struct memorizer_kobj *kobj;
+	read_lock_irqsave(&object_list_spinlock, flags);
+	list_for_each (p, &object_list) {
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+		read_locking_print_memorizer_kobj(kobj, "Dump Free'd kobj");
+	}
+	read_unlock_irqrestore(&object_list_spinlock, flags);
+}
+
+//----
+//==-- Memorizer Access Processing ----------------------------------------==//
+//----
+
+static struct access_from_counts *
+__alloc_afc(void)
+{
+	struct access_from_counts * afc = NULL;
+	afc = (struct access_from_counts *)
+	memalloc(sizeof(struct access_from_counts));
+	return afc;
+}
+
+/**
+ * init_access_counts_object() - initialize data for the object
+ * @afc:	object to init
+ * @ip:		ip of access
+ */
+static inline void
+init_access_counts_object(struct access_from_counts *afc, uint64_t ip, pid_t
+		pid)
+{
+	INIT_LIST_HEAD(&(afc->list));
+	afc->ip = ip;
+	afc->writes = 0;
+	afc->reads = 0;
+	if (track_calling_context)
+		afc->caller = cur_caller;
+	else
+		afc->caller = NULL;
+}
+
+/**
+ * alloc_new_and_init_access_counts() - allocate a new access count and init
+ * @ip:		the access from value
+ */
+static inline struct access_from_counts *
+alloc_and_init_access_counts(uint64_t ip, pid_t pid)
+{
+	struct access_from_counts * afc = NULL;
+	afc = __alloc_afc();
+	init_access_counts_object(afc, ip, pid);
+	track_access_counts_alloc();
+	return afc;
+}
+
+/**
+ * access_from_counts - search kobj's access_from for an entry from src_ip
+ * @src_ip:	the ip to search for
+ * @kobj:	the object to search within
+ *
+ * This function does not do any locking and therefore assumes the caller will
+ * already have at least a reader lock. This is a big aggregate function, but
+ * given that it will occur a lot we will be searching the list for a given
+ * object, therefore we can easily do insertion if we don't find it, keeping a
+ * linearly monotonic sorted list.
+ *
+ * Here we insert a new entry for each (ip,threadid) tuple.
+ */
+static inline struct access_from_counts *
+unlckd_insert_get_access_counts(uint64_t src_ip, pid_t pid, struct
+		memorizer_kobj *kobj)
+{
+	struct list_head * listptr;
+	struct access_from_counts *entry;
+	struct access_from_counts * afc = NULL;
+	list_for_each (listptr, &(kobj->access_counts)) {
+		entry = list_entry(listptr, struct access_from_counts, list);
+		if (src_ip == entry->ip) {
+			if (kobj->alloc_type == MEM_NONE) {
+				if (entry->caller == cur_caller)
+					return entry;
+				else if (cur_caller < entry->caller)
+					break;
+			} else {
+				return entry;
+			}
+		} else if (src_ip < entry->ip) {
+			break;
+		}
+	}
+	/* allocate the new one and initialize the count none in list */
+	afc = alloc_and_init_access_counts(src_ip, pid);
+	if (afc)
+		list_add_tail(&(afc->list), listptr);
+	return afc;
+}
+
+/**
+ * update_kobj_access() - find and update the object information
+ * @memorizer_mem_access:	The access to account for
+ *
+ * @src_va_ptr: PC for source of operation
+ * @va_ptr: the virtual address being written to
+ * @pid: pid of access
+ * @access_type: type of access (read/write)
+ *
+ * Find the object associated with this memory write, search for the src ip in
+ * the access structures, incr if found or alloc and add new if not.
+ *
+ * Executes from the context of memorizer_mem_access and therefore we are
+ * already operating with interrupts off and preemption disabled, and thus we
+ * cannot sleep.
+ */
+
+static int reports_shown = 0;
+
+static inline int find_and_update_kobj_access(uintptr_t src_va_ptr,
+		uintptr_t va_ptr, pid_t pid, size_t access_type, size_t size)
+{
+	struct memorizer_kobj *kobj = NULL;
+	struct access_from_counts *afc = NULL;
+
+	if (in_pool(va_ptr)) {
+		track_access(MEM_MEMORIZER,size);
+		return;
+	}
+
+	/* Get the kernel object associated with this VA */
+	kobj = lt_get_kobj(va_ptr);
+
+	if (!kobj) {
+		if (is_induced_obj(va_ptr)) {
+			kobj = general_kobjs[MEM_INDUCED];
+			track_access(MEM_INDUCED,size);
+		} else if (in_memblocks(va_ptr)) {
+			kobj = __create_kobj(MEM_UFO_MEMBLOCK, va_ptr, size,
+					MEM_UFO_MEMBLOCK);
+			if (!kobj) {
+				kobj = general_kobjs[MEM_MEMBLOCK];
+				track_untracked_access(MEM_MEMBLOCK,size);
+			} else {
+				track_access(MEM_MEMBLOCK,size);
+			}
+		} else {
+			enum AllocType AT = kasan_obj_type(va_ptr,size);
+			kobj =  general_kobjs[AT];
+			switch(AT){
+				case MEM_STACK_PAGE:
+					kobj = __create_kobj(MEM_STACK_PAGE, va_ptr,
+							size, MEM_UFO_GLOBAL);
+					track_access(MEM_STACK_PAGE,size);
+					break;
+                case MEM_HEAP:
+#if 1
+                    // Debugging feature to print a KASAN report for missed heap accesses.
+                        // Only prints up to 5 reports.
+                    if (reports_shown < 5){
+                        kasan_report((unsigned long) va_ptr, size, 1, &kasan_report);
+                        reports_shown++;
+                    }
+#endif
+                    kobj = add_heap_UFO(va_ptr);
+                    
+                    track_access(MEM_UFO_HEAP,size);
+                    break;
+                case MEM_GLOBAL:
+                    kobj = __create_kobj(MEM_UFO_GLOBAL, va_ptr,
+                                 size, MEM_UFO_GLOBAL);
+                    track_access(MEM_UFO_GLOBAL,size);
+                    break;
+                case MEM_NONE:
+                    kobj = __create_kobj(MEM_UFO_NONE, va_ptr,
+                                 size, MEM_UFO_NONE);
+                    track_access(MEM_UFO_NONE,size);
+                    break;
+                default:
+                    track_untracked_access(AT,size);
+			}
+		}
+	} else {
+		track_access(kobj->alloc_type, size); 
+	}
+
+	/* Grab the object lock here */
+	write_lock(&kobj->rwlock);
+
+	/* Search access queue to the entry associated with src_ip */
+	afc = unlckd_insert_get_access_counts(src_va_ptr, pid, kobj);
+
+	/* increment the counter associated with the access type */
+	if (afc)
+		access_type ? ++(afc->writes) : ++(afc->reads);
+
+	write_unlock(&kobj->rwlock);
+	return afc ? 0 : -1;
+}
+
+/**
+ * drain_and_process_access_queue() - remove entries from the queue and do stats
+ * @mawls:	the percpu wl struct to drain
+ *
+ * While the list is not empty take the top element and update the kobj it
+ * accessed. Note that the kobj for this could be not found so we just ignore it
+ * and move on if the update function failed.
+ */
+/*
+static inline void drain_and_process_access_queue(struct mem_access_worklists *
+ma_wls)
+{
+	while(ma_wls->head >= 0){
+		//pr_info("Head: %ld", ma_wls->head);
+		find_and_update_kobj_access(
+			&(ma_wls->wls[ma_wls->selector][ma_wls->head])
+		);
+		--ma_wls->head;
+	}
+}
+ */
+//==-- Memorizer memory access tracking -----------------------------------==//
+
+/**
+ * set_comm_and_pid - Find the execution context of the ld/st
+ *
+ * Set the pid and the task name. These are together because we want to optimize
+ * the number of branches in this to make it faster.
+ */
+static inline void set_comm_and_pid(struct memorizer_mem_access *ma)
+{
+	char *comm;
+	char *hardirq = "hardirq";
+	char *softirq = "softirq";
+
+	/* task information */
+	if (unlikely(in_irq())) {
+		ma->pid = 0;
+		comm = hardirq;
+	} else if (unlikely(in_softirq())) {
+		ma->pid = 0;
+		comm = softirq;
+	} else {
+		ma->pid = task_pid_nr(current);
+		/*
+		 * There is a small chance of a race with set_task_comm(),
+		 * however using get_task_comm() here may cause locking
+		 * dependency issues with current->alloc_lock. In the worst
+		 * case, the command line is not correct.
+		 */
+		comm = current->comm;
+	}
+#if 0 /* TODO: this is to make the testing faster */
+	int i;
+	for (i=0; i<sizeof(comm); i++)
+		ma->comm[i] = comm[i];
+	ma->comm[i] = '\0';
+#endif
+}
+
+/**
+ * Requires: Calculate the callee's stack frame size
+ * and callee's arg size if arg registers are full.
+ * @ip: is the callee's virtual address.
+ * @parent_ip: is the caller's virtual address.
+ */
+void __cyg_profile_func_enter(void *ip, void *parent_ip)
+{
+	if (!cfg_log_on && !stack_trace_on)
+		return;
+	/* Prevent infinete loop */
+	if (__memorizer_enter())
+		return;
+
+	if (track_calling_context)
+		cur_caller = parent_ip;
+
+	/* Disable interrupt */
+	unsigned long flags;
+	local_irq_save(flags);
+
+#if defined(__x86_64__)
+#if INLINE_EVENT_PARSE
+	/**
+	 * | caller sp |
+	 * | ret addr  |
+	 * | callee bp |
+	 * | ...       |
+	 * | callee sp |
+	 * | cyg bp    |
+	 * 
+	 * In order to calculate func bp, we need to dereference
+	 * the callee bp and callee bp + 0x10 is the func sp. 
+	 */
+	struct pt_regs pt_regs;
+	if (stack_trace_on) {
+		uintptr_t callee_bp = 0, callee_sp = 0;
+		register uintptr_t cyg_rbp asm("rbp");
+		callee_bp = *(uintptr_t *)cyg_rbp; // deference callee bp
+		callee_sp = cyg_rbp + 0x10; // Prologue pushes the return address (0x8) and RBP (0x8)
+		/* Store function bp and sp into pt_regs structure */
+		pt_regs.bp = callee_bp;
+		pt_regs.sp = callee_sp;
+	}
+
+	/* cfg_update_counts creates <from, to, callee kobj, args kobj> tuple */
+	cfg_update_counts(cfgtbl, parent_ip, ip, &pt_regs, stack_trace_on);
+#endif
+
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+EXPORT_SYMBOL(__cyg_profile_func_enter);
+
+/**
+ * Future work: The stack frame kobjs are never free and there are lots
+ * of these kobjs. In the future, we can free the kobjs here and restore
+ * the lookup table pointing to the MEM_STACK_PAGE kobj.
+ * @ip: is the callee's virtual address.
+ * @parent_ip: is the caller's virtual address.
+ */
+void __cyg_profile_func_exit(void *ip, void *parent_ip)
+{
+
+}
+EXPORT_SYMBOL(__cyg_profile_func_exit);
+
+
+/**
+ * memorizer_mem_access() - record associated data with the load or store
+ * @addr:	The virtual address being accessed
+ * @size:	The number of bits for the load/store
+ * @write:	True if the memory access is a write (store)
+ * @ip:		IP of the invocing instruction
+ *
+ * Memorize, ie. log, the particular data access.
+ */
+void __always_inline memorizer_mem_access(uintptr_t addr, size_t size, bool
+		write, uintptr_t ip)
+{
+	unsigned long flags;
+	if (unlikely(!memorizer_log_access) || unlikely(!memorizer_enabled)) {
+		track_disabled_access();
+		return;
+	}
+
+	if (current->kasan_depth > 0) {
+		track_induced_access();
+		return;
+	}
+
+	if (__memorizer_enter()) {
+		track_induced_access();
+		return;
+	}
+
+
+#if INLINE_EVENT_PARSE 
+	local_irq_save(flags);
+	find_and_update_kobj_access(ip,addr,-1,write,size);
+	local_irq_restore(flags);
+#else
+	//trace_printk("%p->%p,%d,%d\n",ip,addr,size,write);
+	//wq_push(addr, size, write, ip, 0);
+#endif
+
+	__memorizer_exit();
+
+#if 0
+	if (buff_init) {
+		while (*buff_fill) {
+			curBuff = (curBuff + 1)%NB;
+			pr_info("Trying Buffer %u\n",curBuff);
+			switchBuffer();
+		}
+
+		local_irq_save(flags);
+
+		if (write)
+			mke.event_type = Memorizer_Mem_Write;
+		else
+			mke.event_type = Memorizer_Mem_Read;
+
+		mke.pid = task_pid_nr(current);
+		mke.event_size = size;
+		mke.event_ip = ip;
+		mke.src_va_ptr = addr;
+		mke.event_jiffies = jiffies;
+
+		mke_ptr = (struct memorizer_kernel_access *)buff_write_end;
+		*mke_ptr = mke;
+		buff_write_end = buff_write_end + sizeof(struct memorizer_kernel_access);
+		*buff_free_size = *buff_free_size - sizeof(struct memorizer_kernel_access);
+
+		/* Check after writing the event to the buffer if there is any more
+		 * space for the next entry to go in - Need to choose the struct with
+		 * the biggest size for this otherwise it may lead to a problem wherein
+		 * the write pointer still points to the end of the buffer and there is
+		 * another event ready to be written which might be bigger than the
+		 * size of the struct that could have reset the pointer 
+		 */
+		if (*buff_free_size < sizeof(struct memorizer_kernel_event)) {
+
+			pr_info("Current Buffer Full, Setting the fill bit\n");
+			*buff_fill = 1;
+			buff_write_end = buff_start;
+		}
+		local_irq_restore(flags);
+		//*buff_end = (unsigned long long)0xbb;
+	}
+	//}
+#endif
+}
+
+void __always_inline memorizer_fork(struct task_struct *p, long nr)
+{
+
+	unsigned long flags;
+
+	return;
+	if (unlikely(!memorizer_enabled))
+		return;
+	if (__memorizer_enter())
+		return;
+
+
+#if 0
+	struct memorizer_kernel_event * evtptr = wq_top();
+	evtptr->event_type = Memorizer_Fork;
+
+	local_irq_save(flags);
+	if (in_irq()) {
+		evtptr->pid = 0;
+		strncpy(evtptr->data.comm, "hardirq", sizeof(evtptr->data.comm));
+	} else if (in_softirq()) {
+		evtptr->pid = 0;
+		strncpy(evtptr->data.comm, "softirq", sizeof(evtptr->data.comm));
+	} else {
+		evtptr->pid = task_pid_nr(p);
+		/*
+		 * There is a small chance of a race with set_task_comm(),
+		 * however using get_task_comm() here may cause locking
+		 * dependency issues with current->alloc_lock. In the worst
+		 *	 case, the command line is not correct.
+		 */
+		strncpy(evtptr->data.comm, p->comm, sizeof(evtptr->data.comm));
+	}
+	//wq_push(0,0,Memorizer_Fork,0,p->comm);
+	//trace_printk("%p->%s,%d,%c\n",p,p->comm,0,Memorizer_Mem_Free);
+	trace_printk("fork:%s,PID:%d\n",p->comm,nr);
+
+	local_irq_restore(flags);
+#endif 
+	/* check to see if stack is allocated */
+	//if (lt_get_kobj(p->stack)) {
+		//pr_crit("Forked: Is stack in live objs? TRUE\n");
+	// }
+	//else {
+		//pr_crit("Forked: Is stack in live objs? FALSE\n");
+	// }
+
+	__memorizer_exit();
+
+#if 0
+	if (buff_init) {
+		while (*buff_fill) {
+			curBuff = (curBuff + 1)%NB;
+			switchBuffer();
+		}
+
+		local_irq_save(flags);
+		mke.event_type = Memorizer_Fork;
+		if (in_irq()) {
+			mke.pid = 0;
+			strncpy(mke.comm, "hardirq", sizeof(mke.comm));
+		} else if (in_softirq()) {
+			mke.pid = 0;
+			strncpy(mke.comm, "softirq", sizeof(mke.comm));
+		} else {
+			mke.pid = nr;
+			/*
+			 * There is a small chance of a race with set_task_comm(),
+			 * however using get_task_comm() here may cause locking
+			 * dependency issues with current->alloc_lock. In the worst
+			 *	 case, the command line is not correct.
+			 */
+			strncpy(mke.comm, p->comm, sizeof(mke.comm));
+		}
+
+		mke_ptr = (struct memorizer_kernel_fork *)buff_write_end;
+		*mke_ptr = mke;
+		buff_write_end = buff_write_end +sizeof(struct memorizer_kernel_fork);	
+		*buff_free_size = *buff_free_size - sizeof(struct memorizer_kernel_fork);
+
+
+		if (*buff_free_size < sizeof(struct memorizer_kernel_event)) {
+
+			*buff_fill = 1;
+			buff_write_end = buff_start;
+		}
+
+
+
+		local_irq_restore(flags);
+	}
+
+#endif
+}
+
+//==-- Memorizer kernel object tracking -----------------------------------==//
+
+/*
+ *
+ */
+static struct kmem_cache * get_slab_cache(const void * addr)
+{
+	if ((addr >= (void *)PAGE_OFFSET) && (addr < high_memory)) {
+		struct page *page = virt_to_head_page(addr);
+		if (PageSlab(page)) {
+			return page->slab_cache;
+			//void *object;
+			//struct kmem_cache *cache = page->slab_cache;
+			//object = nearest_obj(cache, page, access_addr);
+			//pr_err("Object at %p, in cache %s size: %d\n", object,
+			//cache->name, cache->object_size);
+		}
+		return NULL;
+	}
+	return NULL;
+}
+
+/*
+ * If we miss lookup the object from the cache.  Note that the init_kobj will
+ * preset a string for the slab name. So these UFOs are aggregated in an
+ * intelligent and still useful way. We've missed the alloc (and thereofre the
+ * alloc site) but we've at least grouped them by type. Assume we get a page
+ * because we are in this case.
+ */
+static struct memorizer_kobj * add_heap_UFO(uintptr_t va)
+{
+	struct memorizer_kobj *kobj = NULL;
+	if ((va >= (void *)PAGE_OFFSET) && (va < high_memory)) {
+		struct page *page = virt_to_head_page(va);
+		if (PageSlab(page)) {
+			void *object;
+			struct kmem_cache *cache = page->slab_cache;
+			object = nearest_obj(cache, page, va);
+			//pr_err("Object at %p, in cache %s size: %d\n", object,
+			//cache->name, cache->object_size);
+			kobj = __create_kobj(MEM_UFO_HEAP, object,
+					cache->object_size,
+					MEM_UFO_HEAP);
+		}
+	}
+	return kobj;
+}
+
+/**
+ * init_kobj() - Initalize the metadata to track the recent allocation
+ */
+static void init_kobj(struct memorizer_kobj * kobj, uintptr_t call_site,
+		uintptr_t ptr_to_kobj, size_t bytes_alloc,
+		enum AllocType AT)
+{
+	rwlock_init(&kobj->rwlock);
+
+	struct kmem_cache * cache;
+
+	if (atomic_long_inc_and_test(&global_kobj_id_count)) {
+		pr_warn("Global kernel object counter overlapped...");
+	}
+
+	/* Zero out the whole object including the comm */
+	memset(kobj, 0, sizeof(struct memorizer_kobj));
+	kobj->alloc_ip = call_site;
+	kobj->va_ptr = ptr_to_kobj;
+	kobj->pa_ptr = __pa(ptr_to_kobj);
+	kobj->size = bytes_alloc;
+	kobj->alloc_jiffies = get_ts();
+	kobj->free_jiffies = 0;
+	kobj->free_ip = 0;
+	kobj->obj_id = atomic_long_read(&global_kobj_id_count);
+	kobj->printed = false;
+	kobj->alloc_type = AT;
+	kobj->args_kobj = NULL;
+	INIT_LIST_HEAD(&kobj->access_counts);
+	INIT_LIST_HEAD(&kobj->object_list);
+
+	/* get the slab name */
+	cache = get_slab_cache(kobj->va_ptr);
+	if (cache) {
+		kobj->slabname = memalloc(strlen(cache->name)+1);
+		strncpy(kobj->slabname, cache->name, strlen(cache->name));
+		kobj->slabname[strlen(cache->name)]='\0';
+	} else {
+		kobj->slabname = "no-slab";
+	}
+
+#if CALL_SITE_STRING == 1
+	/* Some of the call sites are not tracked correctly so don't try */
+	if (call_site)
+		kallsyms_lookup((unsigned long) call_site, NULL, NULL,
+				//&(kobj->modsymb), kobj->funcstr);
+			NULL, kobj->funcstr);
+#endif
+#if TASK_STRING == 1
+	/* task information */
+	if (in_irq()) {
+		kobj->pid = 0;
+		strncpy(kobj->comm, "hardirq", sizeof(kobj->comm));
+	} else if (in_softirq()) {
+		kobj->pid = 0;
+		strncpy(kobj->comm, "softirq", sizeof(kobj->comm));
+	} else {
+		kobj->pid = current->pid;
+		/*
+		 * There is a small chance of a race with set_task_comm(),
+		 * however using get_task_comm() here may cause locking
+		 * dependency issues with current->alloc_lock. In the worst
+		 * case, the command line is not correct.
+		 */
+		strncpy(kobj->comm, current->comm, sizeof(kobj->comm));
+	}
+#endif
+
+#if MEMORIZER_DEBUG >= 5
+	__print_memorizer_kobj(kobj, "Allocated and initalized kobj");
+#endif
+}
+
+/**
+ * free_access_from_entry() --- free the entry from the kmem_cache
+ */
+static void free_access_from_entry(struct access_from_counts *afc)
+{
+	//TODO clean up all the kmem_cache_free stuff
+	//kmem_cache_free(access_from_counts_cache, afc);
+	//TODO Create Free function here with new memalloc allocator
+}
+
+/**
+ * free_access_from_list() --- for each element remove from list and free
+ */
+static void free_access_from_list(struct list_head *afc_lh)
+{
+	struct access_from_counts *afc;
+	struct list_head *p;
+	struct list_head *tmp;
+	list_for_each_safe(p, tmp, afc_lh) {
+		afc = list_entry(p, struct access_from_counts, list);
+		list_del(&afc->list);
+		free_access_from_entry(afc);
+	}
+}
+
+/**
+ * free_kobj() --- free the kobj from the kmem_cache
+ * @kobj:	The memorizer kernel object metadata
+ *
+ * FIXME: there might be a small race here between the write unlock and the
+ * kmem_cache_free. If another thread is trying to read the kobj and is waiting
+ * for the lock, then it could get it. I suppose the whole *free_kobj operation
+ * needs to be atomic, which might be proivded by locking the list in general.
+ */
+static void free_kobj(struct memorizer_kobj * kobj)
+{
+	write_lock(&kobj->rwlock);
+	free_access_from_list(&kobj->access_counts);
+	write_unlock(&kobj->rwlock);
+	//kmem_cache_free(kobj_cache, kobj);
+	//TODO add new free function here from memalloc allocator
+	track_kobj_free();
+}
+
+/**
+ * clear_free_list() --- remove entries from free list and free kobjs
+ */
+static void clear_dead_objs(void)
+{
+	struct memorizer_kobj *kobj;
+	struct list_head *p;
+	struct list_head *tmp;
+	unsigned long flags;
+	pr_info("Clearing the free'd kernel objects\n");
+	/* Avoid rentrance while freeing the list */
+	while(!__memorizer_enter())
+		yield();
+	write_lock_irqsave(&object_list_spinlock, flags);
+	list_for_each_safe(p, tmp, &object_list) {
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+		/* If free_jiffies is 0 then this object is live */
+		if (kobj->free_jiffies > 0) {
+			/* remove the kobj from the free-list */
+			list_del(&kobj->object_list);
+			/* Free the object data */
+			free_kobj(kobj);
+		}
+	}
+	write_unlock_irqrestore(&object_list_spinlock, flags);
+	__memorizer_exit();
+}
+
+/**
+ * clear_printed_objects() --- remove entries from free list and free kobjs
+ */
+static void clear_printed_objects(void)
+{
+	struct memorizer_kobj *kobj;
+	struct list_head *p;
+	struct list_head *tmp;
+	unsigned long flags;
+	pr_info("Clearing the free'd and printed kernel objects\n");
+	__memorizer_enter();
+	write_lock_irqsave(&object_list_spinlock, flags);
+	list_for_each_safe(p, tmp, &object_list) {
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+		/* If free_jiffies is 0 then this object is live */
+		if (kobj->free_jiffies > 0 && kobj->printed) {
+			/* remove the kobj from the free-list */
+			list_del(&kobj->object_list);
+			/* Free the object data */
+			free_kobj(kobj);
+		}
+	}
+	write_unlock_irqrestore(&object_list_spinlock, flags);
+	__memorizer_exit();
+}
+
+/**
+ * add_kobj_to_rb_tree - add the object to the tree
+ * @kobj:	Pointer to the object to add to the tree
+ *
+ * Standard rb tree insert. The key is the range. So if the object is allocated
+ * < than the active node's region then traverse left, if greater than traverse
+ * right, and if not that means we have an overlap and have a problem in
+ * overlapping allocations.
+ */
+static struct memorizer_kobj * unlocked_insert_kobj_rbtree(struct memorizer_kobj
+		*kobj, struct rb_root
+		*kobj_rbtree_root)
+{
+	struct memorizer_kobj *parent;
+	struct rb_node **link;
+	struct rb_node *rb_parent = NULL;
+
+	link = &(kobj_rbtree_root->rb_node);
+	while (*link) {
+		rb_parent = *link;
+		parent = rb_entry(rb_parent, struct memorizer_kobj, rb_node);
+		if (kobj->va_ptr + kobj->size <= parent->va_ptr) {
+			link = &parent->rb_node.rb_left;
+		} else if (parent->va_ptr + parent->size <= kobj->va_ptr) {
+			link = &parent->rb_node.rb_right;
+		} else {
+			pr_err("Cannot insert 0x%lx into the object search tree"
+					" (overlaps existing)\n", kobj->va_ptr);
+			__print_memorizer_kobj(parent, "");
+			//kmem_cache_free(kobj_cache, kobj);
+			//TODO add free here
+			kobj = NULL;
+			break;
+		}
+	}
+	if (likely(kobj != NULL)) {
+		rb_link_node(&kobj->rb_node, rb_parent, link);
+		rb_insert_color(&kobj->rb_node, kobj_rbtree_root);
+	}
+	return kobj;
+}
+
+/**
+ * search_kobj_from_rbtree() - lookup the kobj from the tree
+ * @kobj_ptr:	The ptr to find the active for
+ * @rbtree:	The rbtree to lookup in
+ *
+ * This function searches for the memorizer_kobj associated with the passed in
+ * pointer in the passed in kobj_rbtree. Since this is a reading on the rbtree
+ * we assume that the particular tree being accessed has had it's lock acquired
+ * properly already.
+ */
+static struct memorizer_kobj * unlocked_lookup_kobj_rbtree(uintptr_t kobj_ptr,
+		struct rb_root *
+		kobj_rbtree_root)
+{
+	struct rb_node *rb = kobj_rbtree_root->rb_node;
+
+	while (rb) {
+		struct memorizer_kobj * kobj =
+			rb_entry(rb, struct memorizer_kobj, rb_node);
+		/* Check if our pointer is less than the current node's ptr */
+		if (kobj_ptr < kobj->va_ptr)
+			rb = kobj->rb_node.rb_left;
+		/* Check if our pointer is greater than the current node's ptr */
+		else if (kobj_ptr >= (kobj->va_ptr + kobj->size))
+			rb = kobj->rb_node.rb_right;
+		/* At this point we have found the node because rb != null */
+		else
+			return kobj;
+	}
+	return NULL;
+}
+
+/**
+ * __memorizer_free_kobj - move the specified objec to free list
+ *
+ * @call_site:	Call site requesting the original free
+ * @ptr:	Address of the object to be freed
+ *
+ * Algorithm:
+ *	1) find the object in the rbtree
+ *	2) add the object to the memorizer process kobj queue
+ *	3) remove the object from the rbtree
+ *
+ * Maybe TODO: Do some processing here as opposed to later? This depends on when
+ * we want to add our filtering.
+ * 0xvv
+ */
+void static __memorizer_free_kobj(uintptr_t call_site, uintptr_t kobj_ptr)
+{
+
+	struct memorizer_kobj *kobj;
+	unsigned long flags;
+
+	/* find and remove the kobj from the lookup table and return the
+	 * kobj */
+	kobj = lt_remove_kobj(kobj_ptr);
+
+	/*
+	 *   * If this is null it means we are freeing something we did
+	 *   not insert
+	 *       * into our tree and we have a missed alloc track,
+	 *       otherwise we update
+	 *           * some of the metadata for free.
+	 *               */
+	if (kobj) {
+		/* Update the free_jiffies for the object */
+		write_lock_irqsave(&kobj->rwlock, flags);
+		kobj->free_jiffies = get_ts();
+		kobj->free_ip = call_site;
+		write_unlock_irqrestore(&kobj->rwlock, flags);
+		track_free();
+		//TODO add free function here
+	}
+	else
+		track_untracked_obj_free();
+}
+
+/**
+ * memorizer_free_kobj - move the specified objec to free list
+ * @call_site:	Call site requesting the original free
+ * @ptr:	Address of the object to be freed
+ *
+ * Algorithm:
+ *	1) find the object in the rbtree
+ *	2) add the object to the memorizer process kobj queue
+ *	3) remove the object from the rbtree
+ *
+ * Maybe TODO: Do some processing here as opposed to later? This depends on when
+ * we want to add our filtering.
+ * 0xvv
+ */
+void static memorizer_free_kobj(uintptr_t call_site, uintptr_t kobj_ptr)
+{
+
+	struct memorizer_kobj *kobj;
+	unsigned long flags;
+
+	if (__memorizer_enter()) {
+		track_induced_free();
+		return;
+	}
+
+	local_irq_save(flags);
+	//wq_push(kobj_ptr, 0, Memorizer_Mem_Free, call_site, 0);
+	//trace_printk("%p->%p,%d,%d\n",call_site,kobj_ptr,0,Memorizer_Mem_Free);
+	__memorizer_free_kobj(call_site, kobj_ptr);
+
+	local_irq_restore(flags);
+	__memorizer_exit();
+
+#if 0
+	if (buff_init) {
+
+		while (*buff_fill) {
+			curBuff = (curBuff + 1)%NB;
+			switchBuffer();
+		}
+
+
+
+		local_irq_save(flags);
+		// Set up the event Struct and Dump it to the Buffer
+		mke.event_type = Memorizer_Mem_Free;
+		mke.pid = task_pid_nr(current);
+		mke.src_va_ptr = call_site;
+		mke.event_ip = kobj_ptr;
+		mke.event_jiffies = jiffies;
+
+		mke_ptr = (struct memorizer_kernel_free *)buff_write_end;
+		*mke_ptr = mke;
+		buff_write_end = buff_write_end + sizeof(struct memorizer_kernel_free);
+		*buff_free_size = *buff_free_size - sizeof(struct memorizer_kernel_free);
+
+		if (*buff_free_size < sizeof(struct memorizer_kernel_event)) {
+			*buff_fill = 1;
+			buff_write_end = buff_start;
+		}
+
+
+		local_irq_restore(flags);
+	}
+	//	}
+#endif
+}
+
+struct memorizer_kobj *create_kobj(uintptr_t call_site, uintptr_t ptr, uint64_t size, enum AllocType AT) {
+	return __create_kobj(call_site, ptr, size, AT);
+}
+
+/**
+ * __create_kobj() - allocate and init kobj assuming locking and rentrance
+ *	protections already enabled.
+ * @call_site:  Address of the call site to the alloc
+ * @ptr:	Pointer to location of data structure in memory
+ * @size:	Size of the allocation
+ * @AT:		Type of allocation
+ */
+static inline struct memorizer_kobj * __create_kobj(uintptr_t call_site,
+		uintptr_t ptr, uint64_t
+		size, enum AllocType AT)
+{
+	struct memorizer_kobj *kobj;
+
+	/* inline parsing */
+	kobj = memalloc(sizeof(struct memorizer_kobj));
+	if (!kobj) {
+		track_failed_kobj_alloc();
+		return NULL;
+	}
+
+	/* initialize all object metadata */
+	init_kobj(kobj, call_site, ptr, size, AT);
+
+	/* memorizer stats tracking */
+	track_alloc(AT);
+
+	/* mark object as live and link in lookup table */
+	lt_insert_kobj(kobj);
+
+	/* Grab the writer lock for the object_list and insert into object list */
+	write_lock(&(list_first_entry(&object_list, struct memorizer_kobj,
+				      object_list))->rwlock);
+	list_add_tail(&kobj->object_list, &object_list);
+	write_unlock(&(list_first_entry(&object_list, struct memorizer_kobj,
+					object_list))->rwlock);
+	return kobj;
+}
+
+/**
+ * memorizer_alloc() - record allocation event
+ * @object:	Pointer to the beginning of hte object
+ * @size:	Size of the object
+ *
+ * Track the allocation and add the object to the set of active object tree.
+ */
+static void inline __memorizer_kmalloc(unsigned long call_site, const void
+		*ptr, uint64_t bytes_req, uint64_t bytes_alloc, gfp_t gfp_flags, enum AllocType AT)
+{
+
+	unsigned long flags;
+
+	if (unlikely(ptr==NULL))
+		return;
+
+	if (unlikely(!memorizer_enabled)) {
+		track_disabled_alloc();
+		return;
+	}
+
+	if (__memorizer_enter()) {
+		/* link in lookup table with dummy event */
+		local_irq_save(flags);
+		lt_insert_induced((uintptr_t)ptr,bytes_alloc);
+		track_induced_alloc();
+		local_irq_restore(flags);
+		return;
+	}
+
+#if 0
+	pid_t pid;
+	if (in_irq()) {
+		pid = 0;
+	} else if (in_softirq()) {
+		pid = 0;
+	} else {
+		pid = current->pid;
+	}
+
+	/* workqueue style */
+	wq_push(ptr, bytes_alloc, Memorizer_Mem_Alloc, call_site, current->comm);
+
+	/* ftrace event queue style */
+	trace_printk("%p->%p,%d,%d\n",call_site,ptr,bytes_alloc,Memorizer_Mem_Alloc);
+#endif 
+
+	local_irq_save(flags);
+	__create_kobj((uintptr_t) call_site, (uintptr_t) ptr, bytes_alloc, AT);
+	local_irq_restore(flags);
+	__memorizer_exit();
+
+#if 0
+	if (buff_init) {
+
+		while (*buff_fill) {
+			curBuff = (curBuff + 1)%NB;
+			switchBuffer();
+		}
+
+		local_irq_save(flags);
+		mke.event_type = Memorizer_Mem_Alloc;
+		mke.event_ip = call_site;
+		mke.src_va_ptr = (uintptr_t)ptr;
+		mke.src_pa_ptr = __pa((uintptr_t)ptr);
+		mke.event_size = bytes_alloc;
+		mke.event_jiffies = jiffies;
+		/* Some of the call sites are not tracked correctly so don't try */
+		if (call_site)
+			kallsyms_lookup((unsigned long) call_site, NULL, NULL,
+					//&(kobj->modsymb), kobj->funcstr);
+				NULL, mke.funcstr);
+		/* task information */
+		if (in_irq()) {
+			mke.pid = 0;
+			strncpy(mke.comm, "hardirq", sizeof(mke.comm));
+		} else if (in_softirq()) {
+			mke.pid = 0;
+			strncpy(mke.comm, "softirq", sizeof(mke.comm));
+		} else {
+			mke.pid = current->pid;
+			/*
+			 * There is a small chance of a race with set_task_comm(),
+			 * however using get_task_comm() here may cause locking
+			 * dependency issues with current->alloc_lock. In the worst
+			 *	 case, the command line is not correct.
+			 */
+			strncpy(mke.comm, current->comm, sizeof(mke.comm));
+		}
+
+		mke_ptr = (struct memorizer_kernel_alloc *)buff_write_end;
+		*mke_ptr = mke;
+		buff_write_end = buff_write_end + sizeof(struct memorizer_kernel_alloc);
+		*buff_free_size = *buff_free_size - sizeof(struct memorizer_kernel_alloc);
+
+		if (*buff_free_size < sizeof(struct memorizer_kernel_event)) {
+			*buff_fill = 1;
+			buff_write_end = buff_start;
+		}
+		local_irq_restore(flags);
+	} else {
+		track_disabled_alloc();
+	}
+
+#endif
+
+	//*buff_end = (unsigned long long)0xaa;
+}
+
+/*** HOOKS similar to the kmem points ***/
+void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t
+		bytes_req, size_t bytes_alloc, gfp_t gfp_flags)
+{
+	__memorizer_kmalloc(call_site, ptr, bytes_req, bytes_alloc, gfp_flags,
+			MEM_KMALLOC);
+}
+
+void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t
+		bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int
+		node)
+{
+	__memorizer_kmalloc(call_site, ptr, bytes_req, bytes_alloc, gfp_flags,
+			MEM_KMALLOC_ND);
+}
+
+void memorizer_kfree(unsigned long call_site, const void *ptr)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+void memorizer_memblock_alloc(phys_addr_t base, phys_addr_t size)
+{
+	track_alloc(MEM_MEMBLOCK);
+	memblock_alloc_t * evt = &memblock_events[memblock_events_top++];
+	evt->loc = base;
+	evt->size = size;
+}
+
+void memorizer_memblock_free(phys_addr_t base, phys_addr_t size)
+{
+}
+
+void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size)
+{
+	track_alloc(MEM_BOOTMEM);
+	__memorizer_kmalloc(call_site, v, size, size, 0, MEM_BOOTMEM);
+	return;
+}
+
+const char * l1str = "lt_l1_tbl";
+const char * l2str = "lt_l2_tbl";
+const char * memorizer_kobjstr = "memorizer_kobj";
+const char * access_from_countsstr = "access_from_counts";
+bool is_memorizer_cache_alloc(char * cache_str)
+{
+	if (!memstrcmp(l1str,cache_str))
+		return true;
+	if (!memstrcmp(l2str,cache_str))
+		return true;
+	if (!memstrcmp(memorizer_kobjstr,cache_str))
+		return true;
+	if (!memstrcmp(access_from_countsstr,cache_str))
+		return true;
+	return false;
+}
+
+
+void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr,
+		unsigned long size, gfp_t gfp_flags)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	__memorizer_kmalloc(call_site, ptr, size, size,
+			gfp_flags, MEM_VMALLOC);
+}
+
+void memorizer_vmalloc_free(unsigned long call_site, const void *ptr)
+{
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+
+// Update the allocation site of a kmem_cache object, only if has current special
+// value of MEMORIZER_PREALLOCED.
+bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void * ptr){
+  
+  struct memorizer_kobj * kobj = lt_get_kobj(ptr);
+
+  if (kobj == NULL){
+    return false;
+  } else {
+    if (kobj -> alloc_ip == MEMORIZER_PREALLOCED){
+      kobj -> alloc_ip = call_site;
+    }
+    return true;
+  }
+}
+
+void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr,
+		struct kmem_cache *s, gfp_t gfp_flags)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	if (!is_memorizer_cache_alloc(s->name))
+		__memorizer_kmalloc(call_site, ptr, s->object_size, s->size,
+				gfp_flags, MEM_KMEM_CACHE);
+}
+
+void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr,
+		struct kmem_cache *s, gfp_t gfp_flags, int node)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	if (!is_memorizer_cache_alloc(s->name))
+		__memorizer_kmalloc(call_site, ptr, s->object_size, s->size,
+				gfp_flags, MEM_KMEM_CACHE_ND);
+}
+
+void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+
+void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned
+		int order, gfp_t gfp_flags)
+{
+
+  if (test_bit(0,&in_getfreepages)){
+    return;
+  }
+    __memorizer_kmalloc(call_site, page_address(page),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            gfp_flags, MEM_ALLOC_PAGES);
+
+}
+
+/* This is a slight variation to memorizer_alloc_pages(). Alloc_pages() can only return
+ * a power-of-two number of pages, whereas alloc_pages_exact() can return
+ * any specific number of pages. We don't want Memorizer to track the gap
+ * between the two, so use this special memorizer hook for this case. */
+void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned
+			   int size, gfp_t gfp_flags)
+{
+
+  // Compute the actual number of bytes that will be allocated
+  unsigned long alloc_size = PAGE_ALIGN(size);
+
+  __memorizer_kmalloc(call_site, ptr,
+		      alloc_size, alloc_size,
+		      gfp_flags, MEM_ALLOC_PAGES);
+
+}
+
+
+void memorizer_start_getfreepages(){
+  test_and_set_bit_lock(0,&in_getfreepages);
+}
+
+void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+			   int order, gfp_t gfp_flags)
+{
+    //TODO: Conflict here where one version used 1 << order, other used 2 << order.
+    __memorizer_kmalloc(call_site, page_address(page),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            gfp_flags, MEM_ALLOC_PAGES);
+
+    clear_bit_unlock(0,&in_getfreepages);
+}
+
+void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned
+		int order)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t)
+			page_address(page));
+}
+
+/**
+ *
+ * Thread should have allocated and this stack should be in the table
+ */
+void memorizer_stack_page_alloc(struct task_struct *task)
+{
+	/* get the object */
+	struct memorizer_kobj * stack_kobj = lt_get_kobj(task->stack);
+	/* if there then just mark it, but it appears to be filtered out */
+	if (!stack_kobj) {
+		void *base = task_stack_page(task);
+		__memorizer_kmalloc(_RET_IP_, base, THREAD_SIZE, THREAD_SIZE,
+				0, MEM_STACK_PAGE);
+	} else {
+		/* change alloc type to stack page alloc */
+		stack_kobj->alloc_type = MEM_STACK_PAGE;
+	}
+}
+
+void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t
+		size)
+{
+	__memorizer_kmalloc(call_site, ptr, size, size, 0, MEM_STACK);
+}
+
+void memorizer_register_global(const void *ptr, size_t size)
+{
+	__memorizer_kmalloc(0, ptr, size, size, 0, MEM_GLOBAL);
+}
+
+void memorizer_alloc(unsigned long call_site, const void *ptr, size_t size,
+		enum AllocType AT)
+{
+	//__memorizer_kmalloc(call_site, ptr, size, size, 0, AT);
+}
+
+//==-- Memorizer Data Export ----------------------------------------------==//
+static unsigned long seq_flags;
+static bool sequence_done = false;
+extern struct list_head *seq_list_start(struct list_head *head, loff_t pos);
+extern struct list_head *seq_list_next(void *v, struct list_head *head, loff_t
+		*ppos);
+
+/*
+ * kmap_seq_start() --- get the head of the free'd kobj list
+ *
+ * Grab the lock here and give back on close. There is an interesting problem
+ * here in that when the data gets to the page size limit for printing, the
+ * sequence file closes the file and opens up again by coming to the start
+ * location having processed a subset of the list already. The problem with this
+ * is that without having __memorizer_enter() it will add objects to the list
+ * between the calls to show and next opening the potential for an infinite
+ * loop. It also adds elements in between start and stop operations.
+ *
+ * For some reason the start is called every time after a *stop*, which allows
+ * more entries to be added to the list thus requiring the extra sequence_done
+ * flag that I added to detect the end of the list. So we add this flag so that
+ * any entries added after won't make the sequence continue forever in an
+ * infinite loop.
+ */
+static void *kmap_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	__memorizer_enter();
+	write_lock_irqsave(&object_list_spinlock, seq_flags);
+
+	if (list_empty(&object_list))
+		return NULL;
+
+	if (*pos == 0) {
+		sequence_done = false;
+		return object_list.next;
+	}
+
+	/* 
+	 * Second call back even after return NULL to stop. This must occur
+	 * after the check to (*pos == 0) otherwise it won't continue after the
+	 * first time a read is executed in userspace. The specs didn't mention
+	 * this but my experiments showed its occurrence. 
+	 */
+	if (sequence_done == true)
+		return NULL;
+
+	return seq_list_start(&object_list, *pos);
+}
+
+/*
+ * kmap_seq_next() --- move the head pointer in the list or return null
+ */
+static void *kmap_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	return seq_list_next(v, &object_list, pos);
+}
+
+/*
+ * kmap_seq_show() - print out the object including access info
+ */
+static int kmap_seq_show(struct seq_file *seq, void *v)
+{
+	struct access_from_counts *afc;
+	struct memorizer_kobj *kobj = list_entry(v, struct memorizer_kobj,
+			object_list);
+	read_lock(&kobj->rwlock);
+	/* If free_jiffies is 0 then this object is live */
+	if (!print_live_obj && kobj->free_jiffies == 0) {
+		read_unlock(&kobj->rwlock);
+		return 0;
+	}
+	kobj->printed = true;
+	/* Print object allocation info */
+	seq_printf(seq,"%-p,%d,%p,%lu,%lu,%lu,%p,%s,%s,%s\n",
+			(void*) kobj->alloc_ip, kobj->pid, (void*) kobj->va_ptr,
+			kobj->size, kobj->alloc_jiffies, kobj->free_jiffies, (void*)
+			kobj->free_ip, alloc_type_str(kobj->alloc_type), kobj->comm,
+			kobj->slabname);
+
+	/* print each access IP with counts and remove from list */
+	list_for_each_entry(afc, &kobj->access_counts, list) {
+		if (kobj->alloc_type == MEM_NONE && track_calling_context) {
+			seq_printf(seq, "  from:%p,caller:%p,%llu,%llu\n",
+					(void *) afc->ip, afc->caller,
+					(unsigned long long) afc->writes,
+					(unsigned long long) afc->reads);
+		} else
+			seq_printf(seq, "  %p,%llu,%llu\n",
+					(void *) afc->ip,
+					(unsigned long long) afc->writes,
+					(unsigned long long) afc->reads);
+	}
+
+	read_unlock(&kobj->rwlock);
+	return 0;
+}
+
+/*
+ * kmap_seq_stop() --- clean up on sequence file stopping
+ *
+ * Must release locks and ensure that we can re-enter. Also must set the
+ * sequence_done flag to avoid an infinit loop, which is required so that we
+ * guarantee completions without reentering due to extra allocations between
+ * this invocation of stop and the start that happens.
+ */
+static void kmap_seq_stop(struct seq_file *seq, void *v)
+{
+	if (!v)
+		sequence_done = true;
+	write_unlock_irqrestore(&object_list_spinlock, seq_flags);
+	__memorizer_exit();
+}
+
+static const struct seq_operations kmap_seq_ops = {
+	.start = kmap_seq_start,
+	.next  = kmap_seq_next,
+	.stop  = kmap_seq_stop,
+	.show  = kmap_seq_show,
+};
+
+static int kmap_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &kmap_seq_ops);
+}
+
+static ssize_t kmap_write(struct file *file, const char __user *user_buf,
+		size_t size, loff_t *ppos)
+{
+#if 0
+	char buf[64];
+	int buf_size;
+	int ret;
+
+	buf_size = min(size, (sizeof(buf) - 1));
+	if (strncpy_from_user(buf, user_buf, buf_size) < 0)
+		return -EFAULT;
+	buf[buf_size] = 0;
+
+	if (strncmp(buf, "clear", 5) == 0) {
+		if (kmemleak_enabled)
+			kmemleak_clear();
+		else
+			__kmemleak_do_cleanup();
+		goto out;
+	}
+#endif
+
+	return 0;
+}
+
+static const struct file_operations kmap_fops = {
+	.owner		= THIS_MODULE,
+	.open		= kmap_open,
+	.write		= kmap_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+/* 
+ * clear_free_list_write() - call the function to clear the free'd kobjs
+ */
+static ssize_t clear_dead_objs_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	clear_dead_objs();
+	*ppos += size;
+	return size;
+}
+
+static const struct file_operations clear_dead_objs_fops = {
+	.owner		= THIS_MODULE,
+	.write		= clear_dead_objs_write,
+};
+
+/* 
+ * clear_free_list_write() - call the function to clear the free'd kobjs
+ */
+static ssize_t drain_active_work_queue_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	__drain_active_work_queue();
+	*ppos += size;
+	return size;
+}
+
+static const struct file_operations drain_active_work_queue_fops = {
+	.owner		= THIS_MODULE,
+	.write		= drain_active_work_queue_write,
+};
+
+/* 
+ * clear_printed_free_list_write() - call the function to clear the printed free'd kobjs
+ */
+static ssize_t clear_printed_list_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	clear_printed_objects();
+	*ppos += size;
+	return size;
+}
+
+static const struct file_operations clear_printed_list_fops = {
+	.owner		= THIS_MODULE,
+	.write		= clear_printed_list_write,
+};
+
+static ssize_t cfgmap_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	unsigned long flags;
+	__memorizer_enter();
+	local_irq_save(flags);
+	cfgmap_clear(cfgtbl);
+	local_irq_restore(flags);
+	__memorizer_exit();
+	*ppos += size;
+	return size;
+}
+
+//static ssize_t cfgmap_read(struct file *file, const char __user *user_buf,
+//size_t size, loff_t *ppos)
+static ssize_t cfgmap_read(struct file *fp, char __user *user_buffer, size_t
+		size, loff_t *ppos)
+{
+	console_print(cfgtbl);
+	*ppos += size;
+	return size;
+}
+
+static int cfgmap_seq_show(struct seq_file *seq, void *v)
+{
+	struct EdgeBucket * b;    
+	int index;
+	for (index = 0; index < cfgtbl -> number_buckets; index++) {
+		b = cfgtbl -> buckets[index];
+		while (b != NULL) {
+			seq_printf(seq,"%lx %lx %ld\n", b -> from, b -> to, b -> count);
+			b = b -> next;
+		}
+	}  
+}
+
+static int cfgmap_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &cfgmap_seq_show, NULL);
+}
+
+static const struct file_operations cfgmap_fops = {
+	.owner		= THIS_MODULE,
+	.write		= cfgmap_write,
+	.open		= cfgmap_open,
+	.read		= seq_read,
+};
+
+static int stats_seq_show(struct seq_file *seq, void *v)
+{
+	return seq_print_stats(seq);
+}
+
+static int show_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &stats_seq_show, NULL);
+}
+
+static const struct file_operations show_stats_fops = {
+	.owner		= THIS_MODULE,
+	.open		= show_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* The debugging info generated by gcc doesn't quite include *everything*,
+ * even when using -g3 for most debugging info. As far as I can tell, the
+ * only things missing are some string constants, etc that are not very
+ * interesting. However, on the uSCOPE analysis side, we really want to map
+ * these back to files / folders for analysis. This interface lets you print
+ * the entire global table exactly as KASAN sees it, so that everything matches
+ * up and we get complete debug info for all globals. */
+static int globaltable_seq_show(struct seq_file *seq, void *v)
+{
+  seq_printf(seq, "%s\n", global_table_text);
+  return 0;
+}
+
+static int globaltable_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &globaltable_seq_show, NULL);
+}
+
+static const struct file_operations globaltable_fops = {
+	.owner		= THIS_MODULE,
+	.open		= globaltable_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+//==-- Memorizer Initializtion --------------------------------------------==//
+
+static int create_buffers(void)
+{
+	int i;
+	unsigned int *temp_size;
+	for (i = 0; i < NB; i++) {
+		buffList[i] = (char *)vmalloc(ML*4096);
+		if (!buffList[i])
+			return 0;
+
+		memset(buffList[i],0,ML*4096);
+		temp_size = (unsigned int *)(buffList[i]+2);
+		*temp_size = ML*4096 - 6;
+
+	}
+	return 1;
+}
+
+/* 2^32 = 4.29 GB */
+/* number of entries at 2^5 / entry 2^25 ~~ 1 GB */
+//const size_t num_entries_perwq = 2^22;
+//#define num_entries_perwq (_AC(1,UL) << 26)
+#define num_entries_perwq (_AC(1,UL) << 20)
+
+struct event_list_wq_data {
+	struct work_struct work;
+	struct memorizer_kernel_event data[num_entries_perwq];
+};
+
+#define num_queues 2
+size_t wq_index = 0;
+size_t wq_selector = 0;
+size_t wq_process_me = 0;
+struct event_list_wq_data mem_events_wq_data[num_queues];
+
+/*
+ * parse_events() - take the event list @data and parse to object cmap
+ *
+ * Note this function makes no assumptions about the calling context. This
+ * function is not an external entry point, and therefore expects it's caller
+ * to manage "in_memorizer" as well as any other global memorizer based locking
+ * required.
+ */
+	static void
+parse_events(struct event_list_wq_data * data)
+{
+	unsigned long i, flags;
+	bool old_access, old_enabled;
+
+	// Saving the old config before disabling the memorizer for aggregation
+	old_enabled = memorizer_enabled;
+	old_access = memorizer_log_access;
+
+	// Disabling the memorizer for aggregation
+	memorizer_enabled = false;
+	memorizer_log_access = false;
+
+	//spin_lock_irqsave(&aggregator_spinlock,flags);
+	gfp_t gfp_flags = GFP_ATOMIC;
+	struct memorizer_kobj *kobj;
+	struct memorizer_kernel_event *mke;
+
+	pr_info("processing workqueue %d\n", wq_selector);
+
+	/* Process the event queue */
+	for (i = 0; i < num_entries_perwq; i++) {
+		mke = &data->data[i];
+#ifdef DEBUG > 5
+		if (i % (int)(num_entries_perwq*.5) == 0)
+			pr_cont("\rQueue Processing: %d/%d", i,num_entries_perwq);
+#endif
+
+		switch (mke->event_type) {
+			case Memorizer_READ:
+			case Memorizer_WRITE:
+				find_and_update_kobj_access(
+						(uintptr_t) mke->data.et.src_va_ptr,
+						(uintptr_t) mke->data.et.va_ptr,(pid_t)mke->pid,
+						(size_t) mke->event_type, mke->data.et.event_size);
+				break;
+			case Memorizer_Mem_Alloc:
+				kobj = memalloc(sizeof(struct memorizer_kobj));
+				if (!kobj) {
+					pr_err("Cannot allocate a memorizer_kobj structure\n");
+				}
+				init_kobj(kobj, (uintptr_t) mke->data.et.src_va_ptr,
+						(uintptr_t) mke->data.et.va_ptr,
+						mke->data.et.event_size, MEM_NONE);
+				/* Grab the writer lock for the object_list */
+				// We are single threaded here don't need to lock
+				//write_lock_irqsave(&object_list_spinlock, flags);
+				lt_insert_kobj(kobj);
+				list_add_tail(&kobj->object_list, &object_list);
+				//write_unlock_irqrestore(&object_list_spinlock, flags);
+				break;
+			case Memorizer_Mem_Free:
+				__memorizer_free_kobj((uintptr_t) mke->data.et.src_va_ptr,
+						(uintptr_t) mke->data.et.va_ptr);
+				break;
+			case Memorizer_Fork:
+				// Add in the code to Handle Forks
+				// Push the data as a struct into the pid_table
+				break;
+			case Memorizer_NULL:
+				break;
+				//default:
+				//pr_info("Handling default case for event dequeue");
+		}
+		mke->event_type = Memorizer_NULL;
+	}
+	//spin_lock_irqrestore(&aggregator_spinlock, flags);
+
+	//pr_info("Finished aggregating event queue.\n");
+
+	// Restoring the old configuration after aggregation
+	memorizer_enabled = old_enabled;
+	memorizer_log_access = old_access;
+
+	// set first entry to Memorizer_NULL for queue selection check */
+	//data->data[0].event_type = Memorizer_NULL;
+}
+
+struct workqueue_struct *wq;
+
+	static void
+mem_events_workhandler(struct work_struct *work)
+{
+	unsigned long flags;
+	__memorizer_enter();
+	local_irq_save(flags);
+	parse_events(container_of(work,struct event_list_wq_data, work));
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+
+/*
+ * Switch the active work queue to the next one and queue the work up.
+ */
+bool work_deferred = false;
+size_t next_to_parse = 0;
+	void 
+switch_to_next_work_queue(void)
+{
+	size_t full = wq_selector;
+	wq_selector = (++wq_selector) % num_queues;
+	pr_info("Queueing work and switching to buffer %u\n", wq_selector);
+
+#if WORKQUEUES == 1
+	queue_work(wq, &(mem_events_wq_data[full].work));
+#else // DEFER with check on irq contexts
+	unsigned long flags;
+	if (unlikely(in_irq()) || unlikely(in_softirq())) {
+		work_deferred = true;
+		next_to_parse = full;
+	} else {
+		local_irq_save(flags);
+		parse_events(&mem_events_wq_data[full]);
+		local_irq_restore(flags);
+	}
+#endif
+
+	/* check to see if the new queue is empty */
+	if (mem_events_wq_data[wq_selector].data[0].event_type !=
+			Memorizer_NULL) {
+		panic("memorizer: tried to switch to non-empty queue\n");
+	}
+
+	/* reset current top to 0 */
+	wq_index = 0;
+}
+
+/* 
+ * wq_top() - return the next open slot in the active wq
+ */
+static inline struct memorizer_kernel_event *wq_top(void)
+{
+	return &(mem_events_wq_data[wq_selector].data[wq_index]);
+}
+
+/* 
+ * wq_push() - add event to the workqueue
+ *
+ * @addr:       destination of operation
+ * @size:       size of the operation
+ * @AccessType: operation type
+ * @ip:         src address of the instruction pointer
+ * @tsk_name:   if this is a fork add the task name
+ *
+ * This function moves the workqueue top in addition to adding
+ *
+ */
+void __always_inline wq_push(uintptr_t addr, size_t size, enum AccessType
+		access_type, uintptr_t ip, char * tsk_name)
+{
+	struct memorizer_kernel_event * evtptr = wq_top();
+	evtptr->event_type = access_type;
+	evtptr->pid = task_pid_nr(current);
+
+	if (access_type < Memorizer_Fork) {
+		evtptr->data.et.src_va_ptr = ip;
+		evtptr->data.et.va_ptr = addr;
+		evtptr->data.et.event_size = size;
+	} else {
+		strncpy(evtptr->data.comm, tsk_name, sizeof(evtptr->data.comm));
+	}
+
+	/* If we are at the end of the queue swap out and schedule work */
+	if (unlikely(wq_index == num_entries_perwq-1)) {
+		switch_to_next_work_queue();
+	} else {
+		++wq_index;
+	}
+
+	/* 
+	 * This is an approach to avoid using workqueues and drain the queue the
+	 * first time we are in process context.
+	 *
+	 * TODO: There is a bug: our consumer is too slow and gets caught on some
+	 * workloads: so we overflow
+	 */
+	if (unlikely(work_deferred)) {
+		if (!(in_irq()) || !(in_softirq())) {
+			unsigned long flags;
+			local_irq_save(flags);
+			parse_events(&mem_events_wq_data[next_to_parse]);
+			local_irq_restore(flags);
+			work_deferred = false;
+		}
+	}
+}
+
+void __drain_active_work_queue()
+{
+	switch_to_next_work_queue();
+}
+
+static void wq_exit(void)
+{
+	//printd();
+	//flush_workqueue(wq);
+	//destroy_workqueue(wq);
+	//printd();
+}
+
+
+/**
+ * init_mem_access_wl - initialize the percpu data structures
+ *
+ * Init all the values to 0 for the selector, head, and tail
+ */
+static void init_mem_access_wls(void)
+{
+	int i, j = 0;
+	//struct memorizer_kernel_event * data = NULL;
+#if 0
+	struct mem_access_worklists * wls;
+	size_t cpu;
+	for_each_possible_cpu(cpu){
+		wls = &per_cpu(mem_access_wls, cpu);
+		wls->selector = 0;
+		wls->head = -1;
+		wls->tail = 0;
+	}
+#endif
+	for (; i < num_queues; i++) {
+		/* initialize the event queue to NULL for properer ring buffer */
+		for (j = 0; j < num_entries_perwq; j++) {
+			mem_events_wq_data[i].data[j].event_type = Memorizer_NULL;
+		}
+
+#if WORKQUEUES == 1
+		/* Setup the workqueue structures */
+		INIT_WORK(&mem_events_wq_data[i].work, &mem_events_workhandler);
+#endif
+	}
+}
+
+/* Fops and Callbacks for char_driver */
+
+static int char_open(struct inode *inode, struct file* file)
+{
+	return 0;
+};
+
+static int char_close(struct inode *inode, struct file* file)
+{
+	return 0;
+};
+
+static int char_mmap(struct file *file, struct vm_area_struct * vm_struct)
+{
+	__memorizer_enter();
+	unsigned long pfn;
+	int i = 0;
+	int bufNum = 252-imajor(file->f_inode);
+	for (i = 0; i < ML; i++) {
+		pfn = vmalloc_to_pfn(buffList[bufNum]+i*PAGE_SIZE);
+		remap_pfn_range(vm_struct, vm_struct->vm_start+i*PAGE_SIZE, pfn, PAGE_SIZE, PAGE_SHARED);
+	}
+	__memorizer_exit();
+	return 0;
+
+};
+
+static const struct file_operations char_driver={
+	.owner = THIS_MODULE,
+	.open = char_open,
+	.release = char_close,
+	.mmap = char_mmap,
+};
+
+static int create_char_devs(void)
+{
+	int i = 0;
+	for (i = 0; i < NB; i++) {
+		dev[i] = kmalloc(sizeof(dev_t), GFP_KERNEL);
+		cd[i] = kmalloc(sizeof(struct cdev), GFP_KERNEL);
+
+		char devName[12];
+		sprintf(devName,"char_dev%u",i);
+		pr_info("%s\n",devName);
+
+		if (alloc_chrdev_region(dev[i],0,1,devName)<0) {
+			pr_warning("Something Went Wrong with allocating char device\n");
+		} else {
+			pr_info("Allocated Region for char device\n");
+		}
+		cdev_init(cd[i],&char_driver);
+		if (cdev_add(cd[i], *dev[i], 1) < 0) {
+			pr_warning("Couldn't add the char device\n");
+		} else {
+			pr_info("Added the char device\n");
+		}
+
+	}
+}
+
+/**
+ * memorizer_init() - initialize memorizer state
+ *
+ * Set enable flag to true which enables tracking for memory access and object
+ * allocation. Allocate the object cache as well.
+ */
+void __init memorizer_init(void)
+{
+	unsigned long flags;
+	int i = 0;
+
+	__memorizer_enter();
+#if INLINE_EVENT_PARSE == 0
+	init_mem_access_wls();
+#endif
+	/* allocate and initialize memorizer internal allocator */
+	memorizer_alloc_init();
+
+	/* initialize the lookup table */
+	lt_init();
+
+	/* initialize the table tracking CFG edges */
+	func_hash_tbl_init();
+	cfgtbl = create_function_hashtable();
+
+	/* Create default catch all objects for types of allocated memory */
+	for (i = 0; i < NumAllocTypes; i++) {
+		general_kobjs[i] = memalloc(sizeof(struct memorizer_kobj));
+		init_kobj(general_kobjs[i], 0, 0, 0, i);
+		write_lock(&object_list_spinlock);
+		list_add_tail(&general_kobjs[i]->object_list, &object_list);
+		write_unlock(&object_list_spinlock);
+	}
+
+	/* Allocate memory for the global metadata table.
+	 * Not used by Memorizer, but used in processing globals offline. */
+	global_table_text = memalloc(global_table_text_size);
+	global_table_ptr = global_table_text;
+
+	local_irq_save(flags);
+	if (memorizer_enabled_boot) {
+		memorizer_enabled = true;
+	} else {
+		memorizer_enabled = false;
+	}
+	if (mem_log_boot) {
+		memorizer_log_access = true;
+	} else {
+		memorizer_log_access = false;
+	}
+	if (cfg_log_boot) {
+		cfg_log_on = true;
+	} else {
+		cfg_log_on = false;
+	}
+	if (stack_trace_boot && !cfg_log_on) {
+		stack_trace_on = true;
+	} else {
+		stack_trace_on = false;
+	}
+	print_live_obj = true;
+	
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+
+/*
+ * Late initialization function.
+ */
+static int memorizer_late_init(void)
+{
+	wq = create_workqueue("wq_memorizer_events");
+
+	unsigned long flags;
+	struct dentry *dentry, *dentryMemDir;
+
+	dentryMemDir = debugfs_create_dir("memorizer", NULL);
+	if (!dentryMemDir)
+		pr_warning("Failed to create debugfs memorizer dir\n");
+
+	dentry = debugfs_create_file("kmap", S_IRUGO, dentryMemDir,
+			NULL, &kmap_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs kmap file\n");
+
+	/* stats interface */
+	dentry = debugfs_create_file("show_stats", S_IRUGO, dentryMemDir,
+			NULL, &show_stats_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs show stats\n");
+
+	dentry = debugfs_create_file("clear_dead_objs", S_IWUGO, dentryMemDir,
+			NULL, &clear_dead_objs_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs clear_dead_objs\n");
+
+	dentry = debugfs_create_file("clear_printed_list", S_IWUGO, dentryMemDir,
+			NULL, &clear_printed_list_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs clear_printed_list\n");
+
+	dentry = debugfs_create_file("cfgmap", S_IRUGO|S_IWUGO, dentryMemDir,
+			NULL, &cfgmap_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs cfgmap\n");
+
+	dentry = debugfs_create_bool("memorizer_enabled", S_IRUGO|S_IWUGO,
+			dentryMemDir, &memorizer_enabled);
+	if (!dentry)
+		pr_warning("Failed to create debugfs memorizer_enabled\n");
+
+	dentry = debugfs_create_bool("memorizer_log_access", S_IRUGO|S_IWUGO,
+			dentryMemDir, &memorizer_log_access);
+	if (!dentry)
+		pr_warning("Failed to create debugfs memorizer_log_access\n");
+
+	dentry = debugfs_create_bool("cfg_log_on", S_IRUGO|S_IWUGO,
+			dentryMemDir, &cfg_log_on);
+	if (!dentry)
+		pr_warning("Failed to create debugfs cfg_log_on\n");
+
+	dentry = debugfs_create_bool("stack_trace_on", S_IRUGO|S_IWUGO,
+			dentryMemDir, &stack_trace_on);
+	if (!dentry)
+		pr_warning("Failed to create debugfs stack_trace_on\n");
+
+	dentry = debugfs_create_bool("print_live_obj", S_IRUGO | S_IWUGO,
+			dentryMemDir, &print_live_obj);
+	if (!dentry)
+		pr_warning("Failed to create debugfs print_live_obj\n");
+
+	dentry = debugfs_create_file("drain_active_work_queue", S_IWUGO, dentryMemDir,
+			NULL, &drain_active_work_queue_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs drain_active_work_queue\n");
+
+	dentry = debugfs_create_file("global_table", S_IRUGO, dentryMemDir,
+				     NULL, &globaltable_fops);
+	if (!dentry)
+		pr_warning("Failed to create debugfs show stats\n");	
+
+	pr_info("Memorizer initialized\n");
+	pr_info("Size of memorizer_kobj:%d\n",sizeof(struct memorizer_kobj));
+	pr_info("Size of memorizer_kernel_event:%d\n",sizeof(struct memorizer_kernel_event));
+	pr_info("FIXADDR_START: %p,  FIXADDR_SIZE %p", FIXADDR_START, FIXADDR_SIZE);	
+	print_pool_info();
+	print_stats(KERN_INFO);
+
+	return 0;
+}
+late_initcall(memorizer_late_init);
+
+/**
+ * init_from_driver() - Initialize memorizer from a driver
+ *
+ * The primary focus of this funciton is to allow for very late enable and init
+ */
+int memorizer_init_from_driver(void)
+{
+	unsigned long flags;
+
+	__memorizer_enter();
+
+	pr_info("Running test from driver...");
+
+	local_irq_save(flags);
+
+	memorizer_enabled = true;
+	memorizer_log_access = true;
+	cfg_log_on = true;
+	local_irq_restore(flags);
+
+	print_stats(KERN_INFO);
+
+#if MEMORIZER_DEBUG >= 5
+	//read_lock_irqsave(&active_kobj_rbtree_spinlock, flags);
+
+	pr_info("The free'd Kobj list");
+	dump_object_list();
+
+	pr_info("The live kernel object tree now:");
+	__print_active_rb_tree(active_kobj_rbtree_root.rb_node);
+
+	//read_unlock_irqrestore(&active_kobj_rbtree_spinlock, flags);
+#endif
+
+	print_stats(KERN_INFO);
+
+	__memorizer_exit();
+	return 0;
+}
+EXPORT_SYMBOL(memorizer_init_from_driver);
diff --git a/mm/memorizer/memorizer.h b/mm/memorizer/memorizer.h
new file mode 100644
index 000000000000..c10dd1008b5f
--- /dev/null
+++ b/mm/memorizer/memorizer.h
@@ -0,0 +1,75 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * 
+ * University of Illinois/NCSA Open Source License 
+ *
+ * Copyright (C) 2018, The Board of Trustees of the University of Illinois.
+ * All rights reserved. 
+ *
+ * Developed by: 
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions: 
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers. 
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission. 
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE. 
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.h
+ *
+ *    Description:  General inclues and utilities for memorizer tracing
+ *                  framework.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef __MEMORIZER_H_
+#define __MEMORIZER_H_
+
+#include <linux/gfp.h>
+
+/* mask to apply to memorizer allocations TODO: verify the list */
+#define gfp_memorizer_mask(gfp)	((GFP_ATOMIC | __GFP_NOTRACK | __GFP_NORETRY | GFP_NOWAIT))
+        
+#if 0
+        &   \
+        ( GFP_ATOMIC            \
+          | __GFP_NOTRACK	    \
+          | __GFP_NORETRY       \
+        )                       \
+        )
+#endif
+//| __GFP_MEMALLOC	    \
+
+/* global timestamp counter */
+static atomic_t timestamp = ATOMIC_INIT(0);
+static long get_ts(void) { return atomic_fetch_add(1,&timestamp); }
+struct memorizer_kobj *create_kobj(uintptr_t call_site, uintptr_t ptr, uint64_t size, enum AllocType AT); 
+#endif /* __MEMORIZER_H_ */
+
diff --git a/mm/memorizer/stats.c b/mm/memorizer/stats.c
new file mode 100644
index 000000000000..f82c81115734
--- /dev/null
+++ b/mm/memorizer/stats.c
@@ -0,0 +1,414 @@
+/*===-- LICENSE -------------------------------------------------------------===
+ *
+ * University of Illinois/NCSA Open Source License
+ *
+ * Copyright (C) 2018, The Board of Trustees of Rice University.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Nathan Dautenhahn in the Department of Computer
+ *    Science at Rice Unversity
+ *    http://nathandautenhahn.com
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  stats.c
+ *
+ *    Description:  
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#include <linux/debugfs.h>
+#include <linux/printk.h>
+#include <linux/seq_file.h>
+
+#include <linux/memorizer.h>
+#include "stats.h"
+#include "kobj_metadata.h"
+
+#define pr_fmt(fmt) "memorizer: " fmt
+
+#ifdef CONFIG_MEMORIZER_STATS
+
+//==-- Debug and Stats Output Code --==//
+
+/* syntactic sugar to reduce line length below */
+static __always_inline int64_t geta(atomic64_t * a) { return atomic64_read(a); }
+static __always_inline void inca(atomic64_t * a) { atomic64_inc(a); }
+static __always_inline void adda(uint64_t i, atomic64_t * a){atomic64_add(i,a);}
+
+/* stats data structure accounting for each type of alloc */
+static atomic64_t untracked_refs[NumAllocTypes];
+static atomic64_t tracked_refs[NumAllocTypes];
+static atomic64_t tracked_allocs[NumAllocTypes];
+static atomic64_t untracked_bytes_accessed[NumAllocTypes];
+static atomic64_t tracked_bytes_accessed[NumAllocTypes];
+static uint64_t bytes_accessed_overflows = 0;
+
+/* Lookup Table */
+static atomic64_t num_l3 = ATOMIC_INIT(0);
+static atomic64_t num_l2 = ATOMIC_INIT(0);
+static atomic64_t num_l1 = ATOMIC_INIT(0);
+static const uint64_t l3size = sizeof(struct lt_l3_tbl);
+static const uint64_t l2size = sizeof(struct lt_l2_tbl);
+static const uint64_t l1size = sizeof(struct lt_l1_tbl);
+
+void __always_inline track_l1_alloc(void){inca(&num_l1);};
+void __always_inline track_l2_alloc(void){inca(&num_l2);};
+void __always_inline track_l3_alloc(void){inca(&num_l3);};
+
+/* Memory Access */
+static atomic64_t tracked_kobj_accesses = ATOMIC_INIT(0);
+static atomic64_t num_induced_accesses = ATOMIC_INIT(0);
+static atomic64_t num_stack_accesses = ATOMIC_INIT(0);
+static atomic64_t num_accesses_while_disabled = ATOMIC_INIT(0);
+static atomic64_t num_untracked_obj_access = ATOMIC_INIT(0);
+
+void __always_inline
+track_access(enum AllocType AT, uint64_t size)
+{
+    inca(&tracked_kobj_accesses);
+    if (AT<NumAllocTypes) {
+        inca(&tracked_refs[AT]);
+        adda(size, &tracked_bytes_accessed[AT]);
+    	if(geta(&tracked_bytes_accessed[AT]) < 0)
+    		bytes_accessed_overflows = 0;
+    }
+}
+
+void __always_inline
+track_induced_access(void)
+{
+    inca(&num_induced_accesses);
+}
+
+void __always_inline
+track_stack_access(void)
+{
+    inca(&num_stack_accesses);
+}
+
+void __always_inline
+track_disabled_access(void)
+{
+    inca(&num_accesses_while_disabled);
+}
+
+void __always_inline
+track_untracked_access(enum AllocType AT, uint64_t size)
+{
+    inca(&num_untracked_obj_access);
+    if (AT<NumAllocTypes) {
+        inca(&untracked_refs[AT]);
+        adda(size, &untracked_bytes_accessed[AT]);
+    	if (geta(&untracked_bytes_accessed[AT]) < 0)
+    		bytes_accessed_overflows = 0;
+    }
+}
+
+/* General object info */
+static atomic64_t num_allocs_while_disabled = ATOMIC_INIT(0);
+static atomic64_t num_induced_allocs = ATOMIC_INIT(0);
+static atomic64_t stats_frees = ATOMIC_INIT(0);
+static atomic64_t num_induced_frees = ATOMIC_INIT(0);
+static atomic64_t stats_untracked_obj_frees = ATOMIC_INIT(0);
+static atomic64_t stats_kobj_frees = ATOMIC_INIT(0);
+static atomic64_t failed_kobj_allocs = ATOMIC_INIT(0);
+static atomic64_t num_access_counts = ATOMIC_INIT(0);
+
+void __always_inline track_disabled_alloc(void) { inca(&num_allocs_while_disabled); }
+void __always_inline track_induced_alloc(void) { inca(&num_induced_allocs); }
+void __always_inline track_free(void) { inca(&stats_frees); }
+void __always_inline track_untracked_obj_free(void) { inca(&stats_untracked_obj_frees); }
+void __always_inline track_induced_free(void) { inca(&num_induced_frees); }
+void __always_inline track_kobj_free(void) { inca(&stats_kobj_frees); }
+void __always_inline track_failed_kobj_alloc(void) { inca(&failed_kobj_allocs); }
+void __always_inline track_access_counts_alloc(void) { inca(&num_access_counts); }
+
+void __always_inline track_alloc(enum AllocType AT)
+{
+    if (AT > NumAllocTypes) {
+        pr_info("Bad allocation type for memorizer!");
+        return;
+    }
+    inca(&tracked_allocs[AT]);
+}
+
+void lt_pr_stats(size_t pr_level)
+{
+    int64_t l3s = geta(&num_l3);
+    int64_t l2s = geta(&num_l2);
+    int64_t l1s = geta(&num_l1);
+	printk(KERN_CRIT "------- Memorizer LT Stats -------\n");
+	printk(KERN_CRIT "  L3: %8d tbls * %6llu KB = %6llu MB\n", 
+            l3s, l3size>>10, (l3s*l3size)>>20);
+	printk(KERN_CRIT "  L2: %8d tbls * %6llu KB = %6llu MB\n", 
+            l2s, l2size>>10, (l2s*l2size)>>20);
+	printk(KERN_CRIT "  L1: %8d tbls * %6llu KB = %6llu MB\n", 
+            l1s, l1size>>10, (l1s*l1size)>>20);
+}
+
+void lt_pr_stats_seq(struct seq_file *seq)
+{
+    int64_t l3s = 1;
+    int64_t l2s = geta(&num_l2);
+    int64_t l1s = geta(&num_l1);
+	seq_printf(seq,"------- Memorizer LT Stats -------\n");
+	seq_printf(seq,"  L3: %8d tbls * %6lld KB = %6lld MB\n",
+            l3s, l3size>>10, (l3s*l3size)>>20);
+	seq_printf(seq,"  L2: %8d tbls * %6lld KB = %6lld MB\n",
+            l2s, l2size>>10, (l2s*l2size)>>20);
+	seq_printf(seq,"  L1: %8d tbls * %6lld KB = %6lld MB\n",
+            l1s, l1size>>10, (l1s*l1size)>>20);
+}
+
+static int64_t _total_tracked_refs(void)
+{
+    int i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_refs[i]);
+    return total;
+}
+
+static int64_t _total_untracked_refs(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&untracked_refs[i]);
+    return total;
+}
+
+static size_t _percent_refs_hit(void)
+{
+    return (_total_tracked_refs() || _total_untracked_refs()) ? 
+            100*_total_tracked_refs() /
+            (_total_untracked_refs()+_total_tracked_refs()) : 0;
+}
+
+static int64_t _total_tracked_bytes(void)
+{
+    int i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_bytes_accessed[i]);
+    return total;
+}
+
+static int64_t _total_untracked_bytes(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&untracked_bytes_accessed[i]);
+    return total;
+}
+
+static size_t _percent_bytes_hit(void)
+{
+    return (_total_tracked_bytes() || _total_untracked_bytes()) ?
+            100*_total_tracked_bytes() /
+            (_total_untracked_bytes()+_total_tracked_bytes()) : 0;
+}
+
+static int64_t _total_tracked(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for ( i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_allocs[i]);
+    return total;
+}
+
+static uint64_t _live_objs(void)
+{
+    return _total_tracked() - geta(&stats_frees);
+}
+
+static int64_t _total_accesses(void)
+{
+    return geta(&tracked_kobj_accesses)
+        + geta(&num_induced_accesses)
+        + geta(&num_accesses_while_disabled)
+        + geta(&num_untracked_obj_access);
+}
+
+/**
+ * print_stats() - print global stats from memorizer 
+ */
+void print_stats(size_t pr_level)
+{
+    int i;
+    printk(KERN_CRIT "------- Memory Accesses -------\n");
+    printk(KERN_CRIT "   Tracked:%16lld\n", geta(&tracked_kobj_accesses));
+    printk(KERN_CRIT "   Missing:%16lld\n", geta(&num_untracked_obj_access));
+    printk(KERN_CRIT "   Induced:%16lld\n", geta(&num_induced_accesses));
+    printk(KERN_CRIT "  Disabled:%16lld\n", geta(&num_accesses_while_disabled));
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "  Total Obs:    %16llu\n", _total_accesses());
+
+/* Print out the access counts */
+    printk(KERN_CRIT "------- Per Object Access Count (hit/miss) -------\n");
+    for ( i = 0; i < NumAllocTypes; i++) {
+            printk(KERN_CRIT "   %-15s: %16lld, %16lld\n",
+                            alloc_type_str(i), geta(&tracked_refs[i]),
+			geta(&untracked_refs[i]));
+	}
+
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+                    _total_tracked_refs(), _total_untracked_refs(),
+                    _percent_refs_hit());
+
+	/* Print out the byte counts using simple total bytes accessed */
+	printk(KERN_CRIT "------- Per Object Bytes Accessed (hit/miss) -------\n");
+    for (i = 0; i < NumAllocTypes; i++) {
+        printk(KERN_CRIT "   %-15s: %16lld, %16lld\n",
+		alloc_type_str(i),
+		geta(&tracked_bytes_accessed[i]),
+		geta(&untracked_bytes_accessed[i]));
+	}
+
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+                    _total_tracked_bytes(), _total_untracked_bytes(),
+                    _percent_bytes_hit());
+	printk(KERN_CRIT "    ****** We had %d overflows on byte counters.\n",
+	       bytes_accessed_overflows);
+
+	/* Print aggregate memory alloc stats for mem types */
+    printk(KERN_CRIT "------- Tracked Memory Allocations -------\n");
+    for (i = 0; i < NumAllocTypes; i++) {
+            printk(KERN_CRIT "   %-15s: %16lld\n",
+                            alloc_type_str(i), geta(&tracked_allocs[i]));
+    }
+    printk(KERN_CRIT "        ------\n");
+    printk(KERN_CRIT "  Total:        %16lld\n", _total_tracked());
+    printk(KERN_CRIT "  Frees:        %16lld\n", geta(&stats_frees));
+    printk(KERN_CRIT "  Live Now:     %16lld\n", _live_objs());
+
+	/* Print out info on missing allocations */
+	/* -- TODO: depracated and can probably remove */
+    printk(KERN_CRIT "------- Missing Allocs -------\n");
+    printk(KERN_CRIT "  Mem disabled: %16lld\n", geta(&num_allocs_while_disabled));
+    printk(KERN_CRIT "  Allocs(InMem):%16lld\n", geta(&num_induced_allocs));
+    printk(KERN_CRIT "  Frees(InMem): %16lld\n", geta(&num_induced_frees));
+    printk(KERN_CRIT "  Frees(NoObj): %16lld\n", geta(&stats_untracked_obj_frees));
+    printk(KERN_CRIT "  kobj fails:   %16lld\n", geta(&failed_kobj_allocs));
+
+    printk(KERN_CRIT "------- Internal Allocs -------\n");
+    /* TODO: right now if we don't drain inline then this is total tracked */
+    printk(KERN_CRIT "  Live KOBJs: %10lld * %d B = %6lld MB\n",
+                    _total_tracked()-geta(&stats_kobj_frees), sizeof(struct
+                            memorizer_kobj),
+                    (_total_tracked()-geta(&stats_kobj_frees)) * sizeof(struct
+                            memorizer_kobj) >> 20 );
+
+    printk(KERN_CRIT "  Total Edgs: %10lld * %d B = %6lld MB\n",
+                    geta(&num_access_counts), sizeof(struct access_from_counts),
+                    geta(&num_access_counts)*sizeof(struct access_from_counts)>>20);
+
+    lt_pr_stats(pr_level);
+}
+
+int seq_print_stats(struct seq_file *seq)
+{
+	int i;
+	seq_printf(seq,"------- Memory Accesses -------\n");
+	seq_printf(seq,"  Tracked:      %16lld\n", geta(&tracked_kobj_accesses));
+	seq_printf(seq,"  Missing:      %16lld\n", geta(&num_untracked_obj_access));
+	seq_printf(seq,"  Induced:      %16lld\n", geta(&num_induced_accesses));
+	seq_printf(seq,"  Disabled:     %16lld\n", geta(&num_accesses_while_disabled));
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"  Total Obs:    %16lld\n", _total_accesses());
+
+	seq_printf(seq,"------- Per Object Access Count (hit/miss) -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld, %16lld\n",
+			   alloc_type_str(i), geta(&tracked_refs[i]),
+			   geta(&untracked_refs[i]));
+	}
+
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+		   _total_tracked_refs(), _total_untracked_refs(),
+		   _percent_refs_hit());
+
+	/* Print out the byte counts using simple total bytes accessed */
+	seq_printf(seq,"------- Per Object Bytes Accessed (hit/miss) -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld, %16lld\n",
+			   alloc_type_str(i),
+			   geta(&tracked_bytes_accessed[i]),
+			   geta(&untracked_bytes_accessed[i]));
+	}
+
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+		   _total_tracked_bytes(), _total_untracked_bytes(),
+		   _percent_bytes_hit());
+	seq_printf(seq,"    ****** We had %d overflows on byte counters.\n",
+	       bytes_accessed_overflows);
+
+	seq_printf(seq,"------- Tracked Memory Allocations -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld\n",
+			   alloc_type_str(i), geta(&tracked_allocs[i]));
+	}
+	seq_printf(seq,"        ------\n");
+	seq_printf(seq,"  Total:        %16lld\n", _total_tracked());
+	seq_printf(seq,"  Frees:        %16lld\n", geta(&stats_frees));
+	seq_printf(seq,"  Live Now:     %16lld\n", _live_objs());
+
+	seq_printf(seq,"------- Missing Allocs -------\n");
+	seq_printf(seq,"  Mem disabled: %16lld\n", geta(&num_allocs_while_disabled));
+	seq_printf(seq,"  Allocs(InMem):%16lld\n", geta(&num_induced_allocs));
+	seq_printf(seq,"  Frees(InMem): %16lld\n", geta(&num_induced_frees));
+	seq_printf(seq,"  Frees(NoObj): %16lld\n", geta(&stats_untracked_obj_frees));
+	seq_printf(seq,"  kobj fails:   %16lld\n", geta(&failed_kobj_allocs));
+
+	seq_printf(seq,"------- Internal Allocs -------\n");
+	/* TODO: right now if we don't drain inline then this is total tracked */
+	seq_printf(seq,"  Live KOBJs: %10lld * %d B = %6lld MB\n",
+		   _total_tracked()-geta(&stats_kobj_frees),
+		   sizeof(struct memorizer_kobj),
+		   (_total_tracked()-geta(&stats_kobj_frees)) * sizeof(struct memorizer_kobj) >> 20 );
+
+	seq_printf(seq,"  Total Edges: %10lld * %d B = %6lld MB\n",
+		   geta(&num_access_counts), sizeof(struct access_from_counts),
+		   geta(&num_access_counts) * sizeof(struct access_from_counts)>>20);
+	lt_pr_stats_seq(seq);
+	return 0;
+}
+
+#endif /* CONFIG_MEMORIZER_STATS */
diff --git a/mm/memorizer/stats.h b/mm/memorizer/stats.h
new file mode 100644
index 000000000000..300777220b37
--- /dev/null
+++ b/mm/memorizer/stats.h
@@ -0,0 +1,105 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * 
+ * University of Illinois/NCSA Open Source License 
+ *
+ * Copyright (C) 2018, The Board of Trustees of the University of Illinois.
+ * All rights reserved. 
+ *
+ * Developed by: 
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions: 
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers. 
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission. 
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE. 
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  stats.h
+ *
+ *    Description:  
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+
+#ifndef _MEMSTATS_H_
+#define _MEMSTATS_H_
+
+#include <asm/atomic.h>
+#include <linux/memorizer.h>
+#include "kobj_metadata.h"
+
+//==-- External Interface --==//
+#ifdef CONFIG_MEMORIZER_STATS
+void track_alloc(enum AllocType AT);
+void track_disabled_alloc(void);
+void track_induced_alloc(void);
+void track_failed_kobj_alloc(void);
+void track_free(void);
+void track_untracked_obj_free(void);
+void track_induced_free(void);
+void track_kobj_free(void);
+void track_access(enum AllocType AT, uint64_t size);
+void track_induced_access(void);
+void track_stack_access(void);
+void track_disabled_access(void);
+void track_untracked_access(enum AllocType AT, uint64_t size);
+void track_access_counts_alloc(void);
+void track_l1_alloc(void);
+void track_l2_alloc(void);
+void track_l3_alloc(void);
+void print_stats(size_t pr_level);
+int seq_print_stats(struct seq_file *seq);
+#else
+static inline void track_alloc(enum AllocType AT){}
+static inline void track_disabled_alloc(void){}
+static inline void track_induced_alloc(void){}
+static inline void track_failed_kobj_alloc(void){}
+static inline void track_free(void){}
+static inline void track_untracked_obj_free(void){}
+static inline void track_induced_free(void){}
+static inline void track_kobj_free(void){}
+static inline void track_access(enum AllocType AT, uint64_t size) {}
+static inline void track_induced_access(void){}
+static inline void track_stack_access(void){}
+static inline void track_disabled_access(void){}
+static inline void track_untracked_access(enum AllocType AT, uint64_t size){}
+static inline void track_access_counts_alloc(void){}
+static inline void track_l1_alloc(void){}
+static inline void track_l2_alloc(void){}
+static inline void track_l3_alloc(void){}
+static inline void print_stats(size_t pr_level){}
+static inline int seq_print_stats(struct seq_file *seq){return 0;}
+#endif
+
+//TODO: Add kernel config option so can be disabled or add boot flag
+
+#endif /* _MEMSTATS_H_ */
+
diff --git a/mm/memorizer/util.h b/mm/memorizer/util.h
new file mode 100644
index 000000000000..b06348c78b62
--- /dev/null
+++ b/mm/memorizer/util.h
@@ -0,0 +1,69 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * 
+ * University of Illinois/NCSA Open Source License 
+ *
+ * Copyright (C) 2018, The Board of Trustees of Rice University.
+ * All rights reserved.
+ *
+ * Developed by:
+ *
+ *    Research Group of Professor Nathan Dautenhahn in the Department of Computer
+ *    Science at Rice Unversity
+ *    http://nathandautenhahn.com
+ *
+ * Copyright (c) 2018, Nathan Dautenhahn
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the Software), to deal
+ * with the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions: 
+ *
+ * Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimers. 
+ *
+ * Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimers in the documentation
+ * and/or other materials provided with the distribution.  Neither the names of
+ * Nathan Dautenhahn or the University of Illinois, nor the names of its
+ * contributors may be used to endorse or promote products derived from this
+ * Software without specific prior written permission. 
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+ * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * WITH THE SOFTWARE. 
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  utilities.h
+ *
+ *    Description:  G
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _UTIL_H_
+#define _UTIL_H_
+
+int memstrcmp(const char *cs, const char *ct)
+{
+	unsigned char c1, c2;
+
+	while (1) {
+		c1 = *cs++;
+		c2 = *ct++;
+		if (c1 != c2)
+			return c1 < c2 ? -1 : 1;
+		if (!c1)
+			break;
+	}
+	return 0;
+}
+#endif /* __utilities.h_H_ */
+
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index ec9f11d4f094..23d0b9919b5d 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -38,6 +38,8 @@
 #include <linux/kthread.h>
 #include <linux/init.h>
 
+#include <linux/memorizer.h>
+
 #include <asm/tlb.h>
 #include "internal.h"
 
@@ -986,6 +988,8 @@ bool out_of_memory(struct oom_control *oc)
 	unsigned long freed = 0;
 	enum oom_constraint constraint = CONSTRAINT_NONE;
 
+    memorizer_print_stats();
+
 	if (oom_killer_disabled)
 		return false;
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index f3e0c69a97b7..4b5db9e6cc32 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1045,6 +1045,7 @@ static __always_inline bool free_pages_prepare(struct page *page,
 	kernel_poison_pages(page, 1 << order, 0);
 	kernel_map_pages(page, 1 << order, 0);
 	kasan_free_pages(page, order);
+	memorizer_free_pages(0, page, order);
 
 	return true;
 }
@@ -3859,6 +3860,8 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
 
 	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);
 
+	memorizer_alloc_pages(_RET_IP_, page, order, gfp_mask);
+
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
@@ -3874,18 +3877,28 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 	 * __get_free_pages() returns a 32-bit address, which cannot represent
 	 * a highmem page
 	 */
+	
+	
 	VM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);
 
+	memorizer_start_getfreepages();
+	
 	page = alloc_pages(gfp_mask, order);
 	if (!page)
 		return 0;
+	
+	memorizer_alloc_getfreepages(_RET_IP_, page, order, gfp_mask);
+	
 	return (unsigned long) page_address(page);
 }
 EXPORT_SYMBOL(__get_free_pages);
 
 unsigned long get_zeroed_page(gfp_t gfp_mask)
 {
-	return __get_free_pages(gfp_mask | __GFP_ZERO, 0);
+  // Memorizer hook here to attribute alloc to this caller
+  unsigned long ret = __get_free_pages(gfp_mask | __GFP_ZERO, 0);
+  memorizer_alloc_pages(_RET_IP_, (void *) ret, 0, gfp_mask);
+  return ret;
 }
 EXPORT_SYMBOL(get_zeroed_page);
 
@@ -4059,7 +4072,13 @@ void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
 	unsigned long addr;
 
 	addr = __get_free_pages(gfp_mask, order);
-	return make_alloc_exact(addr, order, size);
+	void * ret = make_alloc_exact(addr, order, size);
+	
+	// Memorizer hook here to attribute alloc to this caller
+	// Special Memorizer hook for exact page allocation
+	memorizer_alloc_pages_exact(_RET_IP_, ret, size, gfp_mask);
+	
+	return ret;
 }
 EXPORT_SYMBOL(alloc_pages_exact);
 
diff --git a/mm/slab_common.c b/mm/slab_common.c
index ae323841adb1..58a2de1404e9 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1054,6 +1054,7 @@ void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
 {
 	void *ret = kmalloc_order(size, flags, order);
 	trace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << order, flags);
+	memorizer_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << order, flags);
 	return ret;
 }
 EXPORT_SYMBOL(kmalloc_order_trace);
diff --git a/mm/slub.c b/mm/slub.c
index 7ec0a965c6a3..029035c95e10 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1379,9 +1379,16 @@ static inline void slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
+static gfp_t last_flags = 0;
 static void setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
+
+  /* This function is called when Slub allocates new objects for a cache.
+   * Memorizer preallocates objects here so any accesses from constructors
+   * are captured correctly. */ 
+  memorizer_kmem_cache_alloc(MEMORIZER_PREALLOCED, object, s, last_flags);
+	
 	setup_object_debug(s, page, object);
 	kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
@@ -1543,6 +1550,9 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	 * so we fall-back to the minimum order allocation.
 	 */
 	alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
+
+	last_flags = alloc_gfp;
+	
 	if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
 		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~(__GFP_RECLAIM|__GFP_NOFAIL);
 
@@ -1559,7 +1569,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 			goto out;
 		stat(s, ORDER_FALLBACK);
 	}
-
+	
 	if (kmemcheck_enabled &&
 	    !(s->flags & (SLAB_NOTRACK | DEBUG_DEFAULT_FLAGS))) {
 		int pages = 1 << oo_order(oo);
@@ -1591,7 +1601,9 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	kasan_poison_slab(page);
 
-	shuffle = shuffle_freelist(s, page);
+	// For Memorizer, let's not shuffle slab. This way there's only one place where setup_object() is called.
+	// shuffle = shuffle_freelist(s, page);
+	shuffle = false;
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
@@ -2725,10 +2737,15 @@ static __always_inline void *slab_alloc(struct kmem_cache *s,
 
 void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
 {
+
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
 
 	trace_kmem_cache_alloc(_RET_IP_, ret, s->object_size,
 				s->size, gfpflags);
+	
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags);
 
 	return ret;
 }
@@ -2739,6 +2756,9 @@ void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
 	kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
@@ -2752,6 +2772,9 @@ void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 
 	trace_kmem_cache_alloc_node(_RET_IP_, ret,
 				    s->object_size, s->size, gfpflags, node);
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmem_cache_alloc_node(_RET_IP_, ret, s, gfpflags, node);
 
 	return ret;
 }
@@ -2766,7 +2789,9 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, s->size, gfpflags, node);
-
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc_node(_RET_IP_, ret, size, s->size, gfpflags, node);
 	kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
@@ -2979,6 +3004,7 @@ void kmem_cache_free(struct kmem_cache *s, void *x)
 		return;
 	slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
 	trace_kmem_cache_free(_RET_IP_, x);
+	memorizer_kmem_cache_free(_RET_IP_, x);
 }
 EXPORT_SYMBOL(kmem_cache_free);
 
@@ -3726,8 +3752,12 @@ void *__kmalloc(size_t size, gfp_t flags)
 	struct kmem_cache *s;
 	void *ret;
 
-	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE))
-		return kmalloc_large(size, flags);
+	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)){
+	  unsigned int order = get_order(size);
+	  void * ret = kmalloc_large(size, flags);
+	  memorizer_kmalloc(_RET_IP_, ret, PAGE_SIZE << order, PAGE_SIZE << order, flags);
+	  return ret;
+	}
 
 	s = kmalloc_slab(size, flags);
 
@@ -3737,6 +3767,9 @@ void *__kmalloc(size_t size, gfp_t flags)
 	ret = slab_alloc(s, flags, _RET_IP_);
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc(_RET_IP_, ret, size, s->size, flags);
 
 	kasan_kmalloc(s, ret, size, flags);
 
@@ -3770,6 +3803,12 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 		trace_kmalloc_node(_RET_IP_, ret,
 				   size, PAGE_SIZE << get_order(size),
 				   flags, node);
+		int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+		if (!update){
+		  memorizer_kmalloc_node(_RET_IP_, ret,
+					 size, PAGE_SIZE << get_order(size),
+					 flags, node);
+		}
 
 		return ret;
 	}
@@ -3782,6 +3821,10 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 	ret = slab_alloc_node(s, flags, node, _RET_IP_);
 
 	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
+	
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
 
 	kasan_kmalloc(s, ret, size, flags);
 
@@ -3864,6 +3907,7 @@ void kfree(const void *x)
 	void *object = (void *)x;
 
 	trace_kfree(_RET_IP_, x);
+	memorizer_kfree(_RET_IP_, x);
 
 	if (unlikely(ZERO_OR_NULL_PTR(x)))
 		return;
@@ -4233,6 +4277,9 @@ void *__kmalloc_track_caller(size_t size, gfp_t gfpflags, unsigned long caller)
 
 	/* Honor the call site pointer we received. */
 	trace_kmalloc(caller, ret, size, s->size, gfpflags);
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc(caller, ret, size, s->size, gfpflags);
 
 	return ret;
 }
@@ -4250,7 +4297,12 @@ void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
 		trace_kmalloc_node(caller, ret,
 				   size, PAGE_SIZE << get_order(size),
 				   gfpflags, node);
-
+		int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+		if (!update){
+		  memorizer_kmalloc_node(caller, ret,
+					 size, PAGE_SIZE << get_order(size),
+					 gfpflags, node);
+		}
 		return ret;
 	}
 
@@ -4263,6 +4315,9 @@ void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
 
 	/* Honor the call site pointer we received. */
 	trace_kmalloc_node(caller, ret, size, s->size, gfpflags, node);
+	int update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+	  memorizer_kmalloc_node(caller, ret, size, s->size, gfpflags, node);
 
 	return ret;
 }
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 3ca82d44edd3..17fd7d6ec3ab 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -1544,6 +1544,11 @@ void vfree_atomic(const void *addr)
  */
 void vfree(const void *addr)
 {
+
+  	// Memorizer hook free. So far I haven't seen frees, so TODO check this.
+  	// In particular, I wasn't sure how to get caller, so I used the builtin below.
+	memorizer_vmalloc_free((unsigned long) addr, __builtin_return_address(0));
+	
 	BUG_ON(in_nmi());
 
 	kmemleak_free(addr);
@@ -1694,6 +1699,7 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 	void *addr;
 	unsigned long real_size = size;
 
+
 	size = PAGE_ALIGN(size);
 	if (!size || (size >> PAGE_SHIFT) > totalram_pages)
 		goto fail;
@@ -1707,6 +1713,9 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 	if (!addr)
 		return NULL;
 
+	// Memorizer hooking here
+	memorizer_vmalloc_alloc((unsigned long) caller, addr, size, gfp_mask);
+
 	/*
 	 * In this function, newly allocated vm_struct has VM_UNINITIALIZED
 	 * flag. It means that vm_struct is not fully initialized.
diff --git a/scripts/cp_test.sh b/scripts/cp_test.sh
new file mode 100755
index 000000000000..34494f0d76bd
--- /dev/null
+++ b/scripts/cp_test.sh
@@ -0,0 +1,18 @@
+#!/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+cd /root
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+echo "Done Copying"
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_access
+echo 0 > memorizer_enabled
+
+cd /root
diff --git a/scripts/event_structs.h b/scripts/event_structs.h
new file mode 100644
index 000000000000..a27a3bda3e5b
--- /dev/null
+++ b/scripts/event_structs.h
@@ -0,0 +1,63 @@
+/*
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Read = 0xcc, Memorizer_Mem_Write = 0xdd, Memorizer_Fork = 0xee};
+enum AccessType {Memorizer_READ=0,Memorizer_WRITE};
+
+struct memorizer_kernel_event {
+	enum EventType event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	enum		AccessType access_type;
+	char		comm[16];
+	char		funcstr[128];
+
+			
+};
+
+struct memorizer_kernel_alloc {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	char		comm[16];
+	char		funcstr[128];
+};
+
+struct memorizer_kernel_free {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_access {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_fork {
+	char		event_type;
+	long		pid;
+	char		comm[16];
+};
+
diff --git a/scripts/memorizer/README.txt b/scripts/memorizer/README.txt
new file mode 100644
index 000000000000..3931b391ba96
--- /dev/null
+++ b/scripts/memorizer/README.txt
@@ -0,0 +1,50 @@
+Files: memorizer.py, test_memorizer.py
+
+Dependencies:
+In order to run the test_memorizer w/ linux test suite, you must 
+wget the latest version from the ltp github repo and set it up.
+Ex:
+wget https://github.com/linux-test-project/ltp/releases/download/20170116/ltp-full-20170116.tar.bz2
+tar xvfj ltp-full-20170116.tar.bz2
+# cd into the untarred dir
+./configure
+make
+sudo make install
+
+Good documentation / examples: http://ltp.sourceforge.net/documentation/how-to/ltp.php
+
+memorizer.py: accepts processes to run in quotes. 
+Ex: python memorizer.py "ls" "mkdir dir"
+In order to run the script, you must have your user be in the 
+memorizer group, which you should setup if not.
+How-to: sudo groupadd memorizer; sudo usermod -aG memorizer <user>
+You will be queried to enter your pw in order to set group 
+permissions on the /sys/kernel/debug dirs which include ftrace
+and memorizer.
+
+test_memorizer.py: accepts either -e, -m, or -h flags.
+Ex: python test_memorizer.py -e
+*All modes will run the setup/cleanup checks to ensure all virtual nodes
+are being set correctly.
+-e: Runs ls, wget, and tar sequentially.
+-m: Runs the linux test suite and saves a human-readable log to 
+/opt/ltp/results/ltp.log
+-h: runs both -e and -m
+As with the memorizer.py, you will need your user to be in the
+memorizer group.  Additionally, you will be queried to enter your
+pw in order to set group permissions on the /opt/ltp dirs.
+
+
+
+===============
+ MEMORIZER 2.0
+===============
+In order to test the new Memorizer with the busybox userland initramfs image, since the system is completely barebones, the environment needs to be set up.
+
+
+./setup_env.sh : Sets up the environment
+./userApp c: Prints the number of remaining bytes in the buffer
+./userApp p: Prints the buffer. Currently, it only prints the first 100 entries in the buffer
+./cp_test: Performs a test and copies the linuxkit directory to the root directory. Initializes the memorizer before the test and disables it afterwards.
+./enable_memorizer: Enables the memorizer and access logging
+./disable_memorizer: Disables the memorier and access logging
diff --git a/scripts/memorizer/cp_test.sh b/scripts/memorizer/cp_test.sh
new file mode 100755
index 000000000000..ef0f6324c4f4
--- /dev/null
+++ b/scripts/memorizer/cp_test.sh
@@ -0,0 +1,19 @@
+#!/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+cd /root
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+echo "Done Copying"
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_access
+echo 0 > memorizer_enabled
+
+cd /root
+./userApp p > outputMap
diff --git a/scripts/memorizer/disable_memorizer.sh b/scripts/memorizer/disable_memorizer.sh
new file mode 100755
index 000000000000..2bcbcb104fb6
--- /dev/null
+++ b/scripts/memorizer/disable_memorizer.sh
@@ -0,0 +1,7 @@
+!#/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_accesses
+echo 0 > memorizer_enabled
+
+
diff --git a/scripts/memorizer/enable_memorizer.sh b/scripts/memorizer/enable_memorizer.sh
new file mode 100755
index 000000000000..cf6001cf655b
--- /dev/null
+++ b/scripts/memorizer/enable_memorizer.sh
@@ -0,0 +1,6 @@
+#!/bin/sh
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
diff --git a/scripts/memorizer/event_structs.h b/scripts/memorizer/event_structs.h
new file mode 100644
index 000000000000..cb3698d99822
--- /dev/null
+++ b/scripts/memorizer/event_structs.h
@@ -0,0 +1,62 @@
+/* This file describes the structs to be used to describe the events happening inside the kernel:
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Access = 0xcc};
+enum AccessType {Memorizer_READ=0,Memorizer_WRITE};
+
+struct memorizer_kernel_event {
+	enum EventType event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	enum		AccessType access_type;
+	char		comm[16];
+	char		funcstr[128];
+
+			
+};
+
+struct memorizer_kernel_alloc {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	char		comm[16];
+	char		funcstr[128];
+};
+
+struct memorizer_kernel_free {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_access {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_fork {
+	char		event_type;
+	long		pid;
+	char		comm[16];
+};
diff --git a/scripts/memorizer/memorizer.py b/scripts/memorizer/memorizer.py
new file mode 100755
index 000000000000..9e0f212275fc
--- /dev/null
+++ b/scripts/memorizer/memorizer.py
@@ -0,0 +1,152 @@
+import sys,threading,os,subprocess,operator,time
+
+mem_path = "/sys/kernel/debug/memorizer/"
+directory = ""
+completed = False
+
+def worker(cmd):
+  ret = os.system(cmd)    
+  if(ret != 0):
+    print "Failed attempt on: " + cmd
+    exit(1)
+
+def basic_cleanup():
+  print "Basic tests completed. Now cleaning up."
+  ret = os.system("rm UPennlogo2.jpg")
+        
+def memManager():
+  while(not completed):
+    stats = subprocess.check_output(["free"])
+    stats_list = stats.split()
+    total_mem = float(stats_list[7])
+    used_mem = float(stats_list[8])
+    memory_usage = used_mem / total_mem
+    if(memory_usage > 0.8):
+      ret = os.system("cat " + mem_path + "kmap >> " + directory + "test.kmap")
+      if ret != 0:
+        print "Failed to append kmap to temp file"
+        exit(1)
+      ret = os.system("echo 1 > " + mem_path + "clear_printed_list")
+      if ret != 0:
+        print "Failed to clear printed list"
+        exit(1)
+    time.sleep(2)
+            
+def startup():
+  ret = os.system("sudo chgrp -R memorizer /opt/")
+  if ret != 0:
+    print "Failed to change group permissions of /opt/"
+    exit(1)
+  os.system("sudo chmod -R g+wrx /opt/")
+  if ret != 0:
+    print "Failed to grant wrx permissions to /opt/"
+    exit(1)
+  # Setup group permissions to ftrace & memorizer directories
+  ret = os.system("sudo chgrp -R memorizer /sys/kernel/debug/")
+  if ret != 0:
+    print "Failed to change memorizer group permissions to /sys/kernel/debug/"
+    exit(1)
+  ret = os.system("sudo chmod -R g+wrx /sys/kernel/debug/")
+  if ret != 0:
+    print "Failed to grant wrx persmissions to /sys/kernel/debug/"
+    exit(1)
+  # Memorizer Startup
+  ret = os.system("echo 1 > " + mem_path + "clear_object_list")
+  if ret != 0:
+    print "Failed to clear object list"
+    exit(1)
+  ret = os.system("echo 0 > " + mem_path + "print_live_obj")
+  if ret != 0:
+    print "Failed to disable live object dumping"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "memorizer_enabled")
+  if ret != 0:
+    print "Failed to enable memorizer object allocation tracking"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "memorizer_log_access")
+  if ret != 0:
+    print "Failed to enable memorizer object access tracking"
+    exit(1)
+
+def cleanup():
+  # Memorizer cleanup
+  ret = os.system("echo 0 > " + mem_path + "memorizer_log_access")
+  if ret != 0:
+    print "Failed to disable memorizer object access tracking"
+    exit(1)
+  ret = os.system("echo 0 > " + mem_path + "memorizer_enabled")
+  if ret != 0:
+    print "Failed to disable memorizer object allocation tracking"
+    exit(1)
+  # Print stats
+  ret = os.system("cat " + mem_path + "show_stats")
+  if ret != 0:
+    print "Failed to display memorizer stats"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "print_live_obj")
+  if ret != 0:
+    print "Failed to enable live object dumping"
+    exit(1)
+  # Make local copies of outputs
+  ret = os.system("cat " + mem_path + "kmap >> " +directory+ "test.kmap")
+  if ret != 0:
+    print "Failed to copy live and freed objs to kmap"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "clear_object_list")
+  if ret != 0:
+    print "Failed to clear all freed objects in obj list"
+    exit(1)
+
+def main(argv):
+  global completed
+  global directory
+  if len(sys.argv) == 1:
+    print "Invalid/missing arg. Please enter -e for basic tests, -m for ltp tests, and/or specify a full process to run in quotes. Specify path using the -p <path> otherwise default to ."
+    return
+  startup()
+  processes = []
+  easy_processes = False
+  next_arg = False
+  for arg in argv:
+    if next_arg: 
+      next_arg = False
+      directory = str(arg) + "/"
+    elif arg == '-p':
+      next_arg = True
+    #User wants to run ltp
+    elif arg == '-m':
+      print "Performing ltp tests" 
+      processes.append("/opt/ltp/runltp -p -l ltp.log")
+      print "See /opt/ltp/results/ltp.log for ltp results"
+    #User wants to run wget,ls,etc.
+    elif arg == '-e':
+      easy_processes = True
+      print "Performing basic ls test"
+      processes.append("ls")
+      print "Performing wget test"
+      processes.append("wget http://www.sas.upenn.edu/~egme/UPennlogo2.jpg")
+  print "Attempting to remove any existing kmaps in the specified path"
+  os.system("rm " + directory + "test.kmap")
+  print "Startup completed. Generating threads."
+  manager = threading.Thread(target=memManager, args=())
+  manager.start()
+  threads = []
+  for process in processes:
+    try:
+      t = threading.Thread(target=worker, args=(process,))
+      threads.append(t)
+      t.start()
+    except:
+      print "Error: unable to start thread"
+  for thr in threads:
+    thr.join()
+  completed = True
+  manager.join()
+  print "Threads ran to completion. Cleaning up."
+  basic_cleanup()
+  cleanup()
+  print "Cleanup successful."
+  return 0
+
+if __name__ == "__main__":
+  main(sys.argv)
diff --git a/scripts/memorizer/setup_env.sh b/scripts/memorizer/setup_env.sh
new file mode 100755
index 000000000000..cc603d047d5f
--- /dev/null
+++ b/scripts/memorizer/setup_env.sh
@@ -0,0 +1,11 @@
+#!/bin/sh
+
+mount -t debugfs nodev /sys/kernel/debug
+mknod /dev/null c 1 3
+cp cp_test.sh /root
+cp userApp /root
+cp enable_memorizer.sh /root
+cp disable_memorizer.sh /root
+cd /root && mknod node c 252 0
+
+
diff --git a/scripts/memorizer/test_cp_memorizer.sh b/scripts/memorizer/test_cp_memorizer.sh
new file mode 100755
index 000000000000..ca292175d19b
--- /dev/null
+++ b/scripts/memorizer/test_cp_memorizer.sh
@@ -0,0 +1,18 @@
+#!/bin/sh
+
+KDIR=/sys/kernel/debug/memorizer
+UDIR=/mnt/host/src/repos/linux-slice/scripts
+cd $WDIR
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+cd /root
+./userApp
+
+cd $WDIR
+echo 0 > memorizer_enabled
+echo 0 > memorizer_log_access
diff --git a/scripts/memorizer/userApp b/scripts/memorizer/userApp
new file mode 100755
index 000000000000..a30326650e04
Binary files /dev/null and b/scripts/memorizer/userApp differ
diff --git a/scripts/memorizer/userApp.c b/scripts/memorizer/userApp.c
new file mode 100644
index 000000000000..70e59257e3f7
--- /dev/null
+++ b/scripts/memorizer/userApp.c
@@ -0,0 +1,237 @@
+#include <stdio.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <inttypes.h>
+#include "event_structs.h"
+
+
+#define ML 400000  // The size of profiler buffer (Unit: memory page)
+
+#define BUFF_MUTEX_LOCK { \
+		while(*buff_mutex); \
+		*buff_mutex = *buff_mutex + 1;\
+	}
+
+#define BUFF_MUTEX_UNLOCK {*buff_mutex = *buff_mutex - 1;}
+
+#define BUFF_FILL_RESET {*buff_fill = 0;}
+
+
+
+static int buf_fd = -1;
+static int buf_len;
+struct stat s ;
+char *buf;
+char *buff_end;
+char *buff_fill;
+struct memorizer_kernel_event *mke_ptr;
+unsigned int *buff_free_size; 
+
+
+
+// This function opens a character device (which is pointed by a file named as fname) and performs the mmap() operation. If the operations are successful, the base address of memory mapped buffer is returned. Otherwise, a NULL pointer is returned.
+void *buf_init(char *fname)
+{
+	unsigned int *kadr;
+
+	if(buf_fd == -1){
+	buf_len = ML * getpagesize();
+	if ((buf_fd=open(fname, O_RDWR|O_SYNC))<0){
+	          printf("File open error. %s\n", fname);
+	          return NULL;
+		}
+	}
+	kadr = mmap(0, buf_len, PROT_READ|PROT_WRITE, MAP_SHARED, buf_fd, 0);
+	if (kadr == MAP_FAILED){
+		printf("Buf file open error.\n");
+		return NULL;
+		}
+	return kadr;
+}
+
+// This function closes the opened character device file
+void buf_exit()
+{
+	if(buf_fd!=-1){
+		close(buf_fd);
+		buf_fd = -1;
+	}
+}
+
+void printAllocHex()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	printf("aa, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	printf("%x, ",mke_ptr->event_size);
+	printf("%lx, ",mke_ptr->event_jiffies);	
+	printf("%x, ",mke_ptr->pid);
+	printf("%s, ",mke_ptr->comm);
+	printf("%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+}
+
+void printAlloc()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	printf("Alloc: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	printf("%u, ",mke_ptr->event_size);
+	printf("%lu, ",mke_ptr->event_jiffies);	
+	printf("%u, ",mke_ptr->pid);
+	printf("%s, ",mke_ptr->comm);
+	printf("%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+}
+
+
+void printFreeHex()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	printf("0xbb, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%lx, ",mke_ptr->event_jiffies);	
+	printf("%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+}
+
+void printFree()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	printf("Free: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%lu, ",mke_ptr->event_jiffies);	
+	printf("%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+}
+
+void printAccessHex(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		printf("0xcc, ");
+	else
+		printf("0xdd, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%x, ",mke_ptr->event_size);
+	printf("%lx, ",mke_ptr->event_jiffies);	
+	printf("%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+}
+
+
+void printAccess(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		printf("Read: ");
+	else
+		printf("Write: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%u, ",mke_ptr->event_size);
+	printf("%lu, ",mke_ptr->event_jiffies);	
+	printf("%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+}
+
+void printFork()
+{
+	struct memorizer_kernel_fork *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_fork *)buf;
+	printf("Fork: ");
+	printf("%ld, ",mke_ptr->pid);
+	printf("%s\n",mke_ptr->comm);
+	buf = buf + sizeof(struct memorizer_kernel_fork);
+
+}
+
+int main (int argc, char *argv[])
+{
+	if(argc != 2)
+	{
+		printf("Incorrect number of Command Line Arguments!\n");
+		return 0;
+	}
+
+	// Open the Character Device and MMap 
+	buf = buf_init("node");
+	if(!buf)
+		return -1;
+
+	//Read and count the MMaped data entries
+	buff_end = (buf + ML*getpagesize()) - 1;
+	buff_fill = buf;
+	buf++;
+	buff_free_size = (unsigned int *)buf;
+	buf = buf + sizeof(unsigned int);
+
+	mke_ptr = (struct memorizer_kernel_event *)buf;
+	if(*argv[1]=='c')
+	{
+		printf("Remaining Bytes: ");
+		printf("%u",*buff_free_size);
+	}
+	else if(*argv[1]=='p')
+	{
+	
+		//TODO: Call different functions for different events
+		while(*buf!=0)
+		{
+			if(*buf == 0xffffffaa)
+				printAlloc();
+			else if (*buf == 0xffffffbb)
+				printFree();
+			else if(*buf == 0xffffffcc)
+				printAccess('r');
+			else if(*buf == 0xffffffdd)
+				printAccess('w');
+			else if(*buf == 0xffffffee)
+				printFork();
+
+		}	
+
+	}
+	else if(*argv[1]=='h')
+	{
+	
+		//TODO: Call different functions for different events
+		while(*buf!=0)
+		{
+			if(*buf == 0xffffffaa)
+				printAllocHex();
+			else if (*buf == 0xffffffbb)
+				printFreeHex();
+			else if(*buf == 0xffffffcc)
+				printAccessHex('r');
+			else if(*buf == 0xffffffdd)
+				printAccessHex('w');
+		}	
+
+	}	
+	buf_exit();
+	
+	return 0;
+}
+
diff --git a/scripts/userApp b/scripts/userApp
new file mode 100755
index 000000000000..5d4afb3f8113
Binary files /dev/null and b/scripts/userApp differ
diff --git a/scripts/userApp.c b/scripts/userApp.c
new file mode 100644
index 000000000000..7a899906b58e
--- /dev/null
+++ b/scripts/userApp.c
@@ -0,0 +1,316 @@
+#define _GNU_SOURCE 
+#include <stdio.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <inttypes.h>
+#include "event_structs.h"
+#include <pthread.h>
+
+
+#define ML 500000  // The size of profiler buffer (Unit: memory page)
+#define NB 16
+#define BUFF_MUTEX_LOCK { \
+		while(*buff_mutex)\
+		*buff_mutex = *buff_mutex + 1;\
+	}
+
+#define BUFF_MUTEX_UNLOCK {*buff_mutex = *buff_mutex - 1;}
+
+#define BUFF_FILL_RESET {*buff_fill = 0;}
+
+
+
+static int buff_fd_list[NB];
+static int buf_len;
+struct stat s ;
+char *buffList[NB];
+char *buf;
+char *buff_end;
+char *buff_start;
+char *buff_fill;
+char *buff_mutex;
+struct memorizer_kernel_event *mke_ptr;
+unsigned int *buff_free_size; 
+char *stringBase;
+unsigned int idx;
+char outputFileName[30];
+FILE *fp;
+unsigned int curBuff = 0;
+
+
+/*
+ * switchBuffer - switches the the buffer being written to, when the buffer is full
+ */
+void switchBuffer()
+{
+		buf = (char *)buffList[curBuff];
+	
+		buff_fill = buf;
+		buf = buf + 1;
+	
+		buff_mutex = buf;
+		buf = buf + 1;
+	
+	
+		buff_free_size = (unsigned int *)buf;
+		buf = buf + sizeof(unsigned int);
+	
+
+		buff_start = buf;
+	
+
+}
+
+
+
+
+
+
+
+
+// This function opens a character device (which is pointed by a file named as fname) and performs the mmap() operation. If the operations are successful, the base address of memory mapped buffer is returned. Otherwise, a NULL pointer is returned.
+void *buf_init(char *fname,int *buf_fd)
+{
+	unsigned int *kadr;
+
+	buf_len = ML * getpagesize();
+	if ((*buf_fd=open(fname, O_RDWR|O_SYNC))<0){
+	          printf("File open error. %s\n", fname);
+	          return NULL;
+	}
+	kadr = mmap(0, buf_len, PROT_READ|PROT_WRITE, MAP_SHARED, *buf_fd, 0);
+	if (kadr == MAP_FAILED){
+		printf("Buf file open error.\n");
+		return NULL;
+		}
+	return kadr;
+}
+
+// This function closes the opened character device file
+void buf_exit(int buf_fd)
+{
+	if(buf_fd!=-1){
+		close(buf_fd);
+		buf_fd = -1;
+	}
+}
+
+void printAllocHex()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	fprintf(fp,"aa, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	fprintf(fp,"%x, ",mke_ptr->event_size);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%x, ",mke_ptr->pid);
+	fprintf(fp,"%s, ",mke_ptr->comm);
+	fprintf(fp,"%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "before 1 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "after 1 incrementing = %u\n", *buff_free_size);
+}
+
+void printAlloc()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	fprintf(fp,"Alloc: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	fprintf(fp,"%u, ",mke_ptr->event_size);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%u, ",mke_ptr->pid);
+	fprintf(fp,"%s, ",mke_ptr->comm);
+	fprintf(fp,"%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "before 2 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "after 2 incrementing = %u\n", *buff_free_size);
+}
+
+
+void printFreeHex()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	fprintf(fp,"0xbb, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "before 3 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "after 3 incrementing = %u\n", *buff_free_size);
+}
+
+void printFree()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	fprintf(fp,"Free: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "before 4 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "after 4 incrementing = %u\n", *buff_free_size);
+}
+
+void printAccessHex(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		fprintf(fp,"0xcc, ");
+	else
+		fprintf(fp,"0xdd, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%x, ",mke_ptr->event_size);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "before 5 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "after 5 incrementing = %u\n", *buff_free_size);
+}
+
+
+void printAccess(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		fprintf(fp,"Read: ");
+	else
+		fprintf(fp,"Write: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%u, ",mke_ptr->event_size);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);	
+	fprintf(fp,"%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "before 6 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "after 6 incrementing = %u\n", *buff_free_size);
+}
+
+void printFork()
+{
+	struct memorizer_kernel_fork *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_fork *)buf;
+	fprintf(fp,"Fork: ");
+	fprintf(fp,"%ld, ",mke_ptr->pid);
+	fprintf(fp,"%s\n",mke_ptr->comm);
+	buf = buf + sizeof(struct memorizer_kernel_fork);
+//	fprintf(stderr, "before 7 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_fork);
+//	fprintf(stderr, "after 7 incrementing = %u\n", *buff_free_size);
+}
+
+int main (int argc, char *argv[])
+{
+	unsigned int i;
+	char devName[12];
+	if(argc != 2)
+	{
+		printf("Incorrect number of Command Line Arguments!\n");
+		return 0;
+	}
+
+
+	for(i = 0;i<NB;i++)
+	{
+		sprintf(devName,"node%u",i);
+		buffList[i] = buf_init(devName,&buff_fd_list[i]);
+		if(!buff_fd_list[i])
+			return -1;
+	}
+	// Open the Character Device and MMap 
+
+	switchBuffer();
+	
+	printf("Choosing inital buffer\n");
+
+
+	if(*argv[1]=='c')
+	{
+		printf("Remaining Bytes: ");
+		printf("%u\n",*buff_free_size);
+	}
+	else if(*argv[1]=='p')
+	{
+
+		while(1)
+		{
+			
+			
+			//We Don't want the memorizer tracking us clearing out the buffer from userspace
+			if(!*buff_fill)
+			{
+				curBuff = (curBuff + 1)%NB;
+				continue;
+			}
+			
+			printf("Userspace: Buffer Full! Now Clearing Buffer %d\n",curBuff);
+
+			
+			sprintf(outputFileName,"ouput%d",idx);
+			fp = fopen(outputFileName,"w+");
+
+
+			//printf("Acquired the Lock\n");
+			while(*buf!=0)
+			{
+				if(*buf == 0xffffffaa)
+					printAlloc();
+				else if (*buf == 0xffffffbb)
+					printFree();
+				else if(*buf == 0xffffffcc)
+					printAccess('r');
+				else if(*buf == 0xffffffdd)
+					printAccess('w');
+				else if(*buf == 0xffffffee)
+					printFork();
+				idx++;
+			}
+			*buff_free_size = (4096 * ML) - 6; 
+			*buff_fill = 0;
+			printf("Done Printing\n");
+
+			fclose(fp);
+			printf("Closed the File Pointer\n");
+	
+			idx++;
+
+		}
+			
+	}
+	for(i = 0;i<NB;i++)
+	{
+		buf_exit(buff_fd_list[i]);
+	
+	}
+
+
+	
+	return 0;
+}
+
+
diff --git a/tools/include/linux/log2.h b/tools/include/linux/log2.h
index 41446668ccce..d5677d39c1e4 100644
--- a/tools/include/linux/log2.h
+++ b/tools/include/linux/log2.h
@@ -12,12 +12,6 @@
 #ifndef _TOOLS_LINUX_LOG2_H
 #define _TOOLS_LINUX_LOG2_H
 
-/*
- * deal with unrepresentable constant logarithms
- */
-extern __attribute__((const, noreturn))
-int ____ilog2_NaN(void);
-
 /*
  * non-constant log of base 2 calculators
  * - the arch may override these in asm/bitops.h if they can be implemented
@@ -78,7 +72,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
 #define ilog2(n)				\
 (						\
 	__builtin_constant_p(n) ? (		\
-		(n) < 1 ? ____ilog2_NaN() :	\
+		(n) < 2 ? 0 :			\
 		(n) & (1ULL << 63) ? 63 :	\
 		(n) & (1ULL << 62) ? 62 :	\
 		(n) & (1ULL << 61) ? 61 :	\
@@ -141,10 +135,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
 		(n) & (1ULL <<  4) ?  4 :	\
 		(n) & (1ULL <<  3) ?  3 :	\
 		(n) & (1ULL <<  2) ?  2 :	\
-		(n) & (1ULL <<  1) ?  1 :	\
-		(n) & (1ULL <<  0) ?  0 :	\
-		____ilog2_NaN()			\
-				   ) :		\
+		1 ) :				\
 	(sizeof(n) <= 4) ?			\
 	__ilog2_u32(n) :			\
 	__ilog2_u64(n)				\
